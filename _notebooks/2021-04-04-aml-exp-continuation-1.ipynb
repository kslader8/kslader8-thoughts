{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Workspace - [Tensorflow] Time Series Example | Continuation 1\n",
    "- categories: [azureml, tensorflow, time-series]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the same as the previous notebook. The goals is simply to update it to make a few enhancements that became apparent during the process of making the first notebook.\n",
    "\n",
    "**Enhancements**\n",
    "- Start using the pipeline api\n",
    "- Adding plots for all models\n",
    "\n",
    "**Bug Fixes**\n",
    "- Normalizing input data\n",
    "- Increasing the test data set for lstm testing & performance plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version 1.2.0\n",
      "numpy version 1.18.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(f\"pandas version {pd.__version__}\")\n",
    "print(f\"numpy version {np.__version__}\")\n",
    "# print(f\"tensorflow version {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml version 1.25.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core as aml\n",
    "\n",
    "from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment, Run\n",
    "from azureml.core import Datastore, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "print(f\"azureml version {aml.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorboard\n",
    "\n",
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "print(f\"tensorboard version {tensorboard.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twelvedata version 1.1.7\n"
     ]
    }
   ],
   "source": [
    "import twelvedata\n",
    "from twelvedata import TDClient\n",
    "\n",
    "print(f\"twelvedata version {twelvedata.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a personal preference of mine to make a .env file per project to encapsulate tokens/secrets/etc outside of notebooks. \n",
    "\n",
    "In this case I created a file named .env with a single variable apikey=(api key) in the same directory as my experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = Path(\".env\")\n",
    "assert env_path.exists()\n",
    "_ = load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "It's useful to set a few global plotting defaults to save from doing them for every plot in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure ML Workspace\n",
    "To setup an Azure ML Workspace you will need an azure account (with credit card). To spin it up simply go to https://portal.azure.com/ and type machine learning in the search bar and create a workspace.\n",
    "\n",
    "Once you have a workspace you will need to download the config.json prior to going to https://ml.azure.com/ to access your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_config_path = Path(\"config.json\")\n",
    "assert workspace_config_path.exists()\n",
    "ws = Workspace.from_config(path=workspace_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twelve Data Client\n",
    "I setup an account at https://twelvedata.com/ to get a free api key to try it out. I had not heard of it before, but it was the first thing that came up in my google search for free market data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = os.environ.get(\"apikey\")\n",
    "td = TDClient(apikey=apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Workspace Compute\n",
    "\n",
    "### Get existing compute cluster or create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new compute target...\n",
      "Creating....\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-04-02T20:04:47.531000+00:00', 'errors': None, 'creationTime': '2021-04-02T20:04:42.205768+00:00', 'modifiedTime': '2021-04-02T20:04:59.941680+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "compute_name = \"aml-compute\"\n",
    "vm_size = \"Standard_NC6\"\n",
    "# vm_size = \"Standard_NC6s_v3\"\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target: ' + compute_name)\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,  # STANDARD_NC6 is GPU-enabled\n",
    "                                                                min_nodes=0,\n",
    "                                                                max_nodes=4)\n",
    "    # create the compute target\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current cluster status, use the 'status' property\n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Workspace Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TwelveData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List ETFs Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>name</th>\n",
       "      <th>currency</th>\n",
       "      <th>exchange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8PSG</td>\n",
       "      <td>Invesco Physical Gold ETC</td>\n",
       "      <td>EUR</td>\n",
       "      <td>XETR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAA</td>\n",
       "      <td>BetaShares Australian High Interest Cash ETF</td>\n",
       "      <td>AUD</td>\n",
       "      <td>ASX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAU</td>\n",
       "      <td>Perth Mint Physical Gold ETF</td>\n",
       "      <td>USD</td>\n",
       "      <td>NYSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AADR</td>\n",
       "      <td>AdvisorShares Dorsey Wright ADR ETF</td>\n",
       "      <td>USD</td>\n",
       "      <td>NYSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AASF</td>\n",
       "      <td>Airlie Australian Share Fund -- ETF Feeder</td>\n",
       "      <td>AUD</td>\n",
       "      <td>ASX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol                                          name currency exchange\n",
       "0   8PSG                     Invesco Physical Gold ETC      EUR     XETR\n",
       "1    AAA  BetaShares Australian High Interest Cash ETF      AUD      ASX\n",
       "2   AAAU                  Perth Mint Physical Gold ETF      USD     NYSE\n",
       "3   AADR           AdvisorShares Dorsey Wright ADR ETF      USD     NYSE\n",
       "4   AASF    Airlie Australian Share Fund -- ETF Feeder      AUD      ASX"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etf_data = td.get_etf_list()\n",
    "etf_list = etf_data.as_json()\n",
    "etf_df = pd.DataFrame(etf_list)\n",
    "etf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get ETF Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(2020, 4, 15), datetime.date(2021, 4, 2))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_date = pd.Timestamp(dt.datetime.today())\n",
    "start_date = end_date - pd.tseries.offsets.BDay(252)\n",
    "\n",
    "start_date.to_pydatetime().date(), end_date.to_pydatetime().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>2.400000e+02</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.532337</td>\n",
       "      <td>318.708476</td>\n",
       "      <td>314.401410</td>\n",
       "      <td>316.757663</td>\n",
       "      <td>3.385244e+06</td>\n",
       "      <td>314.852005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.684477</td>\n",
       "      <td>30.533911</td>\n",
       "      <td>30.831302</td>\n",
       "      <td>30.738751</td>\n",
       "      <td>1.376637e+06</td>\n",
       "      <td>30.938463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>250.960010</td>\n",
       "      <td>255.490010</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.960010</td>\n",
       "      <td>7.530980e+05</td>\n",
       "      <td>250.599230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>294.715005</td>\n",
       "      <td>296.435857</td>\n",
       "      <td>293.557575</td>\n",
       "      <td>295.067500</td>\n",
       "      <td>2.356972e+06</td>\n",
       "      <td>291.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>314.860000</td>\n",
       "      <td>317.341200</td>\n",
       "      <td>313.375000</td>\n",
       "      <td>315.255005</td>\n",
       "      <td>3.077382e+06</td>\n",
       "      <td>313.968755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>342.612498</td>\n",
       "      <td>345.372500</td>\n",
       "      <td>340.907497</td>\n",
       "      <td>342.372490</td>\n",
       "      <td>4.080284e+06</td>\n",
       "      <td>340.965635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>366.205990</td>\n",
       "      <td>368.290010</td>\n",
       "      <td>366.030000</td>\n",
       "      <td>368.160000</td>\n",
       "      <td>8.397805e+06</td>\n",
       "      <td>363.463540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open        high         low       close        volume  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  2.400000e+02   \n",
       "mean   316.532337  318.708476  314.401410  316.757663  3.385244e+06   \n",
       "std     30.684477   30.533911   30.831302   30.738751  1.376637e+06   \n",
       "min    250.960010  255.490010  250.000000  250.960010  7.530980e+05   \n",
       "25%    294.715005  296.435857  293.557575  295.067500  2.356972e+06   \n",
       "50%    314.860000  317.341200  313.375000  315.255005  3.077382e+06   \n",
       "75%    342.612498  345.372500  340.907497  342.372490  4.080284e+06   \n",
       "max    366.205990  368.290010  366.030000  368.160000  8.397805e+06   \n",
       "\n",
       "              ema  \n",
       "count  240.000000  \n",
       "mean   314.852005  \n",
       "std     30.938463  \n",
       "min    250.599230  \n",
       "25%    291.208500  \n",
       "50%    313.968755  \n",
       "75%    340.965635  \n",
       "max    363.463540  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = \"VOO\"\n",
    "ts = td.time_series(\n",
    "    symbol=ticker, \n",
    "    interval=\"1day\",\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    outputsize=300\n",
    ")\n",
    "\n",
    "df = ts.with_ema().as_pandas()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>366.20599</td>\n",
       "      <td>368.29001</td>\n",
       "      <td>366.03000</td>\n",
       "      <td>368.16000</td>\n",
       "      <td>4591212</td>\n",
       "      <td>363.46354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>362.85999</td>\n",
       "      <td>365.82001</td>\n",
       "      <td>362.85999</td>\n",
       "      <td>364.29001</td>\n",
       "      <td>4870674</td>\n",
       "      <td>362.28942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>363.79001</td>\n",
       "      <td>363.79001</td>\n",
       "      <td>361.28500</td>\n",
       "      <td>363.00000</td>\n",
       "      <td>3637520</td>\n",
       "      <td>361.78927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>362.66000</td>\n",
       "      <td>364.67001</td>\n",
       "      <td>361.10971</td>\n",
       "      <td>363.79001</td>\n",
       "      <td>3062900</td>\n",
       "      <td>361.48659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>359.42999</td>\n",
       "      <td>364.35001</td>\n",
       "      <td>358.75000</td>\n",
       "      <td>363.95999</td>\n",
       "      <td>3212525</td>\n",
       "      <td>360.91074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    datetime       open       high        low      close   volume        ema\n",
       "0 2021-04-01  366.20599  368.29001  366.03000  368.16000  4591212  363.46354\n",
       "1 2021-03-31  362.85999  365.82001  362.85999  364.29001  4870674  362.28942\n",
       "2 2021-03-30  363.79001  363.79001  361.28500  363.00000  3637520  361.78927\n",
       "3 2021-03-29  362.66000  364.67001  361.10971  363.79001  3062900  361.48659\n",
       "4 2021-03-26  359.42999  364.35001  358.75000  363.95999  3212525  360.91074"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure Workspace Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload ETF Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-26 04:00:00</td>\n",
       "      <td>359.42999</td>\n",
       "      <td>364.35001</td>\n",
       "      <td>358.75000</td>\n",
       "      <td>363.95999</td>\n",
       "      <td>3212525</td>\n",
       "      <td>360.91074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-25 04:00:00</td>\n",
       "      <td>357.42001</td>\n",
       "      <td>360.23999</td>\n",
       "      <td>354.14001</td>\n",
       "      <td>359.47000</td>\n",
       "      <td>5361270</td>\n",
       "      <td>360.14842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-24 04:00:00</td>\n",
       "      <td>360.70999</td>\n",
       "      <td>362.26999</td>\n",
       "      <td>357.44000</td>\n",
       "      <td>357.57999</td>\n",
       "      <td>3989728</td>\n",
       "      <td>360.31803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-23 04:00:00</td>\n",
       "      <td>359.79501</td>\n",
       "      <td>362.51001</td>\n",
       "      <td>359.79501</td>\n",
       "      <td>362.32001</td>\n",
       "      <td>1208455</td>\n",
       "      <td>361.00254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-22 04:00:00</td>\n",
       "      <td>359.88000</td>\n",
       "      <td>363.50000</td>\n",
       "      <td>359.76999</td>\n",
       "      <td>362.10999</td>\n",
       "      <td>3320390</td>\n",
       "      <td>360.67317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime       open       high        low      close   volume  \\\n",
       "0 2021-03-26 04:00:00  359.42999  364.35001  358.75000  363.95999  3212525   \n",
       "1 2021-03-25 04:00:00  357.42001  360.23999  354.14001  359.47000  5361270   \n",
       "2 2021-03-24 04:00:00  360.70999  362.26999  357.44000  357.57999  3989728   \n",
       "3 2021-03-23 04:00:00  359.79501  362.51001  359.79501  362.32001  1208455   \n",
       "4 2021-03-22 04:00:00  359.88000  363.50000  359.76999  362.10999  3320390   \n",
       "\n",
       "         ema  \n",
       "0  360.91074  \n",
       "1  360.14842  \n",
       "2  360.31803  \n",
       "3  361.00254  \n",
       "4  360.67317  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_or_upload_df(ws, data_store, df, ticker):\n",
    "    \n",
    "    dataset_name = f'{ticker.lower()}_ds'\n",
    "    try: \n",
    "        ds = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "        df = ds.to_pandas_dataframe()\n",
    "    except:\n",
    "        Dataset.Tabular.register_pandas_dataframe(df, data_store, dataset_name)\n",
    "        ds = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "        df = ds.to_pandas_dataframe()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "aml_df = get_or_upload_df(ws, data_store, df.reset_index(), ticker)\n",
    "aml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'aml-exp'\n",
    "aml_exp = Path(src_dir)\n",
    "if not aml_exp.exists(): aml_exp.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aml-exp/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile aml-exp/train.py\n",
    "\n",
    "# Standard Libraries\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# 3rd Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core import Dataset\n",
    "from azureml.core import Model\n",
    "\n",
    "from azureml.tensorboard.export import export_to_tensorboard\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Classes \n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "               train_df, val_df, test_df,\n",
    "               label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    def plot(self, plot_col, model=None, max_subplots=3):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        inputs, labels = self.example\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                          marker='X', edgecolors='k', label='Predictions',\n",
    "                          c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time')\n",
    "        \n",
    "        \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "          data=data,\n",
    "          targets=None,\n",
    "          sequence_length=self.total_window_size,\n",
    "          sequence_stride=1,\n",
    "          shuffle=True,\n",
    "          batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, label_index=None):\n",
    "        super().__init__()\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            return inputs\n",
    "        result = inputs[:, :, self.label_index]\n",
    "        return result[:, :, tf.newaxis]\n",
    "    \n",
    "# Global Variables\n",
    "MAX_EPOCHS = 20\n",
    "CONV_WIDTH = 3\n",
    "\n",
    "# Read in Args\n",
    "parser = argparse.ArgumentParser(description='Train')\n",
    "parser.add_argument('--dataset_name', type=str, dest='dataset_name')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Paths\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "os.makedirs('./outputs/log', exist_ok=True)\n",
    "\n",
    "\n",
    "# ML Run\n",
    "run = Run.get_context()\n",
    "workspace = run.experiment.workspace\n",
    "\n",
    "\n",
    "# ML Dataset\n",
    "ds = Dataset.get_by_name(workspace=workspace, name=args.dataset_name)\n",
    "df = ds.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "# Date Feature Prep\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "date_time = pd.to_datetime(df.datetime)\n",
    "timestamp_s = date_time.map(dt.datetime.timestamp)\n",
    "\n",
    "df['day_sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "df['day_cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "df['year_sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "df['year_cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "\n",
    "# Data Filter\n",
    "features = ['day_sin', 'day_cos', 'ema']\n",
    "target = 'close'\n",
    "columns = features + [target]\n",
    "df = df[columns]\n",
    "\n",
    "# Data Splitting\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.6)]\n",
    "val_df = df[int(n*0.6):int(n*0.8)]\n",
    "test_df = df[int(n*0.8):]\n",
    "\n",
    "\n",
    "# Data Normalization\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "\n",
    "# Data Distribution Check\n",
    "df_std = (df - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(df.keys(), rotation=45)\n",
    "run.log_image('feature_distribution_check', plot=plt)\n",
    "\n",
    "\n",
    "# Data Windows\n",
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    label_columns=[target])\n",
    "wide_window = WindowGenerator(\n",
    "    input_width=24, label_width=24, shift=1,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    label_columns=[target])\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1, shift=1,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    label_columns=[target])\n",
    "wide_conv_window = WindowGenerator(\n",
    "    input_width=24 + CONV_WIDTH - 1,\n",
    "    label_width=24, shift=1,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    label_columns=[target])\n",
    "\n",
    "\n",
    "# Train Baseline\n",
    "baseline = Baseline(label_index=single_step_window.column_indices.get(target))\n",
    "baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "val_performance, tst_performance = {}, {}\n",
    "val_performance['baseline'] = baseline.evaluate(single_step_window.val)\n",
    "tst_performance['baseline'] = baseline.evaluate(single_step_window.test, verbose=0)\n",
    "\n",
    "wide_window.plot(target, baseline)\n",
    "run.log_image('baseline_pred', plot=plt)\n",
    "\n",
    "# Train Models\n",
    "def compile_and_fit(model, window, patience=4):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "    return history\n",
    "\n",
    "# Train Linear Model\n",
    "linear = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "history = compile_and_fit(linear, single_step_window)\n",
    "\n",
    "val_performance['linear'] = linear.evaluate(single_step_window.val)\n",
    "tst_performance['linear'] = linear.evaluate(single_step_window.test, verbose=0)\n",
    "\n",
    "tf.saved_model.save(linear, './outputs/model/linear')\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax = fig1.add_subplot(111)\n",
    "ax.bar(x = range(len(train_df.columns)),\n",
    "        height=linear.layers[0].kernel[:,0].numpy())\n",
    "ax.set_xticks(range(len(train_df.columns)))\n",
    "_ = ax.set_xticklabels(train_df.columns, rotation=45)\n",
    "run.log_image('linear_coef', plot=plt)\n",
    "\n",
    "wide_window.plot(target, linear)\n",
    "run.log_image('linear_pred', plot=plt)\n",
    "\n",
    "# Train Single Step Dense Model\n",
    "single_step_dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "history = compile_and_fit(single_step_dense, single_step_window)\n",
    "\n",
    "val_performance['single_step_dense'] = single_step_dense.evaluate(single_step_window.val)\n",
    "tst_performance['single_step_dense'] = single_step_dense.evaluate(single_step_window.test, verbose=0)\n",
    "\n",
    "tf.saved_model.save(single_step_dense, './outputs/model/single_step_dense')\n",
    "\n",
    "wide_window.plot(target, single_step_dense)\n",
    "run.log_image('single_step_dense_pred', plot=plt)\n",
    "\n",
    "# Train Multi Step Dense Model\n",
    "multi_step_dense = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "    tf.keras.layers.Reshape([1, -1]),\n",
    "])\n",
    "history = compile_and_fit(multi_step_dense, conv_window)\n",
    "\n",
    "val_performance['multi_step_dense'] = multi_step_dense.evaluate(conv_window.val)\n",
    "tst_performance['multi_step_dense'] = multi_step_dense.evaluate(conv_window.test, verbose=0)\n",
    "\n",
    "tf.saved_model.save(multi_step_dense, './outputs/model/multi_step_dense')\n",
    "\n",
    "conv_window.plot(target, multi_step_dense)\n",
    "run.log_image('multi_step_dense_pred', plot=plt)\n",
    "\n",
    "# Train Conv Model\n",
    "conv = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,\n",
    "                           kernel_size=(CONV_WIDTH,),\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "history = compile_and_fit(conv, conv_window)\n",
    "# TODO - log training epoch history to tesnorboard\n",
    "\n",
    "val_performance['conv'] = conv.evaluate(conv_window.val)\n",
    "tst_performance['conv'] = conv.evaluate(conv_window.test, verbose=0)\n",
    "\n",
    "tf.saved_model.save(conv, './outputs/model/conv')\n",
    "\n",
    "wide_conv_window.plot(target, conv)\n",
    "run.log_image('conv_pred', plot=plt)\n",
    "\n",
    "# Train LSTM Model\n",
    "lstm = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(10, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "history = compile_and_fit(lstm, wide_window)\n",
    "\n",
    "val_performance['lstm'] = lstm.evaluate(wide_window.val)\n",
    "tst_performance['lstm'] = lstm.evaluate(wide_window.test, verbose=0)\n",
    "\n",
    "tf.saved_model.save(lstm, './outputs/model/lstm')\n",
    "\n",
    "wide_window.plot(target, lstm)\n",
    "run.log_image('lstm_pred', plot=plt)\n",
    "\n",
    "# Performance\n",
    "x = np.arange(len(val_performance))\n",
    "width = 0.3\n",
    "\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = lstm.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in tst_performance.values()]\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax = fig2.add_subplot(111)\n",
    "b1 = ax.bar(x - 0.2, val_mae, width, label='validation')\n",
    "b2 = ax.bar(x + 0.2, test_mae, width, label='test')\n",
    "ax.set_xticks(range(len(val_mae)))\n",
    "_ = ax.set_xticklabels(val_performance.keys(), rotation=45)\n",
    "run.log_image('performance_mae', plot=plt)\n",
    "\n",
    "\n",
    "# Log Results & Select Best Model\n",
    "best_model, best_score = None, None\n",
    "if run is not None:\n",
    "    \n",
    "    for k, v in val_performance.items():\n",
    "        run.log_list(f'val_{k}', v)\n",
    "        \n",
    "    for k, v in tst_performance.items():\n",
    "        run.log_list(f'tst_{k}', v)\n",
    "        try:\n",
    "            mae = float(v[1])    \n",
    "            if best_score is None and best_model is None: \n",
    "                best_model = k\n",
    "                best_score = mae\n",
    "            elif best_score > mae:\n",
    "                best_model = k\n",
    "                best_score = mae   \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    run.log('best_model', best_model)\n",
    "    run.log('best_score', best_score)\n",
    "\n",
    "if best_model != \"baseline\": model = run.register_model(model_name=best_model, model_path=f'outputs/model/{best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - break train.py into at least one more script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Training Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = compute_target\n",
    "\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Add some packages relied on by data prep step\n",
    "deps = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn', 'matplotlib', 'seaborn'], \n",
    "    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]', \n",
    "                  'azureml-pipeline', 'azureml.tensorboard', 'azureml-interpret'], \n",
    "    python_version='3.6.2',\n",
    "    pin_sdk_version=True)\n",
    "deps.add_tensorflow_pip_package(core_type='gpu', version='2.3.1')\n",
    "aml_run_config.environment.python.conda_dependencies = deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name=\"train_step\",\n",
    "                         source_directory=src_dir,\n",
    "                         script_name=\"train.py\", \n",
    "                         arguments=['--dataset_name', f'{ticker.lower()}_ds'],\n",
    "                         runconfig=aml_run_config, \n",
    "                         allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [step1]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "experiment = Experiment(ws, 'aml_exp')\n",
    "script_run = experiment.submit(pipeline)\n",
    "script_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1730b25471643b9879efa9c106f497d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/0e922ade-bbce-44c9-94c1-33ebdb17e44f?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&tid=e6777dcd-6f87-4dd0-92e5-e98312157dac\", \"run_id\": \"0e922ade-bbce-44c9-94c1-33ebdb17e44f\", \"run_properties\": {\"run_id\": \"0e922ade-bbce-44c9-94c1-33ebdb17e44f\", \"created_utc\": \"2021-04-02T20:15:57.154569Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2021-04-02T20:34:09.757305Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://minionlab0571264079.blob.core.windows.net/azureml/ExperimentRun/dcid.0e922ade-bbce-44c9-94c1-33ebdb17e44f/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=lmpoTgVmvgJq9Ow7T%2FrY%2BkdXOc2BBrxBUph2JWkxYeY%3D&st=2021-04-04T15%3A45%3A05Z&se=2021-04-04T23%3A55%3A05Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://minionlab0571264079.blob.core.windows.net/azureml/ExperimentRun/dcid.0e922ade-bbce-44c9-94c1-33ebdb17e44f/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=QBEBGQ3kJo9dLghpe0j4ZnZfak%2BqaUFqfQsL0HBl9AQ%3D&st=2021-04-04T15%3A45%3A05Z&se=2021-04-04T23%3A55%3A05Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://minionlab0571264079.blob.core.windows.net/azureml/ExperimentRun/dcid.0e922ade-bbce-44c9-94c1-33ebdb17e44f/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=iibgqye025UTJvpqz5UeH3e%2FVKwft43hR185CPscg8U%3D&st=2021-04-04T15%3A45%3A05Z&se=2021-04-04T23%3A55%3A05Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:18:12\", \"run_number\": \"42\", \"run_queued_details\": {\"status\": \"Finished\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"d6af5866-c862-4fd9-afe6-6ed590bdc74a\", \"name\": \"train_step\", \"status\": \"Finished\", \"start_time\": \"2021-04-02T20:31:18.622275Z\", \"created_time\": \"2021-04-02T20:16:00.414708Z\", \"end_time\": \"2021-04-02T20:34:05.421824Z\", \"duration\": \"0:18:05\", \"run_number\": 43, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-04-02T20:16:00.414708Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2021-04-02 20:16:00Z] Submitting 1 runs, first five are: a3314772:d6af5866-c862-4fd9-afe6-6ed590bdc74a\\n[2021-04-02 20:34:08Z] Completing processing run id d6af5866-c862-4fd9-afe6-6ed590bdc74a.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {}, \"module_nodes\": {\"a3314772\": {\"node_id\": \"a3314772\", \"name\": \"train_step\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"d6af5866-c862-4fd9-afe6-6ed590bdc74a\"}}, \"edges\": [], \"child_runs\": [{\"run_id\": \"d6af5866-c862-4fd9-afe6-6ed590bdc74a\", \"name\": \"train_step\", \"status\": \"Finished\", \"start_time\": \"2021-04-02T20:31:18.622275Z\", \"created_time\": \"2021-04-02T20:16:00.414708Z\", \"end_time\": \"2021-04-02T20:34:05.421824Z\", \"duration\": \"0:18:05\", \"run_number\": 43, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-04-02T20:16:00.414708Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.25.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(script_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it best to simply go to the experiment portal url to review from the gui. It contains all the runs from your experiment and makes it easy to review changes from a central location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/0e922ade-bbce-44c9-94c1-33ebdb17e44f?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&tid=e6777dcd-6f87-4dd0-92e5-e98312157dac'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_run.get_portal_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can choose to do the model review inside the notebook too. \n",
    "\n",
    "The first place to look when doing this is the experiments metrics. In this example I'm logging the mse and mae for validation & test datasets for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_name = 'train_step'\n",
    "step = script_run.find_step_run(step_name)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_step_dense: 0.1610\n",
      "\n",
      "\u001b[1mModel                        Val      Tst\u001b[0m\n",
      "baseline                 : 0.1176 | 0.1872\n",
      "conv                     : 0.0875 | 0.1773\n",
      "linear                   : 3.0643 | 4.7024\n",
      "lstm                     : 0.9358 | 2.6143\n",
      "multi_step_dense         : 0.1977 | 0.6740\n",
      "single_step_dense        : 0.1043 | 0.1610\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = step.get_metrics()\n",
    "\n",
    "print(f\"{metrics_dict.get('best_model')}: {metrics_dict.get('best_score'):0.4f}\")\n",
    "print()\n",
    "print('\\033[1m' + f\"Model                        Val      Tst\" + '\\033[0m')\n",
    "metrics_list = list(filter(lambda v: isinstance(v[1], list), metrics_dict.items()))\n",
    "\n",
    "mae_metrics = []\n",
    "\n",
    "for name, values in metrics_list:\n",
    "    splits = name.split(\"_\")\n",
    "    grp, model = splits[0], \"_\".join(splits[1:])\n",
    "    mae_metrics.append((model, grp, values[1]))\n",
    "    \n",
    "mae_metrics = list(sorted(mae_metrics, key=lambda o: o[0]))\n",
    "\n",
    "for cur, nxt in zip(mae_metrics[0::2], mae_metrics[1::2]):\n",
    "    name_1, _, value_1 = cur\n",
    "    name_2, _, value_2 = nxt\n",
    "    assert name_1 == name_2\n",
    "    print(f'{name_1:25s}: {value_1:0.4f} | {value_2:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Stored Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be useful to review the log files to figure out wtf is going wrong constantly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/20_image_build_log.txt',\n",
       " 'azureml-logs/55_azureml-execution-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt',\n",
       " 'azureml-logs/process_info.json',\n",
       " 'azureml-logs/process_status.json',\n",
       " 'baseline_pred_1617395547.png',\n",
       " 'conv_pred_1617395562.png',\n",
       " 'feature_distribution_check_1617395545.png',\n",
       " 'linear_coef_1617395551.png',\n",
       " 'linear_pred_1617395551.png',\n",
       " 'logs/azureml/106_azureml.log',\n",
       " 'logs/azureml/dataprep/backgroundProcess.log',\n",
       " 'logs/azureml/dataprep/backgroundProcess_Telemetry.log',\n",
       " 'logs/azureml/executionlogs.txt',\n",
       " 'logs/azureml/job_prep_azureml.log',\n",
       " 'logs/azureml/job_release_azureml.log',\n",
       " 'logs/azureml/stderrlogs.txt',\n",
       " 'logs/azureml/stdoutlogs.txt',\n",
       " 'lstm_pred_1617395572.png',\n",
       " 'multi_step_dense_pred_1617395558.png',\n",
       " 'outputs/model/conv/saved_model.pb',\n",
       " 'outputs/model/conv/variables/variables.data-00000-of-00001',\n",
       " 'outputs/model/conv/variables/variables.index',\n",
       " 'outputs/model/linear/saved_model.pb',\n",
       " 'outputs/model/linear/variables/variables.data-00000-of-00001',\n",
       " 'outputs/model/linear/variables/variables.index',\n",
       " 'outputs/model/lstm/saved_model.pb',\n",
       " 'outputs/model/lstm/variables/variables.data-00000-of-00001',\n",
       " 'outputs/model/lstm/variables/variables.index',\n",
       " 'outputs/model/multi_step_dense/saved_model.pb',\n",
       " 'outputs/model/multi_step_dense/variables/variables.data-00000-of-00001',\n",
       " 'outputs/model/multi_step_dense/variables/variables.index',\n",
       " 'outputs/model/single_step_dense/saved_model.pb',\n",
       " 'outputs/model/single_step_dense/variables/variables.data-00000-of-00001',\n",
       " 'outputs/model/single_step_dense/variables/variables.index',\n",
       " 'performance_mae_1617395572.png',\n",
       " 'single_step_dense_pred_1617395555.png']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = step.get_file_names()\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Compute Cluster\n",
    "This is an important step if you don't want save some money 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting compute cleanup\n",
      "deleting aml-compute instance\n",
      "compute cleanup complete\n"
     ]
    }
   ],
   "source": [
    "print(\"starting compute cleanup\")\n",
    "\n",
    "for name, compute in ws.compute_targets.items():\n",
    "    print(f\"deleting {name} instance\")\n",
    "    compute.delete()\n",
    "    \n",
    "while len(ws.compute_targets.items()) != 0:\n",
    "    continue\n",
    "\n",
    "print(\"compute cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
