{
  
    
        "post0": {
            "title": "City Division EDA",
            "content": "Objectives . The objectives of this analysis are: . Characterize the expenditures of the City’s Divisions or Cost Centres by establishing the number and frequency of transactions, their typical amounts, dispersion, etc. | Identify meaningful groups of Divisions or Cost Centres that behave similarly in terms of their expense behaviour. | Identify meaningful anomalies in the data that may require closer examination. | . Notebook Setup . %load_ext lab_black %load_ext autoreload %autoreload 2 %matplotlib inline %config Completer.use_jedi = False . Libraries . import io import requests import functools from pathlib import Path . import klib import stumpy import numpy as np import pandas as pd import zipfile as zp import matplotlib as mpl import matplotlib.pyplot as plt print(f&quot;{np.__version__=}&quot;) print(f&quot;{pd.__version__=}&quot;) print(f&quot;{klib.__version__=}&quot;) print(f&quot;{mpl.__version__=}&quot;) print(f&quot;{stumpy.__version__=}&quot;) . np.__version__=&#39;1.20.3&#39; pd.__version__=&#39;1.2.3&#39; klib.__version__=&#39;0.1.5&#39; mpl.__version__=&#39;3.3.4&#39; stumpy.__version__=&#39;1.8.0&#39; . from string_grouper import group_similar_strings . Global Variables . data_dir = Path(&quot;data&quot;) data_dir.absolute() . WindowsPath(&#39;C:/Users/kslad/Documents/Projects/kslader8-thoughts/_notebooks/data&#39;) . Data . Loading . meta_data_req = ( &quot;https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action/package_show&quot;, {&quot;id&quot;: &quot;ebc3f9c2-2f80-4405-bf4f-5fb309581485&quot;}, ) . meta_payload = requests.get(*meta_data_req).json() meta_payload[&quot;result&quot;] . {&#39;license_title&#39;: &#39;Open Government Licence – Toronto&#39;, &#39;owner_unit&#39;: None, &#39;relationships_as_object&#39;: [], &#39;topics&#39;: &#39;City government,Finance&#39;, &#39;owner_email&#39;: &#39;pcard@toronto.ca&#39;, &#39;excerpt&#39;: &#39;The dataset contains details of all purchases made by City staff members using City-issued credit cards also referred to as PCards or purchasing cards. &#39;, &#39;private&#39;: False, &#39;owner_division&#39;: &#39;Accounting Services&#39;, &#39;num_tags&#39;: 7, &#39;id&#39;: &#39;ebc3f9c2-2f80-4405-bf4f-5fb309581485&#39;, &#39;metadata_created&#39;: &#39;2019-07-23T17:52:47.150105&#39;, &#39;refresh_rate&#39;: &#39;Monthly&#39;, &#39;title&#39;: &#39;PCard Expenditures&#39;, &#39;license_url&#39;: &#39;https://open.toronto.ca/open-data-license/&#39;, &#39;state&#39;: &#39;active&#39;, &#39;information_url&#39;: &#39;http://www.toronto.ca/finance/index.htm&#39;, &#39;license_id&#39;: &#39;open-government-licence-toronto&#39;, &#39;type&#39;: &#39;dataset&#39;, &#39;resources&#39;: [{&#39;cache_last_updated&#39;: None, &#39;package_id&#39;: &#39;ebc3f9c2-2f80-4405-bf4f-5fb309581485&#39;, &#39;datastore_active&#39;: False, &#39;id&#39;: &#39;c3a307f6-09c1-4db4-b7de-c35a53344a65&#39;, &#39;size&#39;: 26624, &#39;format&#39;: &#39;XLS&#39;, &#39;state&#39;: &#39;active&#39;, &#39;hash&#39;: &#39;&#39;, &#39;description&#39;: &#39;&#39;, &#39;is_preview&#39;: False, &#39;last_modified&#39;: &#39;2019-07-24T20:25:05.559452&#39;, &#39;url_type&#39;: &#39;upload&#39;, &#39;mimetype&#39;: &#39;application/vnd.ms-excel&#39;, &#39;cache_url&#39;: None, &#39;extract_job&#39;: None, &#39;name&#39;: &#39;readme-file&#39;, &#39;created&#39;: &#39;2019-07-23T17:52:47.544377&#39;, &#39;url&#39;: &#39;https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/ebc3f9c2-2f80-4405-bf4f-5fb309581485/resource/c3a307f6-09c1-4db4-b7de-c35a53344a65/download/pcard_expenditures_readme.xls&#39;, &#39;mimetype_inner&#39;: None, &#39;position&#39;: 0, &#39;revision_id&#39;: &#39;2b64cfe2-0a96-4243-9eff-f27237c602c2&#39;, &#39;resource_type&#39;: None}, {&#39;cache_last_updated&#39;: None, &#39;package_id&#39;: &#39;ebc3f9c2-2f80-4405-bf4f-5fb309581485&#39;, &#39;datastore_active&#39;: False, &#39;id&#39;: &#39;6b394ed0-1de6-403b-a907-a3608509e300&#39;, &#39;size&#39;: 46902535, &#39;format&#39;: &#39;ZIP&#39;, &#39;state&#39;: &#39;active&#39;, &#39;hash&#39;: &#39;&#39;, &#39;description&#39;: &#39;&#39;, &#39;is_preview&#39;: False, &#39;last_modified&#39;: &#39;2019-09-03T19:44:00&#39;, &#39;url_type&#39;: &#39;upload&#39;, &#39;mimetype&#39;: &#39;application/zip&#39;, &#39;cache_url&#39;: None, &#39;extract_job&#39;: &#39;Airflow: upload_remote_files&#39;, &#39;name&#39;: &#39;pcard-expenses&#39;, &#39;created&#39;: &#39;2020-12-07T21:47:55.929457&#39;, &#39;url&#39;: &#39;https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/ebc3f9c2-2f80-4405-bf4f-5fb309581485/resource/6b394ed0-1de6-403b-a907-a3608509e300/download/expenditures.zip&#39;, &#39;mimetype_inner&#39;: None, &#39;position&#39;: 1, &#39;revision_id&#39;: &#39;84e77a3a-7692-4696-8297-6d029c4a033a&#39;, &#39;resource_type&#39;: None}], &#39;limitations&#39;: None, &#39;num_resources&#39;: 2, &#39;collection_method&#39;: None, &#39;tags&#39;: [{&#39;vocabulary_id&#39;: None, &#39;state&#39;: &#39;active&#39;, &#39;display_name&#39;: &#39;council spending&#39;, &#39;id&#39;: &#39;8050749a-8ee9-4858-a640-50ef88ce9a73&#39;, &#39;name&#39;: &#39;council spending&#39;}, {&#39;vocabulary_id&#39;: None, &#39;state&#39;: &#39;active&#39;, &#39;display_name&#39;: &#39;pcard&#39;, &#39;id&#39;: &#39;a8554c91-fbf8-4faf-b774-ab2ff619d448&#39;, &#39;name&#39;: &#39;pcard&#39;}], &#39;is_retired&#39;: False, &#39;groups&#39;: [], &#39;creator_user_id&#39;: &#39;150d5301-86ec-44a3-a070-50f2cea839c9&#39;, &#39;dataset_category&#39;: &#39;Document&#39;, &#39;relationships_as_subject&#39;: [], &#39;name&#39;: &#39;pcard-expenditures&#39;, &#39;metadata_modified&#39;: &#39;2020-12-07T21:47:57.302587&#39;, &#39;isopen&#39;: False, &#39;url&#39;: None, &#39;notes&#39;: &#39; n nSee README file (PCard Expenditures Readme.xls) n nCouncil PCard expenses are not included in this report. Councillor expenses are disclosed separately through the quarterly Councillor Office Expense Reports. n nPCard expenses for the Offices of the Auditor General, Integrity Commissioner, Lobbyist Registrar and Ombudsman are not included as part of this report. PCard expenses for these offices are posted separately on the websites of the Offices of the Auditor General, Integrity Commissioner, Lobbyist Registrar and Ombudsman. n&#39;, &#39;owner_org&#39;: &#39;95a064ae-77e8-4ef0-a4e3-4e2d43e1f066&#39;, &#39;last_refreshed&#39;: &#39;2019-09-03T19:44:00.000000&#39;, &#39;image_url&#39;: None, &#39;formats&#39;: &#39;XLS,ZIP&#39;, &#39;owner_section&#39;: None, &#39;organization&#39;: {&#39;description&#39;: &#39;&#39;, &#39;created&#39;: &#39;2019-07-23T15:51:35.236542&#39;, &#39;title&#39;: &#39;City of Toronto&#39;, &#39;name&#39;: &#39;city-of-toronto&#39;, &#39;is_organization&#39;: True, &#39;state&#39;: &#39;active&#39;, &#39;image_url&#39;: &#39;&#39;, &#39;revision_id&#39;: &#39;62817a72-3dc5-450a-8cc4-fef2c51660da&#39;, &#39;type&#39;: &#39;organization&#39;, &#39;id&#39;: &#39;95a064ae-77e8-4ef0-a4e3-4e2d43e1f066&#39;, &#39;approval_status&#39;: &#39;approved&#39;}, &#39;revision_id&#39;: &#39;2b64cfe2-0a96-4243-9eff-f27237c602c2&#39;, &#39;civic_issues&#39;: &#39;Fiscal responsibility&#39;} . resources_list = [ resource.get(&quot;url&quot;) for resource in meta_payload.get(&quot;result&quot;, {}).get(&quot;resources&quot;, {}) ] resources_dict = {resource.split(&quot;&quot;&quot;/&quot;&quot;&quot;)[-1]: resource for resource in resources_list} resources_dict.keys() . dict_keys([&#39;pcard_expenditures_readme.xls&#39;, &#39;expenditures.zip&#39;]) . pd.read_excel(resources_dict.get(&quot;pcard_expenditures_readme.xls&quot;)).dropna() . PCARD EXPENDITURES Unnamed: 1 . 1 Field | Description | . 2 DIVISION | Name of City of Toronto division | . 3 BATCH-TRANSACTION ID | Unique identifier assigned by credit card company | . 4 TRANSACTION DATE | Date of purchase | . 5 CARD POSTING DATE | Date purchase posted by credit card company | . 6 MERCHANT NAME | Name of vendor where purchase was made | . 7 TRANSACTION AMOUNT | Amount of purchase in Canadian currency (inclu... | . 8 TRANSACTION CURRENCY | Currency of country | . 9 ORIGINAL AMOUNT | Amount of Purchase in the currency of purchase... | . 10 ORIGINAL CURRENCY | Currency in which purchase was made | . 11 G/L ACCOUNT | Description of the type of expense made and to... | . 12 G/L ACCOUNT DESCRIPTION | Long text description of the expense | . 13 COST CENTRE/WBS ELEMENT/ORDER | Represents the Organization Unit or the &quot;where... | . 14 COST CENTRE/WBS ELEMENT/ORDER DESCRIPTION | Description of the Organization Unit or the &quot;w... | . 15 MERCHANT TYPE (MCC) | A four-digit number assigned by MasterCard and... | . 16 MERCHANT TYPE DESCRIPTION | Description of the MCC assigned to the merchant | . 17 PURPOSE | Free-form text describing purchase by the card... | . expenditures_dir = data_dir / &quot;expenditures&quot; r = requests.get(resources_dict.get(&quot;expenditures.zip&quot;)) z = zp.ZipFile(io.BytesIO(r.content)) z.extractall(expenditures_dir) . list(expenditures_dir.glob(&quot;*&quot;)) . [WindowsPath(&#39;data/expenditures/PCard Expenses_201706.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_201707.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_201708.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_201709.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_201710.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2017_11_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2017_12_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_01_Final_revised.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_02_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_03_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_04_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_05_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_06_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_07.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_08.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_09.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_10.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_11.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2018_12.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2019_01.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2019_02.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2019_03.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2019_04.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2019_05.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2019_06.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCard Expenses_2019_07.FINAL.XLSX&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201101_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201102_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201103_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201104_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201105_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201106_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201107_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201108_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201109_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201110_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201111_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201112_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201201_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201202_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201203_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201204_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201205_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201206_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201207_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201208_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201209_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201210_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201211_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201212_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201301_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201302_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201303_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201304_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201305_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201306_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201307_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201308_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201309_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201310_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201311_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201312_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201401_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201402_final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201403_final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201404_final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201405_final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201406_final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201407_final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201408_final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201409_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201410_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201411_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201412_Final.xls&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201501_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201502_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201503_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201504_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201505_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201506_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201507_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201508_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201509_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201510_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201511_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201512_Final.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201601.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201602.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201603.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201604.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201605.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201606.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201607.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201608.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201609.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201610.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201611.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201612.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201701.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201702.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201703.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201704.xlsx&#39;), WindowsPath(&#39;data/expenditures/PCardExpenses_201705.xlsx&#39;)] . kb_conv = 1_024 mb_conv = kb_conv ** 2 gb_nv = kb_conv ** 3 bytes_size = expenditures_dir.stat().st_size f&quot;{bytes_size / mb_conv:,.3} MB&quot; . &#39;0.0469 MB&#39; . dfs = [pd.read_excel(xls) for xls in expenditures_dir.glob(&quot;*.xlsx&quot;)] aggr_df = pd.concat(dfs, ignore_index=True) . pd.concat( [aggr_df.head(1).T, aggr_df.sample(2).T, aggr_df.tail(1).T, aggr_df.dtypes], axis=1 ) . 0 296462 104882 297995 0 . Division PUBLIC HEALTH | PARKS, FORESTRY &amp; RECREATION | NaN | NaN | object | . Batch-Transaction ID 4608-1 | NaN | NaN | NaN | object | . Transaction Date 2017-06-15 00:00:00 | 2017-05-17 00:00:00 | NaT | NaT | datetime64[ns] | . Card Posting Dt 2017-06-16 00:00:00 | 2017-05-18 00:00:00 | NaT | NaT | datetime64[ns] | . Merchant Name PAYPAL *OBC2012 | COMFORT VACCUM SERVICE | NaN | NaN | object | . Transaction Amt. 50.0 | 721.52 | NaN | NaN | float64 | . Trx Currency CAD | CAD | NaN | NaN | object | . Original Amount 50.0 | 721.52 | 642.96 | 20729.48 | float64 | . Original Currency CAD | CAD | CAD | USD | object | . G/L Account 4760 | 2120 | NaN | NaN | object | . G/L Account Description MEMBERSHIP FEES | PARTS - MACHINERY &amp; EQUIPMENT | NaN | NaN | object | . Cost Centre /WBS Element / Order PH3071 | NaN | NaN | NaN | object | . Cost Centre /WBS Element / Order Description MATERNAL INFANT HEALTH PROGRAM SUPPORT | NaN | NaN | NaN | object | . Merchant Type 8641.0 | 5722.0 | NaN | NaN | float64 | . Merchant Type Description Associations - Civic, Social, and Frater | Household Appliance Stores | NaN | NaN | object | . Purpose MEMBERSHIP FEE | FLOOR MACHINE REPAIRS | NaN | NaN | object | . Unnamed: 16 NaN | NaN | NaN | NaN | float64 | . Batch Transaction ID NaN | 4565-171 | NaN | NaN | object | . Cost Center / WBS Element / Order NaN | NaN | NaN | NaN | object | . Cost Center / WBS Element / Order Description NaN | NaN | NaN | NaN | object | . Exp Type Desc NaN | NaN | NaN | NaN | object | . Division NaN | NaN | NaN | NaN | object | . Cost Center / WBLS Element / Order Description NaN | NaN | NaN | NaN | object | . Cost Center / WBS Element / Order # NaN | NaN | NaN | NaN | object | . Cost Center / WBS Element / Order # Description NaN | NaN | NaN | NaN | object | . Card Posting Date NaT | NaT | NaT | NaT | datetime64[ns] | . Transaction Amount NaN | NaN | 642.96 | NaN | float64 | . Transaction Currency NaN | NaN | CAD | NaN | object | . Merchant Type (MCC) NaN | NaN | NaN | NaN | float64 | . Trx.Currency NaN | NaN | NaN | NaN | object | . Cost Centre / WBS Element / Order No NaN | NaN | NaN | NaN | object | . Cost Centre / WBS Element / Order No. Description NaN | WALLACE EMERSON-OPERATIONS | NaN | NaN | object | . Cost Centre / WBS Element / Order No. NaN | P07728 | NaN | NaN | object | . Tr Currency NaN | NaN | NaN | NaN | object | . Cost Centre / WBS element / Order NaN | NaN | NaN | NaN | object | . Cost Centre / WBS element / Order Description NaN | NaN | NaN | NaN | object | . Cost Centre / WBS Element / Order No. NaN | NaN | NaN | NaN | object | . Cost Centre / WBS Element / Order No. Decription NaN | NaN | NaN | NaN | object | . Cost Centre / WBS Element / Order NaN | NaN | NaN | NaN | object | . Cost Centre / WBS Element / Order Description NaN | NaN | NaN | NaN | object | . Unnamed: 17 NaN | NaN | NaN | NaN | float64 | . Cost Centre / WBS Element / Order NaN | NaN | NaN | NaN | object | . Unnamed: 18 NaN | NaN | NaN | NaN | float64 | . Unnamed: 19 NaN | NaN | NaN | NaN | float64 | . Divison NaN | NaN | NaN | NaN | object | . Cost Centre/ WBS Element / Order NaN | NaN | NaN | NaN | object | . Cost Centre/ WBS Element / Order Description NaN | NaN | NaN | NaN | object | . Basic Preprocessing . clean and standardizes column names . aggr_df = klib.clean_column_names(aggr_df) aggr_df.columns . Duplicate column names detected! Columns with index [17, 21, 29, 32, 34, 35, 36, 38, 39, 41, 45, 46] and names [&#39;batch_transaction_id&#39;, &#39;division&#39;, &#39;trx_currency&#39;, &#39;cost_centre_wbs_element_order_no&#39;, &#39;cost_centre_wbs_element_order&#39;, &#39;cost_centre_wbs_element_order_description&#39;, &#39;cost_centre_wbs_element_order_no&#39;, &#39;cost_centre_wbs_element_order&#39;, &#39;cost_centre_wbs_element_order_description&#39;, &#39;cost_centre_wbs_element_order&#39;, &#39;cost_centre_wbs_element_order&#39;, &#39;cost_centre_wbs_element_order_description&#39;]) have been renamed to [&#39;batch_transaction_id_17&#39;, &#39;division_21&#39;, &#39;trx_currency_29&#39;, &#39;cost_centre_wbs_element_order_no_32&#39;, &#39;cost_centre_wbs_element_order_34&#39;, &#39;cost_centre_wbs_element_order_description_35&#39;, &#39;cost_centre_wbs_element_order_no_36&#39;, &#39;cost_centre_wbs_element_order_38&#39;, &#39;cost_centre_wbs_element_order_description_39&#39;, &#39;cost_centre_wbs_element_order_41&#39;, &#39;cost_centre_wbs_element_order_45&#39;, &#39;cost_centre_wbs_element_order_description_46&#39;]. Long column names detected (&gt;25 characters). Consider renaming the following columns [&#39;cost_centre_wbs_element_order&#39;, &#39;cost_centre_wbs_element_order_description&#39;, &#39;cost_center_wbs_element_order&#39;, &#39;cost_center_wbs_element_order_description&#39;, &#39;cost_center_wbls_element_order_description&#39;, &#39;cost_center_wbs_element_order_hash&#39;, &#39;cost_center_wbs_element_order_hash_description&#39;, &#39;cost_centre_wbs_element_order_no&#39;, &#39;cost_centre_wbs_element_order_no_description&#39;, &#39;cost_centre_wbs_element_order_no_32&#39;, &#39;cost_centre_wbs_element_order_34&#39;, &#39;cost_centre_wbs_element_order_description_35&#39;, &#39;cost_centre_wbs_element_order_no_36&#39;, &#39;cost_centre_wbs_element_order_no_decription&#39;, &#39;cost_centre_wbs_element_order_38&#39;, &#39;cost_centre_wbs_element_order_description_39&#39;, &#39;cost_centre_wbs_element_order_41&#39;, &#39;cost_centre_wbs_element_order_45&#39;, &#39;cost_centre_wbs_element_order_description_46&#39;]. . C: tools Anaconda3 envs blog lib site-packages klib clean.py:76: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True. data.columns.str.replace(&#34; n&#34;, &#34;_&#34;) . Index([&#39;division&#39;, &#39;batch_transaction_id&#39;, &#39;transaction_date&#39;, &#39;card_posting_dt&#39;, &#39;merchant_name&#39;, &#39;transaction_amt&#39;, &#39;trx_currency&#39;, &#39;original_amount&#39;, &#39;original_currency&#39;, &#39;g_l_account&#39;, &#39;g_l_account_description&#39;, &#39;cost_centre_wbs_element_order&#39;, &#39;cost_centre_wbs_element_order_description&#39;, &#39;merchant_type&#39;, &#39;merchant_type_description&#39;, &#39;purpose&#39;, &#39;unnamed_16&#39;, &#39;batch_transaction_id_17&#39;, &#39;cost_center_wbs_element_order&#39;, &#39;cost_center_wbs_element_order_description&#39;, &#39;exp_type_desc&#39;, &#39;division_21&#39;, &#39;cost_center_wbls_element_order_description&#39;, &#39;cost_center_wbs_element_order_hash&#39;, &#39;cost_center_wbs_element_order_hash_description&#39;, &#39;card_posting_date&#39;, &#39;transaction_amount&#39;, &#39;transaction_currency&#39;, &#39;merchant_type_mcc&#39;, &#39;trx_currency_29&#39;, &#39;cost_centre_wbs_element_order_no&#39;, &#39;cost_centre_wbs_element_order_no_description&#39;, &#39;cost_centre_wbs_element_order_no_32&#39;, &#39;tr_currency&#39;, &#39;cost_centre_wbs_element_order_34&#39;, &#39;cost_centre_wbs_element_order_description_35&#39;, &#39;cost_centre_wbs_element_order_no_36&#39;, &#39;cost_centre_wbs_element_order_no_decription&#39;, &#39;cost_centre_wbs_element_order_38&#39;, &#39;cost_centre_wbs_element_order_description_39&#39;, &#39;unnamed_17&#39;, &#39;cost_centre_wbs_element_order_41&#39;, &#39;unnamed_18&#39;, &#39;unnamed_19&#39;, &#39;divison&#39;, &#39;cost_centre_wbs_element_order_45&#39;, &#39;cost_centre_wbs_element_order_description_46&#39;], dtype=&#39;object&#39;) . Missing Values . aggregate data missing values plot . klib.missingval_plot(aggr_df) . GridSpec(6, 6) . remove nan divisions . clean_df = aggr_df[aggr_df[&quot;division&quot;].notna()] clean_df.division.isnull().sum(), aggr_df.division.isnull().sum(), aggr_df.shape[0] . (0, 57788, 297996) . klib.missingval_plot(clean_df) . GridSpec(6, 6) . merge similar divisions . divisions_list = list(clean_df.division.unique()) divisions_list . [&#39;PUBLIC HEALTH&#39;, &#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, &#39;PARKS, FORESTRY &amp; RECREATION&#39;, &#39;TRANSPORTATION SERVICES &#39;, &#39;EMPLOYMENT &amp; SOCIAL SERVICES&#39;, &#39;TORONTO WATER&#39;, &#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, &#39;TORONTO PARAMEDIC SERVICES&#39;, &#39;MUNICIPAL LICENSING &amp; STANDARDS&#39;, &#39;FIRE SERVICES&#39;, &#34;CITY CLERK&#39;S OFFICE&#34;, &#39;STRATEGIC COMMUNICATIONS &#39;, &#39;FACILITIES MANAGEMENT DIVISON&#39;, &#39;LEGAL SERVICES&#39;, &#39;HUMAN RESOURCES &#39;, &#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, &#39;SOLID WASTE MANAGEMENT&#39;, &#39;ACCOUNTING SERVICES &#39;, &#39;CITY PLANNING&#39;, &#34;CHILDREN&#39;S SERVICES &#34;, &#39;ENVIRONMENT &amp; ENERGY OFFICE&#39;, &#39;SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION &#39;, &#39;INFORMATION &amp; TECHNOLOGY &#39;, &#39;CORPORATE FINANCE&#39;, &#39;FLEET SERVICES&#39;, &#39;ENGINEERING &amp; CONSTRUCTION SERVICES&#39;, &#39;FINANCE &amp; ADMINISTRATION&#39;, &#39;STRATEGIC &amp; CORPORATE POLICY&#39;, &#39;REVENUE SERVICES &#39;, &#39;POLICY, PLANNING, FINANCE &amp; ADMINISTRATION&#39;, &#39;TORONTO BUILDING &#39;, &#39;PURCHASING &amp; MATERIALS MANAGEMENT &#39;, &#39;311 TORONTO&#39;, &#39;OFFICE OF EMERGENCY MANAGEMENT &#39;, &#39;FINANCIAL PLANNING&#39;, &#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS &#39;, &#39;AFFORDABLE HOUSING OFFICE&#39;, &#39;DEPUTY CITY MGR &amp; CFO&#39;, &#39;CORPORATE CONTRACTS&#39;, &#39;COURT SERVICES &#39;, &#39;CORPORATE SECURITY&#39;, &#39;CFO&#39;, &#39;DEPUTY CITY MGR INTERNAL SERVICES&#39;, &#39;REAL ESTATE SERVICES&#39;, &#39;TREASURER&#39;, &#39;EXECUTIVE MANAGEMENT&#39;, &#39;CITY MANAGER&#39;, &#39;FACILITIES MANAGEMENT&#39;, &#39;ENVIRONMENT &amp; ENERGY&#39;, &#39;STRATEGIC COMMUNICATIONS&#39;, &#39;TRANSPORTATION&#39;, &#39;CHILDRENS SERVICES&#39;, &#39;REVENUE SERVICES&#39;, &#39;CITY CLERKS OFFICE&#39;, &#39;AFFORDABLE HOUSING&#39;, &#39;ACCOUNTING SERVICES&#39;, &#39;OFFICE OF EMERGENCY MANAGEMENT&#39;, &#39;INFORMATION &amp; TECHNOLOGY&#39;, &#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS&#39;, &#39;PURCHASING &amp; MATERIALS MANAGEMENT&#39;, &#39;TORONTO BUILDING&#39;, &#39;LONG-TERM CARE HOMES &amp; SERVICES&#39;, &#39;OFFICE OF THE DCM - INTERNAL SERVICES CLUSTER&#39;, &#39;SHELTER, SUPPORT &amp; HOUSING ADMINSTRATION&#39;, &#39;SOCIAL DEVELOPMENT, FINANICE &amp; ADMINISTRATION&#39;, &#39;COURT SERVICES&#39;, &#39;POLICY, PLANNING, FINANICE &amp; ADMINISTRATION&#39;, &#39;EXECUTIVE MGT&#39;, &#39;TORONTO EMPLOYMENT &amp; SOCIAL SERVICES&#39;, &#39;CHIEF INNOVATION OFFICE&#39;, &#39;OFFICE OF THE TREASURER&#39;, &#39;CHIEF FINANCIAL OFFICER&#39;, &#39;OFFICE OF THE CONTROLER&#39;, &#39;OFFICE OF THE DCM - CORPORATE SERVICES&#39;, &#39;CHIEF TRANSFORMATION OFFICE&#39;, &#39;FINANCE AND ADMIN&#39;, &#39;LONG TERM CARE HOMES&#39;, &#39;SHELTER, SUPPORT and HOUSING ADMINISTRATION&#39;, &#39;EMERGENCY MEDICAL SERVICES&#39;, &#39;FACILITIES MANAGEMENT &#39;, &#39;MUNICIPAL LICENSING &amp; STANDARDS &#39;, &#39;ENGINEERING AND CONSTRUCTION SERVICES&#39;, &#34;CITY MANAGER&#39;S OFFICE&#34;, &#39;INTERNAL AUDIT&#39;] . divisions_series = pd.Series(divisions_list) map_df = group_similar_strings(divisions_series) map_df[&quot;division&quot;] = divisions_series print(f&quot;divisions raw: {len(map_df.division.unique())}&quot;) print(f&quot;divisions combined: {len(map_df.group_rep.unique())}&quot;) . divisions raw: 84 divisions combined: 62 . def get_group_rep(map_df: pd.DataFrame, division: str) -&gt; str: return map_df[map_df.division == division].head(1).group_rep.values[0] clean_up_division = functools.partial(get_group_rep, map_df) . clean_df = clean_df.reset_index() clean_df[&quot;division&quot;] = clean_df.division.apply(clean_up_division) . clean_df.division.unique() . array([&#39;PUBLIC HEALTH&#39;, &#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, &#39;PARKS, FORESTRY &amp; RECREATION&#39;, &#39;TRANSPORTATION SERVICES &#39;, &#39;EMPLOYMENT &amp; SOCIAL SERVICES&#39;, &#39;TORONTO WATER&#39;, &#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, &#39;TORONTO PARAMEDIC SERVICES&#39;, &#39;MUNICIPAL LICENSING &amp; STANDARDS&#39;, &#39;FIRE SERVICES&#39;, &#34;CITY CLERK&#39;S OFFICE&#34;, &#39;STRATEGIC COMMUNICATIONS &#39;, &#39;FACILITIES MANAGEMENT DIVISON&#39;, &#39;LEGAL SERVICES&#39;, &#39;HUMAN RESOURCES &#39;, &#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, &#39;SOLID WASTE MANAGEMENT&#39;, &#39;ACCOUNTING SERVICES &#39;, &#39;CITY PLANNING&#39;, &#34;CHILDREN&#39;S SERVICES &#34;, &#39;ENVIRONMENT &amp; ENERGY OFFICE&#39;, &#39;SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION &#39;, &#39;INFORMATION &amp; TECHNOLOGY &#39;, &#39;CORPORATE FINANCE&#39;, &#39;FLEET SERVICES&#39;, &#39;ENGINEERING &amp; CONSTRUCTION SERVICES&#39;, &#39;FINANCE &amp; ADMINISTRATION&#39;, &#39;STRATEGIC &amp; CORPORATE POLICY&#39;, &#39;REVENUE SERVICES &#39;, &#39;POLICY, PLANNING, FINANCE &amp; ADMINISTRATION&#39;, &#39;TORONTO BUILDING &#39;, &#39;PURCHASING &amp; MATERIALS MANAGEMENT &#39;, &#39;311 TORONTO&#39;, &#39;OFFICE OF EMERGENCY MANAGEMENT &#39;, &#39;FINANCIAL PLANNING&#39;, &#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS &#39;, &#39;AFFORDABLE HOUSING OFFICE&#39;, &#39;DEPUTY CITY MGR &amp; CFO&#39;, &#39;CORPORATE CONTRACTS&#39;, &#39;COURT SERVICES &#39;, &#39;CORPORATE SECURITY&#39;, &#39;CFO&#39;, &#39;DEPUTY CITY MGR INTERNAL SERVICES&#39;, &#39;REAL ESTATE SERVICES&#39;, &#39;TREASURER&#39;, &#39;EXECUTIVE MANAGEMENT&#39;, &#39;CITY MANAGER&#39;, &#39;FACILITIES MANAGEMENT&#39;, &#39;CHILDRENS SERVICES&#39;, &#39;CITY CLERKS OFFICE&#39;, &#39;OFFICE OF THE DCM - INTERNAL SERVICES CLUSTER&#39;, &#39;EXECUTIVE MGT&#39;, &#39;CHIEF INNOVATION OFFICE&#39;, &#39;OFFICE OF THE TREASURER&#39;, &#39;CHIEF FINANCIAL OFFICER&#39;, &#39;OFFICE OF THE CONTROLER&#39;, &#39;OFFICE OF THE DCM - CORPORATE SERVICES&#39;, &#39;CHIEF TRANSFORMATION OFFICE&#39;, &#39;FINANCE AND ADMIN&#39;, &#39;EMERGENCY MEDICAL SERVICES&#39;, &#34;CITY MANAGER&#39;S OFFICE&#34;, &#39;INTERNAL AUDIT&#39;], dtype=object) . convert to more efficient dtypes . clean_df = klib.convert_datatypes(clean_df) clean_df.dtypes . index int32 division category batch_transaction_id string transaction_date datetime64[ns] card_posting_dt datetime64[ns] merchant_name string transaction_amt float32 trx_currency category original_amount float32 original_currency category g_l_account category g_l_account_description category cost_centre_wbs_element_order category cost_centre_wbs_element_order_description category merchant_type float32 merchant_type_description category purpose string unnamed_16 float32 batch_transaction_id_17 string cost_center_wbs_element_order category cost_center_wbs_element_order_description category exp_type_desc category division_21 category cost_center_wbls_element_order_description category cost_center_wbs_element_order_hash category cost_center_wbs_element_order_hash_description category card_posting_date datetime64[ns] transaction_amount float32 transaction_currency category merchant_type_mcc float32 trx_currency_29 category cost_centre_wbs_element_order_no category cost_centre_wbs_element_order_no_description category cost_centre_wbs_element_order_no_32 category tr_currency category cost_centre_wbs_element_order_34 category cost_centre_wbs_element_order_description_35 category cost_centre_wbs_element_order_no_36 category cost_centre_wbs_element_order_no_decription category cost_centre_wbs_element_order_38 category cost_centre_wbs_element_order_description_39 category unnamed_17 float32 cost_centre_wbs_element_order_41 category unnamed_18 float32 unnamed_19 float32 divison category cost_centre_wbs_element_order_45 category cost_centre_wbs_element_order_description_46 category dtype: object . pool subset of cols (by minimizing duplicates and loss of information) . clean_df = klib.data_cleaning(clean_df) clean_df.dtypes . C: tools Anaconda3 envs blog lib site-packages klib clean.py:76: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True. data.columns.str.replace(&#34; n&#34;, &#34;_&#34;) . Long column names detected (&gt;25 characters). Consider renaming the following columns [&#39;cost_center_wbs_element_order&#39;, &#39;cost_center_wbs_element_order_description&#39;, &#39;cost_centre_wbs_element_order_no_description&#39;, &#39;cost_centre_wbs_element_order_no_32&#39;, &#39;cost_centre_wbs_element_order_description_39&#39;, &#39;cost_centre_wbs_element_order_41&#39;]. Shape of cleaned data: (240208, 27)Remaining NAs: 2487521 Changes: Dropped rows: 0 of which 0 duplicates. (Rows: []) Dropped columns: 21 of which 0 single valued. Columns: [] Dropped missing values: 4962027 Reduced memory by at least: 0.21 MB (-0.59%) . index int32 division category batch_transaction_id string transaction_date datetime64[ns] card_posting_dt datetime64[ns] merchant_name string transaction_amt Float32 trx_currency category original_amount Float32 original_currency category g_l_account category g_l_account_description category merchant_type Float32 merchant_type_description category purpose string batch_transaction_id_17 string cost_center_wbs_element_order category cost_center_wbs_element_order_description category card_posting_date datetime64[ns] transaction_amount Float32 transaction_currency category merchant_type_mcc Float32 trx_currency_29 category cost_centre_wbs_element_order_no_description category cost_centre_wbs_element_order_no_32 category cost_centre_wbs_element_order_description_39 category cost_centre_wbs_element_order_41 category dtype: object . Global Plots . klib.corr_plot(clean_df) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature-correlation (pearson)&#39;}&gt; . EDA | Division . for div, grp in clean_df.groupby(&quot;division&quot;): print(div) klib.dist_plot(grp.drop(&quot;index&quot;, axis=1)) plt.show() . 311 TORONTO . ACCOUNTING SERVICES . AFFORDABLE HOUSING OFFICE . CFO . CHIEF FINANCIAL OFFICER . CHIEF INNOVATION OFFICE . CHIEF TRANSFORMATION OFFICE No columns with numeric data were detected. CHILDREN&#39;S SERVICES . CHILDRENS SERVICES . CITY CLERK&#39;S OFFICE . CITY CLERKS OFFICE . CITY MANAGER . CITY MANAGER&#39;S OFFICE No columns with numeric data were detected. CITY PLANNING . CORPORATE CONTRACTS . CORPORATE FINANCE . CORPORATE SECURITY . COURT SERVICES . DEPUTY CITY MGR &amp; CFO . DEPUTY CITY MGR INTERNAL SERVICES . ECONOMIC DEVELOPMENT &amp; CULTURE Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . EMERGENCY MEDICAL SERVICES . EMPLOYMENT &amp; SOCIAL SERVICES . ENGINEERING &amp; CONSTRUCTION SERVICES . ENVIRONMENT &amp; ENERGY OFFICE . EXECUTIVE MANAGEMENT . EXECUTIVE MGT . FACILITIES MANAGEMENT . FACILITIES MANAGEMENT DIVISON Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . FINANCE &amp; ADMINISTRATION . FINANCE AND ADMIN No columns with numeric data were detected. FINANCIAL PLANNING . FIRE SERVICES . FLEET SERVICES . HUMAN RESOURCES . INFORMATION &amp; TECHNOLOGY . INTERNAL AUDIT No columns with numeric data were detected. LEGAL SERVICES . LONG TERM CARE HOMES &amp; SERVICES . MUNICIPAL LICENSING &amp; STANDARDS . OFFICE OF EMERGENCY MANAGEMENT . OFFICE OF THE CONTROLER . OFFICE OF THE DCM - CORPORATE SERVICES . OFFICE OF THE DCM - INTERNAL SERVICES CLUSTER . OFFICE OF THE TREASURER . PARKS, FORESTRY &amp; RECREATION Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . PENSION, PAYROLL &amp; EMPLOYEE BENEFITS . POLICY, PLANNING, FINANCE &amp; ADMINISTRATION . PUBLIC HEALTH . PURCHASING &amp; MATERIALS MANAGEMENT . REAL ESTATE SERVICES . REVENUE SERVICES . SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION . SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION . SOLID WASTE MANAGEMENT . STRATEGIC &amp; CORPORATE POLICY . STRATEGIC COMMUNICATIONS . TORONTO BUILDING . TORONTO PARAMEDIC SERVICES . TORONTO WATER Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . TRANSPORTATION SERVICES . TREASURER . EDA | Division + Transaction Year . for div, grp in clean_df.groupby( [clean_df.division, clean_df.transaction_date.dt.year] ): print(div) klib.dist_plot(grp.drop(&quot;index&quot;, axis=1)) plt.show() . (&#39;311 TORONTO&#39;, 2017.0) . (&#39;311 TORONTO&#39;, 2018.0) . (&#39;311 TORONTO&#39;, 2019.0) . (&#39;ACCOUNTING SERVICES &#39;, 2015.0) . (&#39;ACCOUNTING SERVICES &#39;, 2016.0) . (&#39;ACCOUNTING SERVICES &#39;, 2017.0) . (&#39;ACCOUNTING SERVICES &#39;, 2018.0) . (&#39;ACCOUNTING SERVICES &#39;, 2019.0) . (&#39;AFFORDABLE HOUSING OFFICE&#39;, 2015.0) . (&#39;AFFORDABLE HOUSING OFFICE&#39;, 2016.0) . (&#39;AFFORDABLE HOUSING OFFICE&#39;, 2017.0) . (&#39;AFFORDABLE HOUSING OFFICE&#39;, 2018.0) . (&#39;AFFORDABLE HOUSING OFFICE&#39;, 2019.0) . (&#39;CFO&#39;, 2017.0) No columns with numeric data were detected. (&#39;CFO&#39;, 2018.0) No columns with numeric data were detected. (&#39;CHIEF FINANCIAL OFFICER&#39;, 2018.0) . (&#39;CHIEF FINANCIAL OFFICER&#39;, 2019.0) . (&#39;CHIEF INNOVATION OFFICE&#39;, 2018.0) . (&#39;CHIEF TRANSFORMATION OFFICE&#39;, 2018.0) No columns with numeric data were detected. (&#39;CHIEF TRANSFORMATION OFFICE&#39;, 2019.0) No columns with numeric data were detected. (&#34;CHILDREN&#39;S SERVICES &#34;, 2015.0) . (&#34;CHILDREN&#39;S SERVICES &#34;, 2016.0) . (&#34;CHILDREN&#39;S SERVICES &#34;, 2017.0) . (&#34;CHILDREN&#39;S SERVICES &#34;, 2018.0) . (&#39;CHILDRENS SERVICES&#39;, 2018.0) . (&#39;CHILDRENS SERVICES&#39;, 2019.0) . (&#34;CITY CLERK&#39;S OFFICE&#34;, 2015.0) . (&#34;CITY CLERK&#39;S OFFICE&#34;, 2016.0) . (&#34;CITY CLERK&#39;S OFFICE&#34;, 2017.0) . (&#34;CITY CLERK&#39;S OFFICE&#34;, 2018.0) . (&#39;CITY CLERKS OFFICE&#39;, 2018.0) . (&#39;CITY CLERKS OFFICE&#39;, 2019.0) . (&#39;CITY MANAGER&#39;, 2016.0) . (&#39;CITY MANAGER&#39;, 2018.0) . (&#34;CITY MANAGER&#39;S OFFICE&#34;, 2016.0) No columns with numeric data were detected. (&#39;CITY PLANNING&#39;, 2015.0) . (&#39;CITY PLANNING&#39;, 2016.0) . (&#39;CITY PLANNING&#39;, 2017.0) . (&#39;CITY PLANNING&#39;, 2018.0) . (&#39;CITY PLANNING&#39;, 2019.0) . (&#39;CORPORATE CONTRACTS&#39;, 2015.0) . (&#39;CORPORATE CONTRACTS&#39;, 2016.0) . (&#39;CORPORATE CONTRACTS&#39;, 2017.0) . (&#39;CORPORATE FINANCE&#39;, 2016.0) No columns with numeric data were detected. (&#39;CORPORATE FINANCE&#39;, 2017.0) . (&#39;CORPORATE FINANCE&#39;, 2018.0) . (&#39;CORPORATE FINANCE&#39;, 2019.0) . (&#39;CORPORATE SECURITY&#39;, 2017.0) . (&#39;CORPORATE SECURITY&#39;, 2018.0) . (&#39;COURT SERVICES &#39;, 2015.0) . (&#39;COURT SERVICES &#39;, 2017.0) No columns with numeric data were detected. (&#39;COURT SERVICES &#39;, 2018.0) . (&#39;COURT SERVICES &#39;, 2019.0) . (&#39;DEPUTY CITY MGR &amp; CFO&#39;, 2015.0) . (&#39;DEPUTY CITY MGR &amp; CFO&#39;, 2016.0) . (&#39;DEPUTY CITY MGR &amp; CFO&#39;, 2017.0) . (&#39;DEPUTY CITY MGR INTERNAL SERVICES&#39;, 2017.0) . (&#39;DEPUTY CITY MGR INTERNAL SERVICES&#39;, 2018.0) . (&#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, 2014.0) . (&#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, 2015.0) . (&#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, 2016.0) . (&#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, 2017.0) . (&#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, 2018.0) . (&#39;ECONOMIC DEVELOPMENT &amp; CULTURE&#39;, 2019.0) . (&#39;EMERGENCY MEDICAL SERVICES&#39;, 2014.0) No columns with numeric data were detected. (&#39;EMERGENCY MEDICAL SERVICES&#39;, 2015.0) . (&#39;EMPLOYMENT &amp; SOCIAL SERVICES&#39;, 2015.0) . (&#39;EMPLOYMENT &amp; SOCIAL SERVICES&#39;, 2016.0) . (&#39;EMPLOYMENT &amp; SOCIAL SERVICES&#39;, 2017.0) . (&#39;EMPLOYMENT &amp; SOCIAL SERVICES&#39;, 2018.0) . (&#39;EMPLOYMENT &amp; SOCIAL SERVICES&#39;, 2019.0) . (&#39;ENGINEERING &amp; CONSTRUCTION SERVICES&#39;, 2015.0) . (&#39;ENGINEERING &amp; CONSTRUCTION SERVICES&#39;, 2016.0) . (&#39;ENGINEERING &amp; CONSTRUCTION SERVICES&#39;, 2017.0) . (&#39;ENVIRONMENT &amp; ENERGY OFFICE&#39;, 2015.0) . (&#39;ENVIRONMENT &amp; ENERGY OFFICE&#39;, 2016.0) . (&#39;ENVIRONMENT &amp; ENERGY OFFICE&#39;, 2017.0) . (&#39;ENVIRONMENT &amp; ENERGY OFFICE&#39;, 2018.0) . (&#39;ENVIRONMENT &amp; ENERGY OFFICE&#39;, 2019.0) . (&#39;EXECUTIVE MANAGEMENT&#39;, 2018.0) . (&#39;EXECUTIVE MANAGEMENT&#39;, 2019.0) . (&#39;EXECUTIVE MGT&#39;, 2018.0) . (&#39;FACILITIES MANAGEMENT&#39;, 2014.0) . (&#39;FACILITIES MANAGEMENT&#39;, 2015.0) . (&#39;FACILITIES MANAGEMENT&#39;, 2018.0) . (&#39;FACILITIES MANAGEMENT&#39;, 2019.0) . (&#39;FACILITIES MANAGEMENT DIVISON&#39;, 2015.0) . (&#39;FACILITIES MANAGEMENT DIVISON&#39;, 2016.0) . (&#39;FACILITIES MANAGEMENT DIVISON&#39;, 2017.0) . (&#39;FACILITIES MANAGEMENT DIVISON&#39;, 2018.0) . (&#39;FINANCE &amp; ADMINISTRATION&#39;, 2017.0) . (&#39;FINANCE &amp; ADMINISTRATION&#39;, 2018.0) No columns with numeric data were detected. (&#39;FINANCE AND ADMIN&#39;, 2019.0) No columns with numeric data were detected. (&#39;FINANCIAL PLANNING&#39;, 2016.0) No columns with numeric data were detected. (&#39;FINANCIAL PLANNING&#39;, 2017.0) . (&#39;FINANCIAL PLANNING&#39;, 2018.0) . (&#39;FINANCIAL PLANNING&#39;, 2019.0) No columns with numeric data were detected. (&#39;FIRE SERVICES&#39;, 2014.0) . (&#39;FIRE SERVICES&#39;, 2015.0) . (&#39;FIRE SERVICES&#39;, 2016.0) . (&#39;FIRE SERVICES&#39;, 2017.0) . (&#39;FIRE SERVICES&#39;, 2018.0) . (&#39;FIRE SERVICES&#39;, 2019.0) . (&#39;FLEET SERVICES&#39;, 2015.0) . (&#39;FLEET SERVICES&#39;, 2016.0) . (&#39;FLEET SERVICES&#39;, 2017.0) . (&#39;FLEET SERVICES&#39;, 2018.0) . (&#39;FLEET SERVICES&#39;, 2019.0) . (&#39;HUMAN RESOURCES &#39;, 2015.0) No columns with numeric data were detected. (&#39;HUMAN RESOURCES &#39;, 2016.0) . (&#39;HUMAN RESOURCES &#39;, 2017.0) . (&#39;INFORMATION &amp; TECHNOLOGY &#39;, 2015.0) . (&#39;INFORMATION &amp; TECHNOLOGY &#39;, 2016.0) . (&#39;INFORMATION &amp; TECHNOLOGY &#39;, 2017.0) . (&#39;INFORMATION &amp; TECHNOLOGY &#39;, 2018.0) . (&#39;INFORMATION &amp; TECHNOLOGY &#39;, 2019.0) . (&#39;INTERNAL AUDIT&#39;, 2016.0) No columns with numeric data were detected. (&#39;LEGAL SERVICES&#39;, 2015.0) . (&#39;LEGAL SERVICES&#39;, 2016.0) . (&#39;LEGAL SERVICES&#39;, 2017.0) . (&#39;LEGAL SERVICES&#39;, 2018.0) . (&#39;LEGAL SERVICES&#39;, 2019.0) . (&#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, 2014.0) . (&#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, 2015.0) . (&#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, 2016.0) . (&#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, 2017.0) . (&#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, 2018.0) . (&#39;LONG TERM CARE HOMES &amp; SERVICES&#39;, 2019.0) . (&#39;MUNICIPAL LICENSING &amp; STANDARDS&#39;, 2015.0) . (&#39;MUNICIPAL LICENSING &amp; STANDARDS&#39;, 2016.0) . (&#39;MUNICIPAL LICENSING &amp; STANDARDS&#39;, 2017.0) . (&#39;MUNICIPAL LICENSING &amp; STANDARDS&#39;, 2018.0) . (&#39;MUNICIPAL LICENSING &amp; STANDARDS&#39;, 2019.0) . (&#39;OFFICE OF EMERGENCY MANAGEMENT &#39;, 2015.0) No columns with numeric data were detected. (&#39;OFFICE OF EMERGENCY MANAGEMENT &#39;, 2016.0) . (&#39;OFFICE OF EMERGENCY MANAGEMENT &#39;, 2017.0) . (&#39;OFFICE OF EMERGENCY MANAGEMENT &#39;, 2018.0) . (&#39;OFFICE OF EMERGENCY MANAGEMENT &#39;, 2019.0) . (&#39;OFFICE OF THE CONTROLER&#39;, 2018.0) No columns with numeric data were detected. (&#39;OFFICE OF THE CONTROLER&#39;, 2019.0) . (&#39;OFFICE OF THE DCM - CORPORATE SERVICES&#39;, 2018.0) . (&#39;OFFICE OF THE DCM - CORPORATE SERVICES&#39;, 2019.0) . (&#39;OFFICE OF THE DCM - INTERNAL SERVICES CLUSTER&#39;, 2018.0) . (&#39;OFFICE OF THE TREASURER&#39;, 2018.0) . (&#39;PARKS, FORESTRY &amp; RECREATION&#39;, 2014.0) . (&#39;PARKS, FORESTRY &amp; RECREATION&#39;, 2015.0) Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . (&#39;PARKS, FORESTRY &amp; RECREATION&#39;, 2016.0) Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . (&#39;PARKS, FORESTRY &amp; RECREATION&#39;, 2017.0) Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . (&#39;PARKS, FORESTRY &amp; RECREATION&#39;, 2018.0) Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . (&#39;PARKS, FORESTRY &amp; RECREATION&#39;, 2019.0) Large dataset detected, using 10000 random samples for the plots. Summary statistics are still based on the entire dataset. . (&#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS &#39;, 2015.0) . (&#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS &#39;, 2016.0) . (&#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS &#39;, 2017.0) . (&#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS &#39;, 2018.0) . (&#39;PENSION, PAYROLL &amp; EMPLOYEE BENEFITS &#39;, 2019.0) . (&#39;POLICY, PLANNING, FINANCE &amp; ADMINISTRATION&#39;, 2015.0) . (&#39;POLICY, PLANNING, FINANCE &amp; ADMINISTRATION&#39;, 2016.0) . (&#39;POLICY, PLANNING, FINANCE &amp; ADMINISTRATION&#39;, 2017.0) . (&#39;POLICY, PLANNING, FINANCE &amp; ADMINISTRATION&#39;, 2018.0) . (&#39;POLICY, PLANNING, FINANCE &amp; ADMINISTRATION&#39;, 2019.0) . (&#39;PUBLIC HEALTH&#39;, 2014.0) No columns with numeric data were detected. (&#39;PUBLIC HEALTH&#39;, 2015.0) . (&#39;PUBLIC HEALTH&#39;, 2016.0) . (&#39;PUBLIC HEALTH&#39;, 2017.0) . (&#39;PUBLIC HEALTH&#39;, 2018.0) . (&#39;PUBLIC HEALTH&#39;, 2019.0) . (&#39;PURCHASING &amp; MATERIALS MANAGEMENT &#39;, 2015.0) . (&#39;PURCHASING &amp; MATERIALS MANAGEMENT &#39;, 2016.0) . (&#39;PURCHASING &amp; MATERIALS MANAGEMENT &#39;, 2017.0) . (&#39;PURCHASING &amp; MATERIALS MANAGEMENT &#39;, 2018.0) . (&#39;PURCHASING &amp; MATERIALS MANAGEMENT &#39;, 2019.0) . (&#39;REAL ESTATE SERVICES&#39;, 2017.0) . (&#39;REAL ESTATE SERVICES&#39;, 2018.0) . (&#39;REAL ESTATE SERVICES&#39;, 2019.0) . (&#39;REVENUE SERVICES &#39;, 2014.0) No columns with numeric data were detected. (&#39;REVENUE SERVICES &#39;, 2015.0) . (&#39;REVENUE SERVICES &#39;, 2016.0) . (&#39;REVENUE SERVICES &#39;, 2017.0) . (&#39;REVENUE SERVICES &#39;, 2018.0) . (&#39;REVENUE SERVICES &#39;, 2019.0) . (&#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, 2014.0) . (&#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, 2015.0) . (&#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, 2016.0) . (&#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, 2017.0) . (&#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, 2018.0) . (&#39;SHELTER, SUPPORT &amp; HOUSING ADMINISTRATION&#39;, 2019.0) . (&#39;SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION &#39;, 2015.0) . (&#39;SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION &#39;, 2016.0) . (&#39;SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION &#39;, 2017.0) . (&#39;SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION &#39;, 2018.0) . (&#39;SOCIAL DEVELOPMENT, FINANCE &amp; ADMINISTRATION &#39;, 2019.0) . (&#39;SOLID WASTE MANAGEMENT&#39;, 2014.0) No columns with numeric data were detected. (&#39;SOLID WASTE MANAGEMENT&#39;, 2015.0) . (&#39;SOLID WASTE MANAGEMENT&#39;, 2016.0) . (&#39;SOLID WASTE MANAGEMENT&#39;, 2017.0) . (&#39;SOLID WASTE MANAGEMENT&#39;, 2018.0) . (&#39;SOLID WASTE MANAGEMENT&#39;, 2019.0) . (&#39;STRATEGIC &amp; CORPORATE POLICY&#39;, 2016.0) . (&#39;STRATEGIC &amp; CORPORATE POLICY&#39;, 2017.0) . (&#39;STRATEGIC &amp; CORPORATE POLICY&#39;, 2018.0) . (&#39;STRATEGIC COMMUNICATIONS &#39;, 2015.0) . (&#39;STRATEGIC COMMUNICATIONS &#39;, 2016.0) . (&#39;STRATEGIC COMMUNICATIONS &#39;, 2017.0) . (&#39;STRATEGIC COMMUNICATIONS &#39;, 2018.0) . (&#39;STRATEGIC COMMUNICATIONS &#39;, 2019.0) . (&#39;TORONTO BUILDING &#39;, 2015.0) . (&#39;TORONTO BUILDING &#39;, 2016.0) . (&#39;TORONTO BUILDING &#39;, 2017.0) . (&#39;TORONTO BUILDING &#39;, 2018.0) . (&#39;TORONTO BUILDING &#39;, 2019.0) . (&#39;TORONTO PARAMEDIC SERVICES&#39;, 2015.0) . (&#39;TORONTO PARAMEDIC SERVICES&#39;, 2016.0) . (&#39;TORONTO PARAMEDIC SERVICES&#39;, 2017.0) . (&#39;TORONTO PARAMEDIC SERVICES&#39;, 2018.0) . (&#39;TORONTO PARAMEDIC SERVICES&#39;, 2019.0) . (&#39;TORONTO WATER&#39;, 2014.0) . (&#39;TORONTO WATER&#39;, 2015.0) . (&#39;TORONTO WATER&#39;, 2016.0) . (&#39;TORONTO WATER&#39;, 2017.0) . (&#39;TORONTO WATER&#39;, 2018.0) . (&#39;TORONTO WATER&#39;, 2019.0) . (&#39;TRANSPORTATION SERVICES &#39;, 2015.0) . (&#39;TRANSPORTATION SERVICES &#39;, 2016.0) . (&#39;TRANSPORTATION SERVICES &#39;, 2017.0) . (&#39;TRANSPORTATION SERVICES &#39;, 2018.0) . (&#39;TRANSPORTATION SERVICES &#39;, 2019.0) . (&#39;TREASURER&#39;, 2018.0) . Looks at the Motifs &amp; Discords . cad_df = clean_df[clean_df[&quot;original_currency&quot;] == &quot;CAD&quot;] toronto_cad_df = cad_df[cad_df[&quot;division&quot;] == &quot;311 TORONTO&quot;] . matrix_profile = stumpy.stump(toronto_cad_df[&quot;original_amount&quot;], m=5) . motif_idx = np.argsort(matrix_profile[:, 0])[0] print(f&quot;The inspecting the series motif&quot;) pd.concat( [ toronto_cad_df.iloc[motif_idx - 2], toronto_cad_df.iloc[motif_idx - 1], toronto_cad_df.iloc[motif_idx], toronto_cad_df.iloc[motif_idx - 1], toronto_cad_df.iloc[motif_idx + 2], ], axis=1, ) . The inspecting the series motif . 73246 73247 73248 73247 73250 . index 93349 | 93350 | 93351 | 93350 | 93353 | . division 311 TORONTO | 311 TORONTO | 311 TORONTO | 311 TORONTO | 311 TORONTO | . batch_transaction_id 5097-52 | 5104-15 | 5104-16 | 5104-15 | 5109-24 | . transaction_date 2018-10-11 00:00:00 | 2018-10-22 00:00:00 | 2018-10-22 00:00:00 | 2018-10-22 00:00:00 | 2018-10-29 00:00:00 | . card_posting_dt NaT | NaT | NaT | NaT | NaT | . merchant_name WAL*MART CANADA INC | PIZZA NOVA 017 | PIZZA NOVA 99 | PIZZA NOVA 017 | TIM HORTONS 2387 QTH | . transaction_amt NaN | NaN | NaN | NaN | NaN | . trx_currency NaN | NaN | NaN | NaN | NaN | . original_amount 40.610001 | 283.839996 | 59.189999 | 283.839996 | 41.939999 | . original_currency CAD | CAD | CAD | CAD | CAD | . g_l_account 2099 | 4820 | 4820 | 4820 | 4820 | . g_l_account_description OTHER OFFICE MATERIAL &amp;MINOR FURNISHING | BUSINESS MEETING EXPENSES | BUSINESS MEETING EXPENSES | BUSINESS MEETING EXPENSES | BUSINESS MEETING EXPENSES | . cost_centre_wbs_element_order NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_description NaN | NaN | NaN | NaN | NaN | . merchant_type NaN | NaN | NaN | NaN | NaN | . merchant_type_description Discount Stores | Quick Payment Service - Fast-Food Restau | Eating Places, Restaurants | Quick Payment Service - Fast-Food Restau | Quick Payment Service - Fast-Food Restau | . purpose STORAGE CABINET FOR VIEWING GALLERY MEETING ROOM | TEN PARTY SIZE PIZZA FOR ELECTION DAY STAFF AP... | 2 PIZZA FOR ELECTION DAY STAFF APPRECIATION OC... | TEN PARTY SIZE PIZZA FOR ELECTION DAY STAFF AP... | 50 COOKIES FOR WORKFORCE DEVELOPMENT MONTH TOU... | . unnamed_16 NaN | NaN | NaN | NaN | NaN | . batch_transaction_id_17 &lt;NA&gt; | &lt;NA&gt; | &lt;NA&gt; | &lt;NA&gt; | &lt;NA&gt; | . cost_center_wbs_element_order TO9301 | TO9301 | TO9301 | TO9301 | TO9301 | . cost_center_wbs_element_order_description CONTACT CENTRE | CONTACT CENTRE | CONTACT CENTRE | CONTACT CENTRE | CONTACT CENTRE | . exp_type_desc NaN | NaN | NaN | NaN | NaN | . division_21 NaN | NaN | NaN | NaN | NaN | . cost_center_wbls_element_order_description NaN | NaN | NaN | NaN | NaN | . cost_center_wbs_element_order_hash NaN | NaN | NaN | NaN | NaN | . cost_center_wbs_element_order_hash_description NaN | NaN | NaN | NaN | NaN | . card_posting_date 2018-10-11 00:00:00 | 2018-10-22 00:00:00 | 2018-10-22 00:00:00 | 2018-10-22 00:00:00 | 2018-10-29 00:00:00 | . transaction_amount 40.610001 | 283.839996 | 59.189999 | 283.839996 | 41.939999 | . transaction_currency CAD | CAD | CAD | CAD | CAD | . merchant_type_mcc 5310.0 | 5814.0 | 5812.0 | 5814.0 | 5814.0 | . trx_currency_29 NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_no NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_no_description NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_no_32 NaN | NaN | NaN | NaN | NaN | . tr_currency NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_34 NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_description_35 NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_no_36 NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_no_decription NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_38 NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_description_39 NaN | NaN | NaN | NaN | NaN | . unnamed_17 NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_41 NaN | NaN | NaN | NaN | NaN | . unnamed_18 NaN | NaN | NaN | NaN | NaN | . unnamed_19 NaN | NaN | NaN | NaN | NaN | . divison NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_45 NaN | NaN | NaN | NaN | NaN | . cost_centre_wbs_element_order_description_46 NaN | NaN | NaN | NaN | NaN | . correct_arc_curve, regime_locations = stumpy.fluss( matrix_profile[:, 1], L=5, n_regimes=2, excl_factor=1 ) .",
            "url": "https://kslader8.github.io/kslader8-thoughts/eda/anamoly-detection/klib/stumpy/incomplete/2021/07/19/eda.html",
            "relUrl": "/eda/anamoly-detection/klib/stumpy/incomplete/2021/07/19/eda.html",
            "date": " • Jul 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Azure ML Workspace - [Tensorflow] Time Series Example | Continuation 2",
            "content": "This notebook is the same as the previous notebook. The goals is simply to update it to make a few enhancements that became apparent during the process of making the first notebook. . Enhancements . Implement multiple step prediciton model | splitting pipeline into two steps: prep_data and train | rendering logged plots | switching to table interface for logging scoring metrics instead of list | . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline %config Completer.use_jedi = False . Libraries . import os import datetime as dt from PIL import Image from pathlib import Path . from dotenv import load_dotenv . import pandas as pd import numpy as np import tensorflow as tf import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt print(f&quot;pandas version {pd.__version__}&quot;) print(f&quot;numpy version {np.__version__}&quot;) print(f&quot;tensorflow version {tf.__version__}&quot;) . pandas version 1.2.3 numpy version 1.19.2 tensorflow version 2.3.0 . import azureml.core as aml from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment, Run from azureml.core import Datastore, Dataset from azureml.core.compute import ComputeTarget, AmlCompute from azureml.core.runconfig import RunConfiguration from azureml.core.conda_dependencies import CondaDependencies from azureml.core import Model from azureml.core.resource_configuration import ResourceConfiguration from azureml.data import OutputFileDatasetConfig from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.widgets import RunDetails print(f&quot;azureml version {aml.__version__}&quot;) . azureml version 1.25.0 . import tensorboard from azureml.tensorboard import Tensorboard print(f&quot;tensorboard version {tensorboard.__version__}&quot;) . tensorboard version 2.4.0 . import twelvedata from twelvedata import TDClient print(f&quot;twelvedata version {twelvedata.__version__}&quot;) . twelvedata version 1.1.8 . Project Environment Variables . This is a personal preference of mine to make a .env file per project to encapsulate tokens/secrets/etc outside of notebooks. . In this case I created a file named .env with a single variable apikey=(api key) in the same directory as my experiment. . env_path = Path(&quot;../.env&quot;) assert env_path.exists() _ = load_dotenv(env_path) . Matplotlib . It&#39;s useful to set a few global plotting defaults to save from doing them for every plot in a notebook . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 8) mpl.rcParams[&#39;axes.grid&#39;] = False . Azure ML Workspace . To setup an Azure ML Workspace you will need an azure account (with credit card). To spin it up simply go to https://portal.azure.com/ and type machine learning in the search bar and create a workspace. . Once you have a workspace you will need to download the config.json prior to going to https://ml.azure.com/ to access your workspace . workspace_config_path = Path(&quot;../config.json&quot;) assert workspace_config_path.exists() ws = Workspace.from_config(path=workspace_config_path) . Twelve Data Client . I setup an account at https://twelvedata.com/ to get a free api key to try it out. I had not heard of it before, but it was the first thing that came up in my google search for free market data... . apikey = os.environ.get(&quot;apikey&quot;) td = TDClient(apikey=apikey) . ML Workspace Compute . Get existing compute cluster or create one . compute_name = &quot;aml-compute&quot; vm_size = &quot;Standard_NC6&quot; # vm_size = &quot;Standard_NC6s_v3&quot; if compute_name in ws.compute_targets: compute_target = ws.compute_targets[compute_name] if compute_target and type(compute_target) is AmlCompute: print(&#39;Found compute target: &#39; + compute_name) else: print(&#39;Creating a new compute target...&#39;) provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size, # STANDARD_NC6 is GPU-enabled min_nodes=0, max_nodes=4) # create the compute target compute_target = ComputeTarget.create( ws, compute_name, provisioning_config) # Can poll for a minimum number of nodes and for a specific timeout. # If no min node count is provided it will use the scale settings for the cluster compute_target.wait_for_completion( show_output=True, min_node_count=None, timeout_in_minutes=20) # For a more detailed view of current cluster status, use the &#39;status&#39; property print(compute_target.status.serialize()) . Creating a new compute target... Creating... SucceededProvisioning operation finished, operation &#34;Succeeded&#34; Succeeded AmlCompute wait for completion finished Minimum number of nodes requested have been provisioned {&#39;currentNodeCount&#39;: 0, &#39;targetNodeCount&#39;: 0, &#39;nodeStateCounts&#39;: {&#39;preparingNodeCount&#39;: 0, &#39;runningNodeCount&#39;: 0, &#39;idleNodeCount&#39;: 0, &#39;unusableNodeCount&#39;: 0, &#39;leavingNodeCount&#39;: 0, &#39;preemptedNodeCount&#39;: 0}, &#39;allocationState&#39;: &#39;Steady&#39;, &#39;allocationStateTransitionTime&#39;: &#39;2021-04-06T17:41:16.106000+00:00&#39;, &#39;errors&#39;: None, &#39;creationTime&#39;: &#39;2021-04-06T17:41:13.362593+00:00&#39;, &#39;modifiedTime&#39;: &#39;2021-04-06T17:41:28.964771+00:00&#39;, &#39;provisioningState&#39;: &#39;Succeeded&#39;, &#39;provisioningStateTransitionTime&#39;: None, &#39;scaleSettings&#39;: {&#39;minNodeCount&#39;: 0, &#39;maxNodeCount&#39;: 4, &#39;nodeIdleTimeBeforeScaleDown&#39;: &#39;PT120S&#39;}, &#39;vmPriority&#39;: &#39;Dedicated&#39;, &#39;vmSize&#39;: &#39;STANDARD_NC6&#39;} . ML Workspace Data . TwelveData . List ETFs Available . etf_data = td.get_etf_list() etf_list = etf_data.as_json() etf_df = pd.DataFrame(etf_list) etf_df.head() . symbol name currency exchange . 0 8PSG | Invesco Physical Gold ETC | EUR | XETR | . 1 AAA | BetaShares Australian High Interest Cash ETF | AUD | ASX | . 2 AAAU | Perth Mint Physical Gold ETF | USD | NYSE | . 3 AADR | AdvisorShares Dorsey Wright ADR ETF | USD | NYSE | . 4 AASF | Airlie Australian Share Fund -- ETF Feeder | AUD | ASX | . Get ETF Time Series . end_date = pd.Timestamp(dt.datetime.today()) start_date = end_date - pd.tseries.offsets.BDay(252) * 2 start_date.to_pydatetime().date(), end_date.to_pydatetime().date() . (datetime.date(2019, 5, 1), datetime.date(2021, 4, 6)) . ticker = &quot;IVOL&quot; ts = td.time_series( symbol=ticker, interval=&quot;1day&quot;, start_date=start_date, end_date=end_date, outputsize=500 ) . df = ts.with_ema().with_vwap().as_pandas() df.describe() . open high low close volume ema vwap . count 475.00000 | 475.000000 | 475.000000 | 475.000000 | 4.750000e+02 | 475.000000 | 475.000000 | . mean 26.60264 | 26.671625 | 26.540325 | 26.608292 | 4.910146e+06 | 26.156760 | 26.606747 | . std 1.12284 | 1.116020 | 1.137535 | 1.123807 | 9.894917e+07 | 3.595103 | 1.123732 | . min 24.05000 | 24.727000 | 24.050000 | 24.440000 | 0.000000e+00 | 0.000000 | 24.405670 | . 25% 25.50000 | 25.579500 | 25.450000 | 25.530000 | 1.120000e+04 | 25.547135 | 25.519500 | . 50% 26.58000 | 26.750000 | 26.510000 | 26.600000 | 4.390000e+04 | 26.538130 | 26.592400 | . 75% 27.40000 | 27.465000 | 27.370000 | 27.410000 | 4.084390e+05 | 27.423495 | 27.421665 | . max 28.95000 | 28.950000 | 28.820000 | 28.945000 | 2.156860e+09 | 28.736060 | 28.905000 | . df.head().reset_index() . datetime open high low close volume ema vwap . 0 2021-04-06 | 28.4200 | 28.4500 | 28.42 | 28.420 | 703070 | 28.51697 | 28.43000 | . 1 2021-04-05 | 28.4295 | 28.4800 | 28.40 | 28.400 | 1569462 | 28.54121 | 28.42667 | . 2 2021-04-01 | 28.4861 | 28.5125 | 28.41 | 28.420 | 3013907 | 28.57651 | 28.44750 | . 3 2021-03-31 | 28.5841 | 28.6294 | 28.53 | 28.590 | 2156860498 | 28.61564 | 28.58313 | . 4 2021-03-30 | 28.5000 | 28.5500 | 28.42 | 28.505 | 5507839 | 28.62205 | 28.49167 | . Azure . Azure Workspace Datastore . data_store = ws.get_default_datastore() . Upload ETF Dataset . def get_or_upload_df(ws, data_store, df, ticker): dataset_name = f&#39;{ticker.lower()}_ds&#39; try: ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() except: Dataset.Tabular.register_pandas_dataframe(df, data_store, dataset_name) ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() return df aml_df = get_or_upload_df(ws, data_store, df.reset_index(), ticker) aml_df.head() . datetime open high low close volume ema vwap . 0 2021-04-01 04:00:00 | 28.4861 | 28.5125 | 28.4100 | 28.420 | 3013907 | 28.57651 | 28.44750 | . 1 2021-03-31 04:00:00 | 28.5841 | 28.6294 | 28.5300 | 28.590 | 2156860498 | 28.61564 | 28.58313 | . 2 2021-03-30 04:00:00 | 28.5000 | 28.5500 | 28.4200 | 28.505 | 5507839 | 28.62205 | 28.49167 | . 3 2021-03-29 04:00:00 | 28.7000 | 28.7200 | 28.6300 | 28.650 | 949622 | 28.65131 | 28.66667 | . 4 2021-03-26 04:00:00 | 28.7153 | 28.7790 | 28.7153 | 28.740 | 1108546 | 28.65164 | 28.74477 | . Training . Create Training Script . src_dir = &#39;aml-exp-multi&#39; aml_exp = Path(src_dir) if not aml_exp.exists(): aml_exp.mkdir() . %%writefile aml-exp-multi/data-prep.py # Standard Libraries import argparse import datetime as dt from pathlib import Path # 3rd Party Libraries import numpy as np import pandas as pd # Plotting Libraries import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt # Azure ML Libraries from azureml.core import Run from azureml.core import Dataset # ML Run run = Run.get_context() workspace = run.experiment.workspace # Read in Args parser = argparse.ArgumentParser() parser.add_argument(&#39;--input_dataset&#39;, dest=&#39;input_dataset&#39;, required=True) parser.add_argument(&#39;--train_dataset&#39;, dest=&#39;train_dataset&#39;, required=True) parser.add_argument(&#39;--val_dataset&#39;, dest=&#39;val_dataset&#39;, required=True) parser.add_argument(&#39;--test_dataset&#39;, dest=&#39;test_dataset&#39;, required=True) args = parser.parse_args() # Dataset &amp; Prep ds = run.input_datasets[&#39;input_dataset&#39;] df = ds.to_pandas_dataframe() # Date Feature Prep day = 24*60*60 year = (365.2425)*day date_time = pd.to_datetime(df.datetime) timestamp_s = date_time.map(dt.datetime.timestamp) df[&#39;day_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / day)) df[&#39;day_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / day)) df[&#39;year_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / year)) df[&#39;year_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / year)) # Data Filter features = [&#39;day_sin&#39;, &#39;day_cos&#39;, &#39;low&#39;, &#39;high&#39;, &#39;volume&#39;, &#39;ema&#39;, &#39;vwap&#39;] target = &#39;close&#39; columns = features + [target] df = df[columns] num_features = df.shape[1] # Data Splitting n = len(df) train_df = df[0:int(n*0.6)] val_df = df[int(n*0.6):int(n*0.8)] test_df = df[int(n*0.8):] # Data Normalization train_mean = train_df.mean() train_std = train_df.std() train_df = (train_df - train_mean) / train_std val_df = (val_df - train_mean) / train_std test_df = (test_df - train_mean) / train_std # Data Distribution Check df_std = (df - train_mean) / train_std df_std = df_std.melt(var_name=&#39;Column&#39;, value_name=&#39;Normalized&#39;) plt.figure(figsize=(12, 6)) ax = sns.violinplot(x=&#39;Column&#39;, y=&#39;Normalized&#39;, data=df_std) _ = ax.set_xticklabels(df.keys(), rotation=45) run.log_image(&#39;feature_distribution_check&#39;, plot=plt) print(run.output_datasets) print(type(args.train_dataset), args.train_dataset) with (Path(args.train_dataset) / &#39;train.csv&#39;).open(&#39;w&#39;) as f: train_df.to_csv(f) with (Path(args.val_dataset)/ &#39;val.csv&#39;).open(&#39;w&#39;) as f: val_df.to_csv(f) with (Path(args.test_dataset) / &#39;test.csv&#39;).open(&#39;w&#39;) as f: test_df.to_csv(f) . Overwriting aml-exp-multi/data-prep.py . %%writefile aml-exp-multi/train.py # Standard Libraries import argparse import json import os import datetime as dt from functools import partial # 3rd Party Libraries import numpy as np import pandas as pd import tensorflow as tf # Plotting Libraries import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt # Azure ML Libraries from azureml.core import Run from azureml.core import Dataset, Datastore from azureml.data.datapath import DataPath from azureml.core import Model from azureml.tensorboard.export import export_to_tensorboard from sklearn.metrics import confusion_matrix # Classes class WindowGenerator(): def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None): # Store the raw data. self.train_df = train_df self.val_df = val_df self.test_df = test_df # Work out the label column indices. self.label_columns = label_columns if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(train_df.columns)} # Work out the window parameters. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(0, input_width) self.input_indices = np.arange(self.total_window_size)[self.input_slice] self.label_start = self.total_window_size - self.label_width self.labels_slice = slice(self.label_start, None) self.label_indices = np.arange(self.total_window_size)[self.labels_slice] def __repr__(self): return &#39; n&#39;.join([ f&#39;Total window size: {self.total_window_size}&#39;, f&#39;Input indices: {self.input_indices}&#39;, f&#39;Label indices: {self.label_indices}&#39;, f&#39;Label column name(s): {self.label_columns}&#39;]) @property def train(self): return self.make_dataset(self.train_df) @property def val(self): return self.make_dataset(self.val_df) @property def test(self): return self.make_dataset(self.test_df) @property def example(self): &quot;&quot;&quot;Get and cache an example batch of `inputs, labels` for plotting.&quot;&quot;&quot; result = getattr(self, &#39;_example&#39;, None) if result is None: # No example batch was found, so get one from the `.train` dataset result = next(iter(self.train)) # And cache it for next time self._example = result return result def split_window(self, features): inputs = features[:, self.input_slice, :] labels = features[:, self.labels_slice, :] if self.label_columns is not None: labels = tf.stack( [labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1) # Slicing doesn&#39;t preserve static shape information, so set the shapes # manually. This way the `tf.data.Datasets` are easier to inspect. inputs.set_shape([None, self.input_width, None]) labels.set_shape([None, self.label_width, None]) return inputs, labels def make_dataset(self, data): data = np.array(data, dtype=np.float32) ds = tf.keras.preprocessing.timeseries_dataset_from_array( data=data, targets=None, sequence_length=self.total_window_size, sequence_stride=1, shuffle=True, batch_size=32,) ds = ds.map(self.split_window) return ds def plot(self, plot_col, model=None, max_subplots=3): plt.figure(figsize=(12, 8)) plot_col_index = self.column_indices[plot_col] inputs, labels = self.example max_n = min(max_subplots, len(inputs)) for n in range(max_n): plt.subplot(max_n, 1, n+1) plt.ylabel(f&#39;{plot_col} [normed]&#39;) plt.plot(self.input_indices, inputs[n, :, plot_col_index], label=&#39;Inputs&#39;, marker=&#39;.&#39;, zorder=-10) if self.label_columns: label_col_index = self.label_columns_indices.get(plot_col, None) else: label_col_index = plot_col_index if label_col_index is None: continue plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors=&#39;k&#39;, label=&#39;Labels&#39;, c=&#39;#2ca02c&#39;, s=64) if model is not None: predictions = model(inputs) plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker=&#39;X&#39;, edgecolors=&#39;k&#39;, label=&#39;Predictions&#39;, c=&#39;#ff7f0e&#39;, s=64) if n == 0: plt.legend() plt.xlabel(&#39;Time&#39;) class MultiStepLastBaseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1]) return tf.tile(tf.expand_dims(inputs[:, -1:, self.label_index], -1), [1, OUT_STEPS, 1]) class RepeatBaseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return inputs result = inputs[:, :, self.label_index] return result[:, :, tf.newaxis] class FeedBack(tf.keras.Model): def __init__(self, units, out_steps, num_features): super().__init__() self.out_steps = out_steps self.units = units self.lstm_cell = tf.keras.layers.LSTMCell(units) # Also wrap the LSTMCell in an RNN to simplify the `warmup` method. self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True) self.dense = tf.keras.layers.Dense(num_features) def warmup(self, inputs): # inputs.shape =&gt; (batch, time, features) # x.shape =&gt; (batch, lstm_units) x, *state = self.lstm_rnn(inputs) # predictions.shape =&gt; (batch, features) prediction = self.dense(x) return prediction, state def call(self, inputs, training=None): # Use a TensorArray to capture dynamically unrolled outputs. predictions = [] # Initialize the lstm state prediction, state = self.warmup(inputs) # Insert the first prediction predictions.append(prediction) # Run the rest of the prediction steps for n in range(1, self.out_steps): # Use the last prediction as input. x = prediction # Execute one lstm step. x, state = self.lstm_cell(x, states=state, training=training) # Convert the lstm output to a prediction. prediction = self.dense(x) # Add the prediction to the output predictions.append(prediction) # predictions.shape =&gt; (time, batch, features) predictions = tf.stack(predictions) # predictions.shape =&gt; (batch, time, features) predictions = tf.transpose(predictions, [1, 0, 2]) return predictions # Global Variables MAX_EPOCHS = 20 OUT_STEPS = 24 CONV_WIDTH = 3 # Paths os.makedirs(&#39;./outputs&#39;, exist_ok=True) os.makedirs(&#39;./outputs/model&#39;, exist_ok=True) os.makedirs(&#39;./outputs/log&#39;, exist_ok=True) # Read in Args parser = argparse.ArgumentParser() parser.add_argument(&#39;--train_dataset&#39;, dest=&#39;train_dataset&#39;, required=True) parser.add_argument(&#39;--val_dataset&#39;, dest=&#39;val_dataset&#39;, required=True) parser.add_argument(&#39;--test_dataset&#39;, dest=&#39;test_dataset&#39;, required=True) args = parser.parse_args() # ML Run run = Run.get_context() workspace = run.experiment.workspace datastore = workspace.get_default_datastore() train_ds = run.input_datasets[&#39;train_dataset&#39;] val_ds = run.input_datasets[&#39;val_dataset&#39;] test_ds = run.input_datasets[&#39;test_dataset&#39;] train_df = train_ds.to_pandas_dataframe() val_df = val_ds.to_pandas_dataframe() test_df = test_ds.to_pandas_dataframe() target = train_df.columns[-1] num_features = train_df.shape[1] # Data Windows multi_window = WindowGenerator(input_width=24, label_width=OUT_STEPS, shift=OUT_STEPS, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) multi_window.plot(target) run.log_image(f&#39;{target}_variable&#39;, plot=plt) # Baseline val_performance, tst_performance = {}, {} def log_result(name, model, window, target, vals, tsts): vals[name] = model.evaluate(window.val) tsts[name] = model.evaluate(window.test, verbose=0) window.plot(target, model) run.log_image(f&#39;{name}_pred&#39;, plot=plt) tf.saved_model.save(model, f&#39;outputs/model/{name}&#39;) log_model = partial(log_result, window=multi_window, target=target, vals=val_performance, tsts=tst_performance) print(f&quot;target indice: {multi_window.column_indices.get(target)}&quot;) last_baseline = MultiStepLastBaseline(-1) last_baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) log_model(&#39;last_baseline&#39;, last_baseline) repeat_baseline = RepeatBaseline(-1) repeat_baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) log_model(&#39;repeat_baseline&#39;, repeat_baseline) # Train Models def compile_and_fit(model, window, patience=4): early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=patience, mode=&#39;min&#39;) model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=[tf.metrics.MeanAbsoluteError()]) history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping]) model.summary() return history compile_and_fit_multi = partial(compile_and_fit, window=multi_window) # Train Linear Model linear = tf.keras.Sequential([ # Take the last time-step. # Shape [batch, time, features] =&gt; [batch, 1, features] tf.keras.layers.Lambda(lambda x: x[:, -1:, :]), # Shape =&gt; [batch, 1, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(linear) log_model(&#39;linear&#39;, linear) # Train Dense Model dense = tf.keras.Sequential([ # Take the last time step. # Shape [batch, time, features] =&gt; [batch, 1, features] tf.keras.layers.Lambda(lambda x: x[:, -1:, :]), # Shape =&gt; [batch, 1, dense_units] tf.keras.layers.Dense(512, activation=&#39;relu&#39;), # Shape =&gt; [batch, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(dense) log_model(&#39;dense&#39;, dense) # Train Conv Model conv = tf.keras.Sequential([ # Shape [batch, time, features] =&gt; [batch, CONV_WIDTH, features] tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]), # Shape =&gt; [batch, 1, conv_units] tf.keras.layers.Conv1D(256, activation=&#39;relu&#39;, kernel_size=(CONV_WIDTH)), # Shape =&gt; [batch, 1, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(conv) log_model(&#39;conv&#39;, conv) # Train LSTM Model lstm = tf.keras.Sequential([ # Shape [batch, time, features] =&gt; [batch, lstm_units] # Adding more `lstm_units` just overfits more quickly. tf.keras.layers.LSTM(32, return_sequences=False), # Shape =&gt; [batch, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(lstm) log_model(&quot;lstm&quot;, lstm) # Train AR RNN Feedback Model feedback = FeedBack(units=32, out_steps=OUT_STEPS, num_features=num_features) prediction, state = feedback.warmup(multi_window.example[0]) history = compile_and_fit_multi(feedback) log_model(&#39;lstm_feedback&#39;, feedback) # Log Results &amp; Select Best Model best_model, best_score = None, None if run is not None: # log results run.log_table(&#39;mae_val&#39;, {k: v[1] for (k, v) in val_performance.items()}) run.log_table(&#39;mse_val&#39;, {k: v[0] for (k, v) in val_performance.items()}) run.log_table(&#39;mae_tst&#39;, {k: v[1] for (k, v) in tst_performance.items()}) run.log_table(&#39;mse_tst&#39;, {k: v[0] for (k, v) in tst_performance.items()}) # select best for k, v in tst_performance.items(): try: mae = float(v[1]) if best_score is None and best_model is None: best_model = k best_score = mae elif best_score &gt; mae: best_model = k best_score = mae except: continue run.log(&#39;best_model&#39;, best_model) run.log(&#39;best_score&#39;, best_score) # best_model_path = f&#39;outputs/model/{best_model}&#39; # model = run.register_model(model_name=best_model, model_path=best_model_path) . Overwriting aml-exp-multi/train.py . Setup Training Environment . aml_run_config = RunConfiguration() aml_run_config.target = compute_target aml_run_config.environment.python.user_managed_dependencies = False # Add some packages relied on by data prep step deps = CondaDependencies.create( conda_packages=[&#39;pandas&#39;,&#39;scikit-learn&#39;, &#39;matplotlib&#39;, &#39;seaborn&#39;], pip_packages=[&#39;azureml-sdk&#39;, &#39;azureml-dataprep[fuse,pandas]&#39;, &#39;azureml-pipeline&#39;, &#39;azureml.tensorboard&#39;, &#39;azureml-interpret&#39;], python_version=&#39;3.6.2&#39;, pin_sdk_version=True) deps.add_tensorflow_pip_package(core_type=&#39;gpu&#39;, version=&#39;2.3.1&#39;) aml_run_config.environment.python.conda_dependencies = deps . Build Data Prep Step . train_data = OutputFileDatasetConfig(name=&quot;train_data&quot;, destination=(data_store, &#39;train/tabular/&#39;)).read_delimited_files() val_data = OutputFileDatasetConfig(name=&quot;val_data&quot;, destination=(data_store, &#39;val/tabular/&#39;)).read_delimited_files() test_data = OutputFileDatasetConfig(name=&quot;test_data&quot;, destination=(data_store, &#39;test/tabular/&#39;)).read_delimited_files() dataset_name = f&#39;{ticker.lower()}_ds&#39; input_dataset = Dataset.get_by_name(ws, dataset_name) prep_data_step = PythonScriptStep( name=&quot;prep_data_step&quot;, source_directory=src_dir, script_name=&quot;data-prep.py&quot;, arguments=[ &quot;--input_dataset&quot;, input_dataset.as_named_input(&quot;input_dataset&quot;), &quot;--train_dataset&quot;, train_data, &quot;--val_dataset&quot;, val_data, &quot;--test_dataset&quot;, test_data ], runconfig=aml_run_config, allow_reuse=True ) . Build Train Step . train_step = PythonScriptStep( name=&quot;train_step&quot;, source_directory=src_dir, script_name=&quot;train.py&quot;, arguments=[ &quot;--train_dataset&quot;, train_data.as_input(name=&quot;train_dataset&quot;), &quot;--val_dataset&quot;, val_data.as_input(name=&quot;val_dataset&quot;), &quot;--test_dataset&quot;, test_data.as_input(name=&quot;test_dataset&quot;) ], runconfig=aml_run_config ) . Build Pipeline . steps = [prep_data_step, train_step] pipeline = Pipeline(workspace=ws, steps=steps) . Run Experiment . %%capture experiment = Experiment(ws, &#39;aml_exp_multi&#39;) script_run = experiment.submit(pipeline) script_run.wait_for_completion(show_output=False) . RunDetails(script_run).show() . Review . Azure Portal . I find it best to simply go to the experiment portal url to review from the gui. It contains all the runs from your experiment and makes it easy to review changes from a central location. . script_run.get_portal_url() . &#39;https://ml.azure.com/runs/b3e487ab-146d-4d0c-a47c-1fb9e3e534ce?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . Data Preprocessing . step_name = &#39;prep_data_step&#39; step = script_run.find_step_run(step_name)[0] step.get_portal_url() . &#39;https://ml.azure.com/runs/fff72717-d728-4ebb-8065-c3fdcaf22876?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . The most useful thing for myself is to display the images stored during the running of a step. . imgs = [f for f in step.get_file_names() if f.endswith(&#39;.png&#39;)] for img in imgs: step.download_file(img) display(img) display(Image.open(img)) Path(img).unlink() . &#39;feature_distribution_check_1617743006.png&#39; . We can see that volumne and ema have wide tails / outliers to review and potentially deal with above. . Model Metrics . However, you can choose to do the model review inside the notebook too. . The first place to look when doing this is the experiments metrics. In this example I&#39;m logging the mse and mae for validation &amp; test datasets for each model . step_name = &#39;train_step&#39; step = script_run.find_step_run(step_name)[0] step.get_portal_url() . &#39;https://ml.azure.com/runs/ddaf55c1-11fb-4a9c-8b19-987e7d61422b?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . metrics_dict = step.get_metrics() scores_df = pd.DataFrame() for k, v in metrics_dict.items(): if isinstance(v, dict): scores_df = scores_df.append(pd.DataFrame.from_dict(v, orient=&#39;index&#39;, columns=[k]).T) scores_df.T.sort_values(&#39;mae_tst&#39;) . mae_val mse_val mae_tst mse_tst . last_baseline 0.107446 | 0.023849 | 0.216114 | 0.091769 | . dense 0.913333 | 0.903850 | 0.269590 | 0.126594 | . repeat_baseline 0.128062 | 0.029194 | 0.276632 | 0.119011 | . lstm 0.816010 | 0.706341 | 0.288581 | 0.131727 | . linear 1.056055 | 1.173291 | 0.318440 | 0.144961 | . conv 0.225789 | 0.099458 | 0.431668 | 0.297038 | . lstm_feedback 0.633682 | 0.755861 | 0.457924 | 0.367349 | . Our baseline is performing really well here. Proof that you should always start simple... . imgs = [f for f in step.get_file_names() if f.endswith(&#39;.png&#39;)] for img in imgs: display(img) step.download_file(img) display(Image.open(img)) Path(img).unlink() . &#39;close_variable_1617743085.png&#39; . &#39;conv_pred_1617743096.png&#39; . &#39;dense_pred_1617743092.png&#39; . &#39;last_baseline_pred_1617743086.png&#39; . &#39;linear_pred_1617743089.png&#39; . &#39;lstm_feedback_pred_1617743118.png&#39; . &#39;lstm_pred_1617743105.png&#39; . &#39;repeat_baseline_pred_1617743087.png&#39; . Clean up . Delete Compute Cluster . This is an important step if you don&#39;t want save some money 😉 . print(&quot;starting compute cleanup&quot;) for name, compute in ws.compute_targets.items(): print(f&quot;deleting {name} instance&quot;) compute.delete() while len(ws.compute_targets.items()) != 0: continue print(&quot;compute cleanup complete&quot;) . starting compute cleanup deleting aml-compute instance compute cleanup complete .",
            "url": "https://kslader8.github.io/kslader8-thoughts/azureml/tensorflow/time-series/2021/04/11/aml-exp-continuation-2.html",
            "relUrl": "/azureml/tensorflow/time-series/2021/04/11/aml-exp-continuation-2.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Dispersion as a Hedge Against Bond Selloff?",
            "content": "Conjecture . When bonds selloff, investors should expect a wave of equity dispersion and subdued index volatility. . What is a dispersion trade? . Dispersion is a relative value trade comprised of . A short position on the underlying index’s implied volatility. The lower the realized volatility the better | A long position on the index components (that are likely to have volatility spike). The higher the average realized voatility the better. | . Why trade dispersion? . Strong demand by outright investors for index performance protection from puts because of . A relatively high implied volatiltiy on the index in relation to its components | A steeper skew on an index’s implied volatitly that that of single stocks | . Strong demand for yield enhancement products resulting in depressed volatility from . The impacts of hedging these products | The call overwritting flows on these products pushing short term vol lower on single stocks | . Dispersion trades capture both of these dislocations! . Why consider dispersion when bonds selloff? . Significant sector rotation, usually leads to larger winners and losers, opening opportunities for depressed volatility levels on the index, but causing single stock volatilities to increase materially | Correlation discrepancies between the equity sector to bonds (USD 10Y Swap vs 3M RV SPX Baskets) | . How to choose the right dispersion? . Two ways to structure the dispersion . Vega Flat - the index vega notional is set equal to the total vega notional of the stocks | Theta Flat - the index vega notional is overweight and higher than the total vega notional on the stocks e.g. index vol: 20% &amp; stocks weighted vol: 30% » vega notional: 30/20= 1.5x the notional on the stocks | . | . Two market dynamics . Bullish Market - Theta Flat outperforms due to the extra short index volatility | Bearish Market - Vega Flat is more resilient thanks being to net long volatitily | . Stock Selection . Ranking the stocks in the index by a metric that helps predict good and bad volatility. e.g. Interest Expense / EBITDA to select the top ~3-5 and the bottom ~4-8 . What risks do you bear with dispersion? . Market Risk - The potential loss of the entire premium invested and unbounded losses in the worst case scenario | Credit Risk - The trade creates a credit risk on the counterparty and the guarantor. The counterparty’s and the guarantor’s insolvency may notably result in the partial or toal loss of the invested amount | Liquidity Risk - Market dislocations may render the product illiquid and impossible to withdroaw from | .",
            "url": "https://kslader8.github.io/kslader8-thoughts/trading/2021/04/09/dispersion-bond-selloff.html",
            "relUrl": "/trading/2021/04/09/dispersion-bond-selloff.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Azure ML Workspace - [Tensorflow] Time Series Example | Continuation 1",
            "content": "This notebook is the same as the previous notebook. The goals is simply to update it to make a few enhancements that became apparent during the process of making the first notebook. . Enhancements . Start using the pipeline api | Adding plots for all models | . Bug Fixes . Normalizing input data | Increasing the test data set for lstm testing &amp; performance plot | . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline . Libraries . import os import datetime as dt from pathlib import Path . from dotenv import load_dotenv . import pandas as pd import numpy as np # import tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt print(f&quot;pandas version {pd.__version__}&quot;) print(f&quot;numpy version {np.__version__}&quot;) # print(f&quot;tensorflow version {tf.__version__}&quot;) . pandas version 1.2.0 numpy version 1.18.5 . import azureml.core as aml from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment, Run from azureml.core import Datastore, Dataset from azureml.core.compute import ComputeTarget, AmlCompute from azureml.core.runconfig import RunConfiguration from azureml.core.conda_dependencies import CondaDependencies from azureml.core import Model from azureml.core.resource_configuration import ResourceConfiguration from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.widgets import RunDetails print(f&quot;azureml version {aml.__version__}&quot;) . azureml version 1.25.0 . import tensorboard from azureml.tensorboard import Tensorboard print(f&quot;tensorboard version {tensorboard.__version__}&quot;) . tensorboard version 2.4.1 . import twelvedata from twelvedata import TDClient print(f&quot;twelvedata version {twelvedata.__version__}&quot;) . twelvedata version 1.1.7 . Project Environment Variables . This is a personal preference of mine to make a .env file per project to encapsulate tokens/secrets/etc outside of notebooks. . In this case I created a file named .env with a single variable apikey=(api key) in the same directory as my experiment. . env_path = Path(&quot;.env&quot;) assert env_path.exists() _ = load_dotenv(env_path) . Matplotlib . It&#39;s useful to set a few global plotting defaults to save from doing them for every plot in a notebook . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 8) mpl.rcParams[&#39;axes.grid&#39;] = False . Azure ML Workspace . To setup an Azure ML Workspace you will need an azure account (with credit card). To spin it up simply go to https://portal.azure.com/ and type machine learning in the search bar and create a workspace. . Once you have a workspace you will need to download the config.json prior to going to https://ml.azure.com/ to access your workspace . workspace_config_path = Path(&quot;config.json&quot;) assert workspace_config_path.exists() ws = Workspace.from_config(path=workspace_config_path) . Twelve Data Client . I setup an account at https://twelvedata.com/ to get a free api key to try it out. I had not heard of it before, but it was the first thing that came up in my google search for free market data... . apikey = os.environ.get(&quot;apikey&quot;) td = TDClient(apikey=apikey) . ML Workspace Compute . Get existing compute cluster or create one . compute_name = &quot;aml-compute&quot; vm_size = &quot;Standard_NC6&quot; # vm_size = &quot;Standard_NC6s_v3&quot; if compute_name in ws.compute_targets: compute_target = ws.compute_targets[compute_name] if compute_target and type(compute_target) is AmlCompute: print(&#39;Found compute target: &#39; + compute_name) else: print(&#39;Creating a new compute target...&#39;) provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size, # STANDARD_NC6 is GPU-enabled min_nodes=0, max_nodes=4) # create the compute target compute_target = ComputeTarget.create( ws, compute_name, provisioning_config) # Can poll for a minimum number of nodes and for a specific timeout. # If no min node count is provided it will use the scale settings for the cluster compute_target.wait_for_completion( show_output=True, min_node_count=None, timeout_in_minutes=20) # For a more detailed view of current cluster status, use the &#39;status&#39; property print(compute_target.status.serialize()) . Creating a new compute target... Creating.... SucceededProvisioning operation finished, operation &#34;Succeeded&#34; Succeeded AmlCompute wait for completion finished Minimum number of nodes requested have been provisioned {&#39;currentNodeCount&#39;: 0, &#39;targetNodeCount&#39;: 0, &#39;nodeStateCounts&#39;: {&#39;preparingNodeCount&#39;: 0, &#39;runningNodeCount&#39;: 0, &#39;idleNodeCount&#39;: 0, &#39;unusableNodeCount&#39;: 0, &#39;leavingNodeCount&#39;: 0, &#39;preemptedNodeCount&#39;: 0}, &#39;allocationState&#39;: &#39;Steady&#39;, &#39;allocationStateTransitionTime&#39;: &#39;2021-04-02T20:04:47.531000+00:00&#39;, &#39;errors&#39;: None, &#39;creationTime&#39;: &#39;2021-04-02T20:04:42.205768+00:00&#39;, &#39;modifiedTime&#39;: &#39;2021-04-02T20:04:59.941680+00:00&#39;, &#39;provisioningState&#39;: &#39;Succeeded&#39;, &#39;provisioningStateTransitionTime&#39;: None, &#39;scaleSettings&#39;: {&#39;minNodeCount&#39;: 0, &#39;maxNodeCount&#39;: 4, &#39;nodeIdleTimeBeforeScaleDown&#39;: &#39;PT120S&#39;}, &#39;vmPriority&#39;: &#39;Dedicated&#39;, &#39;vmSize&#39;: &#39;STANDARD_NC6&#39;} . ML Workspace Data . TwelveData . List ETFs Available . etf_data = td.get_etf_list() etf_list = etf_data.as_json() etf_df = pd.DataFrame(etf_list) etf_df.head() . symbol name currency exchange . 0 8PSG | Invesco Physical Gold ETC | EUR | XETR | . 1 AAA | BetaShares Australian High Interest Cash ETF | AUD | ASX | . 2 AAAU | Perth Mint Physical Gold ETF | USD | NYSE | . 3 AADR | AdvisorShares Dorsey Wright ADR ETF | USD | NYSE | . 4 AASF | Airlie Australian Share Fund -- ETF Feeder | AUD | ASX | . Get ETF Time Series . end_date = pd.Timestamp(dt.datetime.today()) start_date = end_date - pd.tseries.offsets.BDay(252) start_date.to_pydatetime().date(), end_date.to_pydatetime().date() . (datetime.date(2020, 4, 15), datetime.date(2021, 4, 2)) . ticker = &quot;VOO&quot; ts = td.time_series( symbol=ticker, interval=&quot;1day&quot;, start_date=start_date, end_date=end_date, outputsize=300 ) df = ts.with_ema().as_pandas() df.describe() . open high low close volume ema . count 240.000000 | 240.000000 | 240.000000 | 240.000000 | 2.400000e+02 | 240.000000 | . mean 316.532337 | 318.708476 | 314.401410 | 316.757663 | 3.385244e+06 | 314.852005 | . std 30.684477 | 30.533911 | 30.831302 | 30.738751 | 1.376637e+06 | 30.938463 | . min 250.960010 | 255.490010 | 250.000000 | 250.960010 | 7.530980e+05 | 250.599230 | . 25% 294.715005 | 296.435857 | 293.557575 | 295.067500 | 2.356972e+06 | 291.208500 | . 50% 314.860000 | 317.341200 | 313.375000 | 315.255005 | 3.077382e+06 | 313.968755 | . 75% 342.612498 | 345.372500 | 340.907497 | 342.372490 | 4.080284e+06 | 340.965635 | . max 366.205990 | 368.290010 | 366.030000 | 368.160000 | 8.397805e+06 | 363.463540 | . df.head().reset_index() . datetime open high low close volume ema . 0 2021-04-01 | 366.20599 | 368.29001 | 366.03000 | 368.16000 | 4591212 | 363.46354 | . 1 2021-03-31 | 362.85999 | 365.82001 | 362.85999 | 364.29001 | 4870674 | 362.28942 | . 2 2021-03-30 | 363.79001 | 363.79001 | 361.28500 | 363.00000 | 3637520 | 361.78927 | . 3 2021-03-29 | 362.66000 | 364.67001 | 361.10971 | 363.79001 | 3062900 | 361.48659 | . 4 2021-03-26 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . Azure . Azure Workspace Datastore . data_store = ws.get_default_datastore() . Upload ETF Dataset . def get_or_upload_df(ws, data_store, df, ticker): dataset_name = f&#39;{ticker.lower()}_ds&#39; try: ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() except: Dataset.Tabular.register_pandas_dataframe(df, data_store, dataset_name) ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() return df aml_df = get_or_upload_df(ws, data_store, df.reset_index(), ticker) aml_df.head() . datetime open high low close volume ema . 0 2021-03-26 04:00:00 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . 1 2021-03-25 04:00:00 | 357.42001 | 360.23999 | 354.14001 | 359.47000 | 5361270 | 360.14842 | . 2 2021-03-24 04:00:00 | 360.70999 | 362.26999 | 357.44000 | 357.57999 | 3989728 | 360.31803 | . 3 2021-03-23 04:00:00 | 359.79501 | 362.51001 | 359.79501 | 362.32001 | 1208455 | 361.00254 | . 4 2021-03-22 04:00:00 | 359.88000 | 363.50000 | 359.76999 | 362.10999 | 3320390 | 360.67317 | . Training . Create Training Script . src_dir = &#39;aml-exp&#39; aml_exp = Path(src_dir) if not aml_exp.exists(): aml_exp.mkdir() . %%writefile aml-exp/train.py # Standard Libraries import argparse import json import os import datetime as dt # 3rd Party Libraries import numpy as np import pandas as pd import tensorflow as tf import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt from azureml.core import Run from azureml.core import Dataset from azureml.core import Model from azureml.tensorboard.export import export_to_tensorboard from azureml.interpret import ExplanationClient from sklearn.metrics import confusion_matrix # Classes class WindowGenerator(): def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None): # Store the raw data. self.train_df = train_df self.val_df = val_df self.test_df = test_df # Work out the label column indices. self.label_columns = label_columns if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(train_df.columns)} # Work out the window parameters. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(0, input_width) self.input_indices = np.arange(self.total_window_size)[self.input_slice] self.label_start = self.total_window_size - self.label_width self.labels_slice = slice(self.label_start, None) self.label_indices = np.arange(self.total_window_size)[self.labels_slice] def __repr__(self): return &#39; n&#39;.join([ f&#39;Total window size: {self.total_window_size}&#39;, f&#39;Input indices: {self.input_indices}&#39;, f&#39;Label indices: {self.label_indices}&#39;, f&#39;Label column name(s): {self.label_columns}&#39;]) @property def train(self): return self.make_dataset(self.train_df) @property def val(self): return self.make_dataset(self.val_df) @property def test(self): return self.make_dataset(self.test_df) @property def example(self): &quot;&quot;&quot;Get and cache an example batch of `inputs, labels` for plotting.&quot;&quot;&quot; result = getattr(self, &#39;_example&#39;, None) if result is None: # No example batch was found, so get one from the `.train` dataset result = next(iter(self.train)) # And cache it for next time self._example = result return result def split_window(self, features): inputs = features[:, self.input_slice, :] labels = features[:, self.labels_slice, :] if self.label_columns is not None: labels = tf.stack( [labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1) # Slicing doesn&#39;t preserve static shape information, so set the shapes # manually. This way the `tf.data.Datasets` are easier to inspect. inputs.set_shape([None, self.input_width, None]) labels.set_shape([None, self.label_width, None]) return inputs, labels def plot(self, plot_col, model=None, max_subplots=3): plt.figure(figsize=(12, 8)) plot_col_index = self.column_indices[plot_col] inputs, labels = self.example max_n = min(max_subplots, len(inputs)) for n in range(max_n): plt.subplot(max_n, 1, n+1) plt.ylabel(f&#39;{plot_col} [normed]&#39;) plt.plot(self.input_indices, inputs[n, :, plot_col_index], label=&#39;Inputs&#39;, marker=&#39;.&#39;, zorder=-10) if self.label_columns: label_col_index = self.label_columns_indices.get(plot_col, None) else: label_col_index = plot_col_index if label_col_index is None: continue plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors=&#39;k&#39;, label=&#39;Labels&#39;, c=&#39;#2ca02c&#39;, s=64) if model is not None: predictions = model(inputs) plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker=&#39;X&#39;, edgecolors=&#39;k&#39;, label=&#39;Predictions&#39;, c=&#39;#ff7f0e&#39;, s=64) if n == 0: plt.legend() plt.xlabel(&#39;Time&#39;) def make_dataset(self, data): data = np.array(data, dtype=np.float32) ds = tf.keras.preprocessing.timeseries_dataset_from_array( data=data, targets=None, sequence_length=self.total_window_size, sequence_stride=1, shuffle=True, batch_size=32,) ds = ds.map(self.split_window) return ds class Baseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return inputs result = inputs[:, :, self.label_index] return result[:, :, tf.newaxis] # Global Variables MAX_EPOCHS = 20 CONV_WIDTH = 3 # Read in Args parser = argparse.ArgumentParser(description=&#39;Train&#39;) parser.add_argument(&#39;--dataset_name&#39;, type=str, dest=&#39;dataset_name&#39;) args = parser.parse_args() # Paths os.makedirs(&#39;./outputs&#39;, exist_ok=True) os.makedirs(&#39;./outputs/model&#39;, exist_ok=True) os.makedirs(&#39;./outputs/log&#39;, exist_ok=True) # ML Run run = Run.get_context() workspace = run.experiment.workspace # ML Dataset ds = Dataset.get_by_name(workspace=workspace, name=args.dataset_name) df = ds.to_pandas_dataframe() # Date Feature Prep day = 24*60*60 year = (365.2425)*day date_time = pd.to_datetime(df.datetime) timestamp_s = date_time.map(dt.datetime.timestamp) df[&#39;day_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / day)) df[&#39;day_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / day)) df[&#39;year_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / year)) df[&#39;year_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / year)) # Data Filter features = [&#39;day_sin&#39;, &#39;day_cos&#39;, &#39;ema&#39;] target = &#39;close&#39; columns = features + [target] df = df[columns] # Data Splitting n = len(df) train_df = df[0:int(n*0.6)] val_df = df[int(n*0.6):int(n*0.8)] test_df = df[int(n*0.8):] # Data Normalization train_mean = train_df.mean() train_std = train_df.std() train_df = (train_df - train_mean) / train_std val_df = (val_df - train_mean) / train_std test_df = (test_df - train_mean) / train_std # Data Distribution Check df_std = (df - train_mean) / train_std df_std = df_std.melt(var_name=&#39;Column&#39;, value_name=&#39;Normalized&#39;) plt.figure(figsize=(12, 6)) ax = sns.violinplot(x=&#39;Column&#39;, y=&#39;Normalized&#39;, data=df_std) _ = ax.set_xticklabels(df.keys(), rotation=45) run.log_image(&#39;feature_distribution_check&#39;, plot=plt) # Data Windows single_step_window = WindowGenerator( input_width=1, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) wide_window = WindowGenerator( input_width=24, label_width=24, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) conv_window = WindowGenerator( input_width=CONV_WIDTH, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) wide_conv_window = WindowGenerator( input_width=24 + CONV_WIDTH - 1, label_width=24, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) # Train Baseline baseline = Baseline(label_index=single_step_window.column_indices.get(target)) baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) val_performance, tst_performance = {}, {} val_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.val) tst_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.test, verbose=0) wide_window.plot(target, baseline) run.log_image(&#39;baseline_pred&#39;, plot=plt) # Train Models def compile_and_fit(model, window, patience=4): early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=patience, mode=&#39;min&#39;) model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=[tf.metrics.MeanAbsoluteError()]) history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping]) return history # Train Linear Model linear = tf.keras.Sequential([ tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(linear, single_step_window) val_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.val) tst_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(linear, &#39;./outputs/model/linear&#39;) fig1 = plt.figure() ax = fig1.add_subplot(111) ax.bar(x = range(len(train_df.columns)), height=linear.layers[0].kernel[:,0].numpy()) ax.set_xticks(range(len(train_df.columns))) _ = ax.set_xticklabels(train_df.columns, rotation=45) run.log_image(&#39;linear_coef&#39;, plot=plt) wide_window.plot(target, linear) run.log_image(&#39;linear_pred&#39;, plot=plt) # Train Single Step Dense Model single_step_dense = tf.keras.Sequential([ tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(single_step_dense, single_step_window) val_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.val) tst_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(single_step_dense, &#39;./outputs/model/single_step_dense&#39;) wide_window.plot(target, single_step_dense) run.log_image(&#39;single_step_dense_pred&#39;, plot=plt) # Train Multi Step Dense Model multi_step_dense = tf.keras.Sequential([ # Shape: (time, features) =&gt; (time*features) tf.keras.layers.Flatten(), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), # Add back the time dimension. # Shape: (outputs) =&gt; (1, outputs) tf.keras.layers.Reshape([1, -1]), ]) history = compile_and_fit(multi_step_dense, conv_window) val_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.val) tst_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.test, verbose=0) tf.saved_model.save(multi_step_dense, &#39;./outputs/model/multi_step_dense&#39;) conv_window.plot(target, multi_step_dense) run.log_image(&#39;multi_step_dense_pred&#39;, plot=plt) # Train Conv Model conv = tf.keras.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=(CONV_WIDTH,), activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), ]) history = compile_and_fit(conv, conv_window) # TODO - log training epoch history to tesnorboard val_performance[&#39;conv&#39;] = conv.evaluate(conv_window.val) tst_performance[&#39;conv&#39;] = conv.evaluate(conv_window.test, verbose=0) tf.saved_model.save(conv, &#39;./outputs/model/conv&#39;) wide_conv_window.plot(target, conv) run.log_image(&#39;conv_pred&#39;, plot=plt) # Train LSTM Model lstm = tf.keras.models.Sequential([ # Shape [batch, time, features] =&gt; [batch, time, lstm_units] tf.keras.layers.LSTM(10, return_sequences=True), # Shape =&gt; [batch, time, features] tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(lstm, wide_window) val_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.val) tst_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.test, verbose=0) tf.saved_model.save(lstm, &#39;./outputs/model/lstm&#39;) wide_window.plot(target, lstm) run.log_image(&#39;lstm_pred&#39;, plot=plt) # Performance x = np.arange(len(val_performance)) width = 0.3 metric_name = &#39;mean_absolute_error&#39; metric_index = lstm.metrics_names.index(&#39;mean_absolute_error&#39;) val_mae = [v[metric_index] for v in val_performance.values()] test_mae = [v[metric_index] for v in tst_performance.values()] fig2 = plt.figure() ax = fig2.add_subplot(111) b1 = ax.bar(x - 0.2, val_mae, width, label=&#39;validation&#39;) b2 = ax.bar(x + 0.2, test_mae, width, label=&#39;test&#39;) ax.set_xticks(range(len(val_mae))) _ = ax.set_xticklabels(val_performance.keys(), rotation=45) run.log_image(&#39;performance_mae&#39;, plot=plt) # Log Results &amp; Select Best Model best_model, best_score = None, None if run is not None: for k, v in val_performance.items(): run.log_list(f&#39;val_{k}&#39;, v) for k, v in tst_performance.items(): run.log_list(f&#39;tst_{k}&#39;, v) try: mae = float(v[1]) if best_score is None and best_model is None: best_model = k best_score = mae elif best_score &gt; mae: best_model = k best_score = mae except: continue run.log(&#39;best_model&#39;, best_model) run.log(&#39;best_score&#39;, best_score) if best_model != &quot;baseline&quot;: model = run.register_model(model_name=best_model, model_path=f&#39;outputs/model/{best_model}&#39;) . Overwriting aml-exp/train.py . . Setup Training Environment . aml_run_config = RunConfiguration() aml_run_config.target = compute_target aml_run_config.environment.python.user_managed_dependencies = False # Add some packages relied on by data prep step deps = CondaDependencies.create( conda_packages=[&#39;pandas&#39;,&#39;scikit-learn&#39;, &#39;matplotlib&#39;, &#39;seaborn&#39;], pip_packages=[&#39;azureml-sdk&#39;, &#39;azureml-dataprep[fuse,pandas]&#39;, &#39;azureml-pipeline&#39;, &#39;azureml.tensorboard&#39;, &#39;azureml-interpret&#39;], python_version=&#39;3.6.2&#39;, pin_sdk_version=True) deps.add_tensorflow_pip_package(core_type=&#39;gpu&#39;, version=&#39;2.3.1&#39;) aml_run_config.environment.python.conda_dependencies = deps . Build Train Step . step1 = PythonScriptStep(name=&quot;train_step&quot;, source_directory=src_dir, script_name=&quot;train.py&quot;, arguments=[&#39;--dataset_name&#39;, f&#39;{ticker.lower()}_ds&#39;], runconfig=aml_run_config, allow_reuse=True) . Build Pipeline . steps = [step1] pipeline = Pipeline(workspace=ws, steps=steps) . Run Experiment . %%capture experiment = Experiment(ws, &#39;aml_exp&#39;) script_run = experiment.submit(pipeline) script_run.wait_for_completion(show_output=False) . RunDetails(script_run).show() . Review . Azure Portal . I find it best to simply go to the experiment portal url to review from the gui. It contains all the runs from your experiment and makes it easy to review changes from a central location. . script_run.get_portal_url() . &#39;https://ml.azure.com/runs/0e922ade-bbce-44c9-94c1-33ebdb17e44f?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . Metrics . However, you can choose to do the model review inside the notebook too. . The first place to look when doing this is the experiments metrics. In this example I&#39;m logging the mse and mae for validation &amp; test datasets for each model . step_name = &#39;train_step&#39; step = script_run.find_step_run(step_name)[0] . metrics_dict = step.get_metrics() print(f&quot;{metrics_dict.get(&#39;best_model&#39;)}: {metrics_dict.get(&#39;best_score&#39;):0.4f}&quot;) print() print(&#39; 033[1m&#39; + f&quot;Model Val Tst&quot; + &#39; 033[0m&#39;) metrics_list = list(filter(lambda v: isinstance(v[1], list), metrics_dict.items())) mae_metrics = [] for name, values in metrics_list: splits = name.split(&quot;_&quot;) grp, model = splits[0], &quot;_&quot;.join(splits[1:]) mae_metrics.append((model, grp, values[1])) mae_metrics = list(sorted(mae_metrics, key=lambda o: o[0])) for cur, nxt in zip(mae_metrics[0::2], mae_metrics[1::2]): name_1, _, value_1 = cur name_2, _, value_2 = nxt assert name_1 == name_2 print(f&#39;{name_1:25s}: {value_1:0.4f} | {value_2:0.4f}&#39;) . single_step_dense: 0.1610 Model Val Tst baseline : 0.1176 | 0.1872 conv : 0.0875 | 0.1773 linear : 3.0643 | 4.7024 lstm : 0.9358 | 2.6143 multi_step_dense : 0.1977 | 0.6740 single_step_dense : 0.1043 | 0.1610 . All Stored Files . It can also be useful to review the log files to figure out wtf is going wrong constantly... . files = step.get_file_names() files . [&#39;azureml-logs/20_image_build_log.txt&#39;, &#39;azureml-logs/55_azureml-execution-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt&#39;, &#39;azureml-logs/65_job_prep-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt&#39;, &#39;azureml-logs/70_driver_log.txt&#39;, &#39;azureml-logs/75_job_post-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt&#39;, &#39;azureml-logs/process_info.json&#39;, &#39;azureml-logs/process_status.json&#39;, &#39;baseline_pred_1617395547.png&#39;, &#39;conv_pred_1617395562.png&#39;, &#39;feature_distribution_check_1617395545.png&#39;, &#39;linear_coef_1617395551.png&#39;, &#39;linear_pred_1617395551.png&#39;, &#39;logs/azureml/106_azureml.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess_Telemetry.log&#39;, &#39;logs/azureml/executionlogs.txt&#39;, &#39;logs/azureml/job_prep_azureml.log&#39;, &#39;logs/azureml/job_release_azureml.log&#39;, &#39;logs/azureml/stderrlogs.txt&#39;, &#39;logs/azureml/stdoutlogs.txt&#39;, &#39;lstm_pred_1617395572.png&#39;, &#39;multi_step_dense_pred_1617395558.png&#39;, &#39;outputs/model/conv/saved_model.pb&#39;, &#39;outputs/model/conv/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/conv/variables/variables.index&#39;, &#39;outputs/model/linear/saved_model.pb&#39;, &#39;outputs/model/linear/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/linear/variables/variables.index&#39;, &#39;outputs/model/lstm/saved_model.pb&#39;, &#39;outputs/model/lstm/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/lstm/variables/variables.index&#39;, &#39;outputs/model/multi_step_dense/saved_model.pb&#39;, &#39;outputs/model/multi_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/multi_step_dense/variables/variables.index&#39;, &#39;outputs/model/single_step_dense/saved_model.pb&#39;, &#39;outputs/model/single_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/single_step_dense/variables/variables.index&#39;, &#39;performance_mae_1617395572.png&#39;, &#39;single_step_dense_pred_1617395555.png&#39;] . Clean up . Delete Compute Cluster . This is an important step if you don&#39;t want save some money 😉 . print(&quot;starting compute cleanup&quot;) for name, compute in ws.compute_targets.items(): print(f&quot;deleting {name} instance&quot;) compute.delete() while len(ws.compute_targets.items()) != 0: continue print(&quot;compute cleanup complete&quot;) . starting compute cleanup deleting aml-compute instance compute cleanup complete .",
            "url": "https://kslader8.github.io/kslader8-thoughts/azureml/tensorflow/time-series/2021/04/04/aml-exp-continuation-1.html",
            "relUrl": "/azureml/tensorflow/time-series/2021/04/04/aml-exp-continuation-1.html",
            "date": " • Apr 4, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Azure ML Workspace - [Tensorflow] Time Series Example",
            "content": "Okay, so I kind of live in Azure ML Workspace these days... leading me to want to make a small notebook utilizing it here. It&#39;s been changing pretty rapidly every ~6 months, so I&#39;m going to include the versions I work on. . If your company is going down the the Azure road for public cloud, Azure ML Workspace (or AWS SageMaker) is probably the best solution to scale easy access to compute, datasets, experiments, etc. to different data science teams accross a large organization. . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline . Libraries . import os import datetime as dt from pathlib import Path . from dotenv import load_dotenv . import pandas as pd import numpy as np # import tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt print(f&quot;pandas version {pd.__version__}&quot;) print(f&quot;numpy version {np.__version__}&quot;) # print(f&quot;tensorflow version {tf.__version__}&quot;) . pandas version 1.2.0 numpy version 1.18.5 . import azureml.core as aml from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment, Run from azureml.core import Datastore, Dataset from azureml.core.compute import ComputeTarget, AmlCompute from azureml.core.runconfig import RunConfiguration from azureml.core.conda_dependencies import CondaDependencies from azureml.core import Model from azureml.core.resource_configuration import ResourceConfiguration from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.widgets import RunDetails print(f&quot;azureml version {aml.__version__}&quot;) . azureml version 1.25.0 . import twelvedata from twelvedata import TDClient print(f&quot;twelvedata version {twelvedata.__version__}&quot;) . twelvedata version 1.1.7 . Project Environment Variables . This is a personal preference of mine to make a .env file per project to encapsulate tokens/secrets/etc outside of notebooks. . In this case I created a file named .env with a single variable apikey=(api key) in the same directory as my experiment. . env_path = Path(&quot;.env&quot;) assert env_path.exists() _ = load_dotenv(env_path) . Matplotlib . It&#39;s useful to set a few global plotting defaults to save from doing them for every plot in a notebook . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 8) mpl.rcParams[&#39;axes.grid&#39;] = False . Azure ML Workspace . To setup an Azure ML Workspace you will need an azure account (with credit card). To spin it up simply go to https://portal.azure.com/ and type machine learning in the search bar and create a workspace. . Once you have a workspace you will need to download the config.json prior to going to https://ml.azure.com/ to access your workspace . workspace_config_path = Path(&quot;config.json&quot;) assert workspace_config_path.exists() ws = Workspace.from_config(path=workspace_config_path) . Twelve Data Client . I setup an account at https://twelvedata.com/ to get a free api key to try it out. I had not heard of it before, but it was the first thing that came up in my google search for free market data... . apikey = os.environ.get(&quot;apikey&quot;) td = TDClient(apikey=apikey) . ML Workspace Compute . Get existing compute cluster or create one . compute_name = &quot;aml-compute&quot; vm_size = &quot;Standard_NC6&quot; # vm_size = &quot;Standard_NC6s_v3&quot; if compute_name in ws.compute_targets: compute_target = ws.compute_targets[compute_name] if compute_target and type(compute_target) is AmlCompute: print(&#39;Found compute target: &#39; + compute_name) else: print(&#39;Creating a new compute target...&#39;) provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size, # STANDARD_NC6 is GPU-enabled min_nodes=0, max_nodes=4) # create the compute target compute_target = ComputeTarget.create( ws, compute_name, provisioning_config) # Can poll for a minimum number of nodes and for a specific timeout. # If no min node count is provided it will use the scale settings for the cluster compute_target.wait_for_completion( show_output=True, min_node_count=None, timeout_in_minutes=20) # For a more detailed view of current cluster status, use the &#39;status&#39; property print(compute_target.status.serialize()) . Creating a new compute target... Creating... SucceededProvisioning operation finished, operation &#34;Succeeded&#34; Succeeded AmlCompute wait for completion finished Minimum number of nodes requested have been provisioned {&#39;currentNodeCount&#39;: 0, &#39;targetNodeCount&#39;: 0, &#39;nodeStateCounts&#39;: {&#39;preparingNodeCount&#39;: 0, &#39;runningNodeCount&#39;: 0, &#39;idleNodeCount&#39;: 0, &#39;unusableNodeCount&#39;: 0, &#39;leavingNodeCount&#39;: 0, &#39;preemptedNodeCount&#39;: 0}, &#39;allocationState&#39;: &#39;Steady&#39;, &#39;allocationStateTransitionTime&#39;: &#39;2021-03-31T18:41:03.129000+00:00&#39;, &#39;errors&#39;: None, &#39;creationTime&#39;: &#39;2021-03-31T18:41:00.552530+00:00&#39;, &#39;modifiedTime&#39;: &#39;2021-03-31T18:41:16.029047+00:00&#39;, &#39;provisioningState&#39;: &#39;Succeeded&#39;, &#39;provisioningStateTransitionTime&#39;: None, &#39;scaleSettings&#39;: {&#39;minNodeCount&#39;: 0, &#39;maxNodeCount&#39;: 4, &#39;nodeIdleTimeBeforeScaleDown&#39;: &#39;PT120S&#39;}, &#39;vmPriority&#39;: &#39;Dedicated&#39;, &#39;vmSize&#39;: &#39;STANDARD_NC6&#39;} . ML Workspace Data . TwelveData . List ETFs Available . etf_data = td.get_etf_list() etf_list = etf_data.as_json() etf_df = pd.DataFrame(etf_list) etf_df.head() . symbol name currency exchange . 0 8PSG | Invesco Physical Gold ETC | EUR | XETR | . 1 AAA | BetaShares Australian High Interest Cash ETF | AUD | ASX | . 2 AAAU | Perth Mint Physical Gold ETF | USD | NYSE | . 3 AADR | AdvisorShares Dorsey Wright ADR ETF | USD | NYSE | . 4 AASF | Airlie Australian Share Fund -- ETF Feeder | AUD | ASX | . Get ETF Time Series . end_date = pd.Timestamp(dt.datetime.today()) start_date = end_date - pd.tseries.offsets.BDay(252) start_date.to_pydatetime().date(), end_date.to_pydatetime().date() . (datetime.date(2020, 4, 13), datetime.date(2021, 3, 31)) . ticker = &quot;VOO&quot; ts = td.time_series( symbol=ticker, interval=&quot;1day&quot;, start_date=start_date, end_date=end_date, outputsize=300 ) df = ts.with_ema().as_pandas() df.describe() . open high low close volume ema . count 241.000000 | 241.000000 | 241.000000 | 241.000000 | 2.410000e+02 | 241.000000 | . mean 315.827105 | 318.008939 | 313.696467 | 316.061946 | 3.388346e+06 | 314.099308 | . std 30.935291 | 30.777465 | 31.051232 | 30.962168 | 1.384690e+06 | 31.300765 | . min 250.960010 | 255.490010 | 250.000000 | 250.960010 | 7.530980e+05 | 247.567540 | . 25% 294.420010 | 296.386990 | 293.149990 | 294.780000 | 2.359274e+06 | 289.318520 | . 50% 314.355010 | 316.260010 | 312.989990 | 314.810000 | 3.065109e+06 | 313.609280 | . 75% 342.239990 | 344.370000 | 340.179990 | 341.989990 | 4.069956e+06 | 340.674540 | . max 365.079990 | 366.049990 | 363.250000 | 365.410000 | 8.397805e+06 | 362.513420 | . df.head().reset_index() . datetime open high low close volume ema . 0 2021-03-31 | 362.85999 | 365.82001 | 362.85999 | 365.41000 | 2687756 | 362.51342 | . 1 2021-03-30 | 363.79001 | 363.79001 | 361.28500 | 363.00000 | 3637520 | 361.78927 | . 2 2021-03-29 | 362.66000 | 364.67001 | 361.10971 | 363.79001 | 3062900 | 361.48659 | . 3 2021-03-26 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . 4 2021-03-25 | 357.42001 | 360.23999 | 354.14001 | 359.47000 | 5361270 | 360.14842 | . Azure . Azure Workspace Datastore . data_store = ws.get_default_datastore() . Upload ETF Dataset . def get_or_upload_df(ws, data_store, df, ticker): dataset_name = f&#39;{ticker.lower()}_ds&#39; try: ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() except: Dataset.Tabular.register_pandas_dataframe(df, data_store, dataset_name) ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() return df aml_df = get_or_upload_df(ws, data_store, df.reset_index(), ticker) aml_df.head() . datetime open high low close volume ema . 0 2021-03-26 04:00:00 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . 1 2021-03-25 04:00:00 | 357.42001 | 360.23999 | 354.14001 | 359.47000 | 5361270 | 360.14842 | . 2 2021-03-24 04:00:00 | 360.70999 | 362.26999 | 357.44000 | 357.57999 | 3989728 | 360.31803 | . 3 2021-03-23 04:00:00 | 359.79501 | 362.51001 | 359.79501 | 362.32001 | 1208455 | 361.00254 | . 4 2021-03-22 04:00:00 | 359.88000 | 363.50000 | 359.76999 | 362.10999 | 3320390 | 360.67317 | . Training . Create Training Script . src_dir = &#39;aml-exp&#39; aml_exp = Path(src_dir) if not aml_exp.exists(): aml_path.mkdir() . %%writefile aml-exp/train.py # Standard Libraries import argparse import json import os import datetime as dt # 3rd Party Libraries import numpy as np import pandas as pd import tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt from azureml.core import Run from azureml.core import Dataset from azureml.core import Model from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.interpret import ExplanationClient from sklearn.metrics import confusion_matrix # Classes class WindowGenerator(): def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None): # Store the raw data. self.train_df = train_df self.val_df = val_df self.test_df = test_df # Work out the label column indices. self.label_columns = label_columns if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(train_df.columns)} # Work out the window parameters. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(0, input_width) self.input_indices = np.arange(self.total_window_size)[self.input_slice] self.label_start = self.total_window_size - self.label_width self.labels_slice = slice(self.label_start, None) self.label_indices = np.arange(self.total_window_size)[self.labels_slice] def __repr__(self): return &#39; n&#39;.join([ f&#39;Total window size: {self.total_window_size}&#39;, f&#39;Input indices: {self.input_indices}&#39;, f&#39;Label indices: {self.label_indices}&#39;, f&#39;Label column name(s): {self.label_columns}&#39;]) @property def train(self): return self.make_dataset(self.train_df) @property def val(self): return self.make_dataset(self.val_df) @property def test(self): return self.make_dataset(self.test_df) @property def example(self): &quot;&quot;&quot;Get and cache an example batch of `inputs, labels` for plotting.&quot;&quot;&quot; result = getattr(self, &#39;_example&#39;, None) if result is None: # No example batch was found, so get one from the `.train` dataset result = next(iter(self.train)) # And cache it for next time self._example = result return result def split_window(self, features): inputs = features[:, self.input_slice, :] labels = features[:, self.labels_slice, :] if self.label_columns is not None: labels = tf.stack( [labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1) # Slicing doesn&#39;t preserve static shape information, so set the shapes # manually. This way the `tf.data.Datasets` are easier to inspect. inputs.set_shape([None, self.input_width, None]) labels.set_shape([None, self.label_width, None]) return inputs, labels def plot(self, plot_col, model=None, max_subplots=3): plt.figure(figsize=(12, 8)) plot_col_index = self.column_indices[plot_col] inputs, labels = self.example max_n = min(max_subplots, len(inputs)) for n in range(max_n): plt.subplot(max_n, 1, n+1) plt.ylabel(f&#39;{plot_col} [normed]&#39;) plt.plot(self.input_indices, inputs[n, :, plot_col_index], label=&#39;Inputs&#39;, marker=&#39;.&#39;, zorder=-10) if self.label_columns: label_col_index = self.label_columns_indices.get(plot_col, None) else: label_col_index = plot_col_index if label_col_index is None: continue plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors=&#39;k&#39;, label=&#39;Labels&#39;, c=&#39;#2ca02c&#39;, s=64) if model is not None: predictions = model(inputs) plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker=&#39;X&#39;, edgecolors=&#39;k&#39;, label=&#39;Predictions&#39;, c=&#39;#ff7f0e&#39;, s=64) if n == 0: plt.legend() plt.xlabel(&#39;Time&#39;) def make_dataset(self, data): data = np.array(data, dtype=np.float32) ds = tf.keras.preprocessing.timeseries_dataset_from_array( data=data, targets=None, sequence_length=self.total_window_size, sequence_stride=1, shuffle=True, batch_size=32,) ds = ds.map(self.split_window) return ds class Baseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return inputs result = inputs[:, :, self.label_index] return result[:, :, tf.newaxis] # Global Variables MAX_EPOCHS = 20 CONV_WIDTH = 3 # Read in Args parser = argparse.ArgumentParser(description=&#39;Train&#39;) parser.add_argument(&#39;--dataset_name&#39;, type=str, dest=&#39;dataset_name&#39;) args = parser.parse_args() # Paths os.makedirs(&#39;./outputs&#39;, exist_ok=True) os.makedirs(&#39;./outputs/model&#39;, exist_ok=True) # ML Run run = Run.get_context() workspace = run.experiment.workspace # ML Dataset ds = Dataset.get_by_name(workspace=workspace, name=args.dataset_name) df = ds.to_pandas_dataframe() # Date Feature Prep day = 24*60*60 year = (365.2425)*day date_time = pd.to_datetime(df.datetime) timestamp_s = date_time.map(dt.datetime.timestamp) df[&#39;day_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / day)) df[&#39;day_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / day)) df[&#39;year_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / year)) df[&#39;year_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / year)) # Data Filter features = [&#39;day_sin&#39;, &#39;day_cos&#39;, &#39;ema&#39;] target = &#39;close&#39; columns = features + [target] df = df[columns] # Data Splitting n = len(df) train_df = df[0:int(n*0.7)] val_df = df[int(n*0.7):int(n*0.9)] test_df = df[int(n*0.9):] # Data Normalization # TODO - normalize step based on train_df # Data Windows single_step_window = WindowGenerator( input_width=1, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) wide_window = WindowGenerator( input_width=24, label_width=24, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) conv_window = WindowGenerator( input_width=CONV_WIDTH, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) # Train Baseline baseline = Baseline(label_index=single_step_window.column_indices.get(target)) baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) val_performance, tst_performance = {}, {} val_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.val) tst_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.test, verbose=0) wide_window.plot(target, baseline) run.log_image(&#39;baseline_pred&#39;, plot=plt) # Train Models def compile_and_fit(model, window, patience=4): early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=patience, mode=&#39;min&#39;) model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=[tf.metrics.MeanAbsoluteError()]) history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping]) return history # Train Linear Model linear = tf.keras.Sequential([ tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(linear, single_step_window) val_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.val) tst_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(linear, &#39;./outputs/model/linear&#39;) fig1 = plt.figure() ax = fig1.add_subplot(111) ax.bar(x = range(len(train_df.columns)), height=linear.layers[0].kernel[:,0].numpy()) ax.set_xticks(range(len(train_df.columns))) _ = ax.set_xticklabels(train_df.columns, rotation=90) run.log_image(&#39;linear_coef&#39;, plot=plt) wide_window.plot(target, linear) run.log_image(&#39;linear_pred&#39;, plot=plt) # Train Single Step Dense Model single_step_dense = tf.keras.Sequential([ tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(single_step_dense, single_step_window) val_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.val) tst_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(single_step_dense, &#39;./outputs/model/single_step_dense&#39;) wide_window.plot(target, single_step_dense) run.log_image(&#39;single_step_dense_pred&#39;, plot=plt) # Train Multi Step Dense Model multi_step_dense = tf.keras.Sequential([ # Shape: (time, features) =&gt; (time*features) tf.keras.layers.Flatten(), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), # Add back the time dimension. # Shape: (outputs) =&gt; (1, outputs) tf.keras.layers.Reshape([1, -1]), ]) history = compile_and_fit(multi_step_dense, conv_window) val_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.val) tst_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.test, verbose=0) tf.saved_model.save(multi_step_dense, &#39;./outputs/model/multi_step_dense&#39;) # wide_window.plot(target, multi_step_dense) # run.log_image(&#39;multi_step_dense_pred&#39;, plot=plt) # Train Conv Model conv = tf.keras.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=(CONV_WIDTH,), activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), ]) history = compile_and_fit(conv, conv_window) val_performance[&#39;conv&#39;] = conv.evaluate(conv_window.val) tst_performance[&#39;conv&#39;] = conv.evaluate(conv_window.test, verbose=0) tf.saved_model.save(conv, &#39;./outputs/model/conv&#39;) # Train LSTM Model lstm = tf.keras.models.Sequential([ # Shape [batch, time, features] =&gt; [batch, time, lstm_units] tf.keras.layers.LSTM(10, return_sequences=True), # Shape =&gt; [batch, time, features] tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(lstm, wide_window) val_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.val) tst_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.test, verbose=0) tf.saved_model.save(lstm, &#39;./outputs/model/lstm&#39;) # Performance x = np.arange(len(val_performance)) width = 0.3 metric_name = &#39;mean_absolute_error&#39; metric_index = lstm.metrics_names.index(&#39;mean_absolute_error&#39;) val_mae = [v[metric_index] for v in val_performance.values()] test_mae = [v[metric_index] for v in tst_performance.values()] fig2 = plt.figure() ax = fig2.add_subplot(111) b1 = ax.bar(x - 0.2, val_mae, width, label=&#39;validation&#39;) # b2 = ax.bar(x + 0.2, test_mae, width, label=&#39;test&#39;) ax.set_xticks(range(len(val_mae))) _ = ax.set_xticklabels(val_performance.keys(), rotation=90) run.log_image(&#39;performance_mae&#39;, plot=plt) # Log Results &amp; Select Best Model best_model, best_score = None, None if run is not None: for k, v in val_performance.items(): run.log_list(f&#39;val_{k}&#39;, v) for k, v in tst_performance.items(): run.log_list(f&#39;tst_{k}&#39;, v) try: mae = float(v[1]) if best_score is None and best_model is None: best_model = k best_score = mae elif best_score &gt; mae: best_model = k best_score = mae except: continue run.log(&#39;best_model&#39;, best_model) run.log(&#39;best_score&#39;, best_score) if best_model != &quot;baseline&quot;: model = run.register_model(model_name=best_model, model_path=f&#39;outputs/model/{best_model}&#39;) #### Setup Training Environment . Overwriting aml-exp/train.py . aml_run_config = RunConfiguration() aml_run_config.target = compute_target aml_run_config.environment.python.user_managed_dependencies = False # Add some packages relied on by data prep step deps = CondaDependencies.create( conda_packages=[&#39;pandas&#39;,&#39;scikit-learn&#39;, &#39;matplotlib&#39;], pip_packages=[&#39;azureml-sdk&#39;, &#39;azureml-dataprep[fuse,pandas]&#39;, &#39;azureml-pipeline&#39;, &#39;azureml-interpret&#39;], python_version=&#39;3.6.2&#39;, pin_sdk_version=True) deps.add_tensorflow_pip_package(core_type=&#39;gpu&#39;, version=&#39;2.3.1&#39;) aml_run_config.environment.python.conda_dependencies = deps src = ScriptRunConfig(source_directory=src_dir, script=&#39;train.py&#39;, arguments=[&#39;--dataset_name&#39;, f&#39;{ticker.lower()}_ds&#39;], run_config=aml_run_config) . Run Experiment . %%capture experiment = Experiment(ws, &#39;aml_exp&#39;) script_run = experiment.submit(src) script_run.wait_for_completion(show_output=False) . RunDetails(script_run).show() . Review . I find it best to simply go to the experiment portal url to review from the gui. It contains all the runs from your experiment and makes it easy to review changes from a central location. . script_run.get_portal_url() . &#39;https://ml.azure.com/runs/aml_exp_1617233359_a7595e84?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . However, you can choose to do the model review inside the notebook too. . The first place to look when doing this is the experiments metrics. In this example I&#39;m logging the mse and mae for validation &amp; test datasets for each model . metrics = script_run.get_metrics() metrics . {&#39;baseline_pred&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/baseline_pred_1617233672.png&#39;, &#39;linear_coef&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/linear_coef_1617233676.png&#39;, &#39;linear_pred&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/linear_pred_1617233676.png&#39;, &#39;single_step_dense_pred&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/single_step_dense_pred_1617233681.png&#39;, &#39;performance_mae&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/performance_mae_1617233696.png&#39;, &#39;val_baseline&#39;: [18.686260223388672, 3.2065956592559814], &#39;val_linear&#39;: [605690.9375, 777.9266357421875], &#39;val_single_step_dense&#39;: [12.129790306091309, 2.596614122390747], &#39;val_multi_step_dense&#39;: [23.9085693359375, 3.903738021850586], &#39;val_conv&#39;: [17.16225242614746, 2.9845776557922363], &#39;val_lstm&#39;: [81859.9296875, 286.0354919433594], &#39;tst_baseline&#39;: [20.958070755004883, 3.920870542526245], &#39;tst_linear&#39;: [502930.53125, 709.0858154296875], &#39;tst_single_step_dense&#39;: [14.61068344116211, 3.2360215187072754], &#39;tst_multi_step_dense&#39;: [20.170698165893555, 4.205769062042236], &#39;tst_conv&#39;: [17.6562557220459, 3.5512776374816895], &#39;best_model&#39;: &#39;single_step_dense&#39;, &#39;best_score&#39;: 3.2360215187072754} . mae_metrics = [] for name, value in metrics.items(): if isinstance(value, list) and len(value) &gt;= 2: splits = name.split(&quot;_&quot;) grp, model = splits[0], &quot;_&quot;.join(splits[1:]) mae_metrics.append((model, grp, value[1])) for model, grp, mae in sorted(mae_metrics, key=lambda o: o[0]): name = f&#39;{model}_{grp}&#39; print(f&#39;{name:25s}: {mae:0.4f}&#39;) . baseline_val : 3.2066 baseline_tst : 3.9209 conv_val : 2.9846 conv_tst : 3.5513 linear_val : 777.9266 linear_tst : 709.0858 lstm_val : 286.0355 multi_step_dense_val : 3.9037 multi_step_dense_tst : 4.2058 single_step_dense_val : 2.5966 single_step_dense_tst : 3.2360 . It can also be useful to review the log files to figure out wtf is going wrong constantly... . files = script_run.get_file_names() files . [&#39;azureml-logs/55_azureml-execution-tvmps_0c357d5ce4f8eda465b3f47f7b95b579e656516823eef14997ff742d99f7565e_d.txt&#39;, &#39;azureml-logs/65_job_prep-tvmps_0c357d5ce4f8eda465b3f47f7b95b579e656516823eef14997ff742d99f7565e_d.txt&#39;, &#39;azureml-logs/70_driver_log.txt&#39;, &#39;azureml-logs/75_job_post-tvmps_0c357d5ce4f8eda465b3f47f7b95b579e656516823eef14997ff742d99f7565e_d.txt&#39;, &#39;azureml-logs/process_info.json&#39;, &#39;azureml-logs/process_status.json&#39;, &#39;baseline_pred_1617233672.png&#39;, &#39;linear_coef_1617233676.png&#39;, &#39;linear_pred_1617233676.png&#39;, &#39;logs/azureml/106_azureml.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess_Telemetry.log&#39;, &#39;logs/azureml/job_prep_azureml.log&#39;, &#39;logs/azureml/job_release_azureml.log&#39;, &#39;outputs/model/conv/saved_model.pb&#39;, &#39;outputs/model/conv/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/conv/variables/variables.index&#39;, &#39;outputs/model/linear/saved_model.pb&#39;, &#39;outputs/model/linear/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/linear/variables/variables.index&#39;, &#39;outputs/model/lstm/saved_model.pb&#39;, &#39;outputs/model/lstm/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/lstm/variables/variables.index&#39;, &#39;outputs/model/multi_step_dense/saved_model.pb&#39;, &#39;outputs/model/multi_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/multi_step_dense/variables/variables.index&#39;, &#39;outputs/model/single_step_dense/saved_model.pb&#39;, &#39;outputs/model/single_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/single_step_dense/variables/variables.index&#39;, &#39;performance_mae_1617233696.png&#39;, &#39;single_step_dense_pred_1617233681.png&#39;] . Clean up . This is an important step if you don&#39;t want to end up having a big bill at the end of the month 😉 . print(&quot;starting compute cleanup&quot;) for name, compute in ws.compute_targets.items(): print(f&quot;deleting {name} instance&quot;) compute.delete() while len(ws.compute_targets.items()) != 0: continue print(&quot;compute cleanup complete&quot;) . starting compute cleanup deleting aml-compute instance deleting ks-nb instance Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; compute cleanup complete .",
            "url": "https://kslader8.github.io/kslader8-thoughts/azureml/tensorflow/time-series/2021/03/29/aml-exp.html",
            "relUrl": "/azureml/tensorflow/time-series/2021/03/29/aml-exp.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "HackerRank Questions - Graph Traversal",
            "content": "I recently did a few hacker rank questions. No promises that these are ideal solutions, but this is my work below. . Problems . import big_o import random from typing import List, Tuple, Dict, Optional . Q1 . Given the number of rows and columns for a grid, find the number of different paths when you can only move down or right assuming you start at the top left and finish at the bottom right. . Example: (7, 3) -&gt; 28; (3,3) -&gt; 6 . def calc_matrix_paths(rows:int, columns:Optional[int]=None) -&gt; int: if columns is None: columns = rows first_row, first_column = 0, 0 count_matrix = [[0 for column in range(columns)] for row in range(rows)] # r for row in range(rows): count_matrix[row][first_column] = 1 # c for column in range(columns): count_matrix[first_row][column] = 1 # r*c for row in range(1, rows): for column in range(1, columns): count_matrix[row][column] = count_matrix[row-1][column] + count_matrix[row][column-1] return count_matrix[rows-1][columns-1] cases = [ (3, 3), # square matrix (7, 3), # non square matrix ] for rows, columns in cases: print(f&quot;rows: {rows} | columns: {columns}&quot;) result = calc_matrix_paths(rows, columns) print(f&quot;number of paths: {result}&quot;) . rows: 3 | columns: 3 number of paths: 6 rows: 7 | columns: 3 number of paths: 28 . big_o.complexities.ALL_CLASSES . [big_o.complexities.Constant, big_o.complexities.Linear, big_o.complexities.Quadratic, big_o.complexities.Cubic, big_o.complexities.Polynomial, big_o.complexities.Logarithmic, big_o.complexities.Linearithmic, big_o.complexities.Exponential] . gen = lambda n: n best, others = big_o.big_o(calc_matrix_paths, gen, min_n=2, max_n=5_000, n_measures=10, n_repeats=2) . print(f&quot;predicted: {best}&quot;) print() for class_, residual in sorted(others.items(), key=lambda t: t[1], reverse=False): print(f&quot;{class_} | {residual}&quot;) . predicted: Exponential: time = -3.2 * 0.002^n (sec) Exponential: time = -3.2 * 0.002^n (sec) | 9.055977664509317 Polynomial: time = -7.5 * x^1.3 (sec) | 22.531114726371193 Cubic: time = -49 + 4.3E-09*n^3 (sec) | 56631.68144191306 Quadratic: time = -72 + 2E-05*n^2 (sec) | 101322.81044136792 Linearithmic: time = -1.1E+02 + 0.01*n*log(n) (sec) | 165159.6456644176 Linear: time = -1.1E+02 + 0.086*n (sec) | 176698.92796259583 Logarithmic: time = -1E+02 + 29*log(n) (sec) | 324034.55046488193 Constant: time = 1E+02 (sec) | 364452.09251446975 . I wonder if the looping is that bad in python... Unless I&#39;m missing something it looks like O(r*c) so should be polynomial time . Q2 . Given an r x c grid, print all cells in spiral order . Example: [[1,2,3],[4,5,6],[7,8,9]] -&gt; 1,2,3,6,9,8,7,4,5 . def get_spiral_order(inp_matrix:List[List[int]]) -&gt; List[int]: start_row = 0 start_col = 0 end_row = len(inp_matrix) end_col = len(inp_matrix[0]) result = [] while end_row &gt; start_row and end_col &gt; start_col: for item in inp_matrix[start_row][start_col: end_col]: result.append(item) for row in range(start_row+1, end_row): result.append(inp_matrix[row][end_col-1]) for col in reversed(list(range(start_col, end_col-1))): result.append(inp_matrix[end_row-1][col]) for i in reversed(list(range(start_row+1, end_row-1))): result.append(inp_matrix[i][start_col]) start_col += 1 start_row += 1 end_col -= 1 end_row -= 1 return result cases = [ [[1,2,3],[4,5,6],[7,8,9]], [[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]], ] for case in cases: print(f&quot;input matrix: {case}&quot;) spiral_order = get_spiral_order(case) print(f&quot;spiral order: {spiral_order}&quot;) . input matrix: [[1, 2, 3], [4, 5, 6], [7, 8, 9]] spiral order: [1, 2, 3, 6, 9, 8, 7, 4, 5] input matrix: [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]] spiral order: [1, 2, 3, 4, 5, 10, 15, 20, 19, 18, 17, 16, 11, 6, 7, 8, 9, 14, 13, 12] . Q3 . Scheduling Problem . You are tasked with creating an algorithm to determine the execution order of a set of tasks. This algorithm will be provided with a list of task labels and a list of task dependencies. Your algorithm should determine a valid execution order of the provided tasks. If there is no valid execution order your algorithm should return None. . Example 1: . labels = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] . where each element in &quot;labels&quot; represents a task label . dependencies = [(&#39;b&#39;, &#39;a&#39;), (&#39;c&#39;, &#39;a&#39;), (&#39;d&#39;, &#39;c&#39;)] . where each element tuple represents a task dependency in which the first element depends on the second . i.e. &#39;b&#39; depends on &#39;a&#39;, &#39;c&#39; depends on &#39;a&#39;, and &#39;d&#39; depends on &#39;c&#39; . In this case a valid execution order would be [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] . another valid execution order could be [&#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;d&#39;] . Example 2: . labels = [&#39;a&#39;, &#39;b&#39;] . dependencies = [(&#39;b&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;)] . In this case there is no valid execution order so your algorithm should return None . from collections import defaultdict, deque def make_dependency_graph(labels: List[str], dependencies: List[Tuple[str]]) -&gt; Dict[str, List[str]]: graph = {} for l in labels: graph[l] = [] for k, v in dependencies: deps = graph[k] deps.append(v) return graph def get_execution_plan(nodes: List[str], graph: Dict[str, List[str]]) -&gt; Optional[List[str]]: to_visit, visited, checked = deque(labels), [], defaultdict(int) while to_visit: cur_place = to_visit.pop() checked[cur_place] += 1 for dep_place in graph.get(cur_place): # dependencies if dep_place not in visited: to_visit.append(cur_place) break else: visited.append(cur_place) if checked[cur_place] &gt; 2: return None return visited for labels, dependencies in [ [[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], [(&#39;a&#39;, &#39;e&#39;), (&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;c&#39;), (&#39;c&#39;, &#39;d&#39;)]], [[&#39;a&#39;, &#39;b&#39;], [(&#39;b&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;)]],]: graph = make_dependency_graph(labels, dependencies) print(f&quot;Dependency Graph: {graph}&quot;) visited = get_execution_plan(labels, graph) print(f&quot;Suggested Execution Plan: {visited}&quot;) . Dependency Graph: {&#39;a&#39;: [&#39;e&#39;, &#39;b&#39;], &#39;b&#39;: [&#39;c&#39;], &#39;c&#39;: [&#39;d&#39;], &#39;d&#39;: [], &#39;e&#39;: []} Suggested Execution Plan: [&#39;e&#39;, &#39;d&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] Dependency Graph: {&#39;a&#39;: [&#39;b&#39;], &#39;b&#39;: [&#39;a&#39;]} Suggested Execution Plan: None .",
            "url": "https://kslader8.github.io/kslader8-thoughts/hacker-rank/2021/02/13/hackerrank-questions.html",
            "relUrl": "/hacker-rank/2021/02/13/hackerrank-questions.html",
            "date": " • Feb 13, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "SpaCy 3 - New Toys",
            "content": "The new version of SpaCy in pre-release hase several new features i&#39;m eagerly awaiting. . Transformer Model | Trainable Sentence Splitter | Improved Interface for Training | Groups of Overlapable Spans | . import json . import spacy from spacy.tokens import Doc from spacy.training import Example spacy.__version__ . &#39;3.0.0rc3&#39; . Models . There is now a transformers based model for greater accuracy . model_acc = spacy.load(&quot;en_core_web_trf&quot;) model_acc.components . [(&#39;transformer&#39;, &lt;spacy_transformers.pipeline_component.Transformer at 0x1be4e4513b0&gt;), (&#39;tagger&#39;, &lt;spacy.pipeline.tagger.Tagger at 0x1be4e457e50&gt;), (&#39;parser&#39;, &lt;spacy.pipeline.dep_parser.DependencyParser at 0x1bd9286b040&gt;), (&#39;ner&#39;, &lt;spacy.pipeline.ner.EntityRecognizer at 0x1bdb7373ee0&gt;), (&#39;attribute_ruler&#39;, &lt;spacy.pipeline.attributeruler.AttributeRuler at 0x1be0a08a600&gt;), (&#39;lemmatizer&#39;, &lt;spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1be0a091900&gt;)] . As well as an improved version of the model we all know and love . model_eff = spacy.load(&quot;en_core_web_sm&quot;) model_eff.components . [(&#39;tok2vec&#39;, &lt;spacy.pipeline.tok2vec.Tok2Vec at 0x1be4ba76860&gt;), (&#39;tagger&#39;, &lt;spacy.pipeline.tagger.Tagger at 0x1be0a1c94a0&gt;), (&#39;parser&#39;, &lt;spacy.pipeline.dep_parser.DependencyParser at 0x1bdb5b896a0&gt;), (&#39;senter&#39;, &lt;spacy.pipeline.senter.SentenceRecognizer at 0x1be4ba76b80&gt;), (&#39;ner&#39;, &lt;spacy.pipeline.ner.EntityRecognizer at 0x1bdb5cb7fa0&gt;), (&#39;attribute_ruler&#39;, &lt;spacy.pipeline.attributeruler.AttributeRuler at 0x1bdb5f156c0&gt;), (&#39;lemmatizer&#39;, &lt;spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1bdb5f6b340&gt;)] . Example Class . I used to dread getting everything formatted into Gold objects previously... The new Example makes a uniform and simple way of formatting data for training . predicted = Doc(model_eff.vocab, words=[&quot;Apply&quot;, &quot;some&quot;, &quot;sun&quot;, &quot;screen&quot;]) token_ref = [&quot;Apply&quot;, &quot;some&quot;, &quot;sun&quot;, &quot;screen&quot;] tags_ref = [&quot;VERB&quot;, &quot;DET&quot;, &quot;NOUN&quot;, &quot;NOUN&quot;] sent_refs = [1, 0, 0, 0] example = Example.from_dict(predicted, {&quot;words&quot;: token_ref, &quot;tags&quot;: tags_ref, &quot;sent_starts&quot;: sent_refs}) example.to_dict() . {&#39;doc_annotation&#39;: {&#39;cats&#39;: {}, &#39;entities&#39;: [&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;], &#39;links&#39;: {}}, &#39;token_annotation&#39;: {&#39;ORTH&#39;: [&#39;Apply&#39;, &#39;some&#39;, &#39;sun&#39;, &#39;screen&#39;], &#39;SPACY&#39;: [True, True, True, True], &#39;TAG&#39;: [&#39;VERB&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;], &#39;LEMMA&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;POS&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;MORPH&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;HEAD&#39;: [0, 1, 2, 3], &#39;DEP&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;SENT_START&#39;: [1, 0, 1, 0]}} . A Statistical Sentence Splitter . This has been one of my dreams for quite a while. I mentioned it a few times on the SpaCy forums too. Very glad to see it make it&#39;s way into the library now. . senter = model_eff.get_pipe(&quot;senter&quot;) optimizer = model_eff.initialize() examples = [example] losses = senter.update(examples, sgd=optimizer) losses . {&#39;senter&#39;: 1.9999818801879883} .",
            "url": "https://kslader8.github.io/kslader8-thoughts/nlp/2021/01/22/spacy-version-3.html",
            "relUrl": "/nlp/2021/01/22/spacy-version-3.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Sorted Collections in Python",
            "content": "I recently was looking at a making predictions on a graph transaction and required maintaining a sorted collection to implement. After spining my wheels (wasting time and failing to build it myself), I google for a bit and found this amazing library called sortedcontainers that is pure python to boot. . If you didn&#39;t know about it previously, check it out. It will make your life easier in the future. . !conda install memory_profiler -n data-structures -y . Collecting package metadata (repodata.json): ...working... done Solving environment: ...working... done # All requested packages already installed. . !conda install sortedcontainers -n data-structures -y . Collecting package metadata (repodata.json): ...working... done Solving environment: ...working... done # All requested packages already installed. . %load_ext autoreload %load_ext memory_profiler %autoreload 2 . from sortedcontainers import SortedList, SortedDict, SortedSet . Sorted List . l = [&#39;e&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;b&#39;] print(f&quot;List({l})&quot;) sl = SortedList(l) print(f&quot;{sl}&quot;) sl *= 10_000_000 print(f&quot;count of c: {sl.count(&#39;c&#39;):,}&quot;) del sl . List([&#39;e&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;b&#39;]) SortedList([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]) count of c: 10,000,000 . %%timeit -n 1 -r 5 %memit sl = SortedList([&#39;e&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;b&#39;]) sl *= 10_000_000 sl.count(&#39;c&#39;) del sl . peak memory: 51.81 MiB, increment: 0.15 MiB peak memory: 52.68 MiB, increment: 0.01 MiB peak memory: 53.14 MiB, increment: -0.00 MiB peak memory: 53.18 MiB, increment: 0.02 MiB peak memory: 53.34 MiB, increment: 0.01 MiB 15.9 s ± 254 ms per loop (mean ± std. dev. of 5 runs, 1 loop each) . Sorted Dict . d = {&#39;c&#39;: 3, &#39;a&#39;: 1, &#39;b&#39;: 2} print(f&quot;Dict({d})&quot;) sd = SortedDict(d) print(f&quot;{sd}&quot;) print(f&quot;pop last item: {sd.popitem(index=-1)}&quot;) del sd . Dict({&#39;c&#39;: 3, &#39;a&#39;: 1, &#39;b&#39;: 2}) SortedDict({&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}) pop last item: (&#39;c&#39;, 3) . %%timeit -n 1 -r 5 %memit sd = SortedDict({&#39;c&#39;: 3, &#39;a&#39;: 1, &#39;b&#39;: 2}) sd.popitem(index=-1) del sd . peak memory: 53.62 MiB, increment: 0.00 MiB peak memory: 53.62 MiB, increment: 0.00 MiB peak memory: 53.62 MiB, increment: 0.00 MiB peak memory: 53.63 MiB, increment: 0.00 MiB peak memory: 53.63 MiB, increment: 0.00 MiB 7.84 s ± 248 ms per loop (mean ± std. dev. of 5 runs, 1 loop each) . Sorted Set . s = set(&#39;abracadabra&#39;) print(f&quot;Set({s})&quot;) ss = SortedSet(s) print(f&quot;{ss}&quot;) print(f&quot;index of c: {ss.bisect_left(&#39;c&#39;)}&quot;) del ss . Set({&#39;d&#39;, &#39;c&#39;, &#39;r&#39;, &#39;b&#39;, &#39;a&#39;}) SortedSet([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]) index of c: 2 . %%timeit -n 1 -r 5 %memit ss = SortedSet(&#39;abracadabra&#39;) ss.bisect_left(&#39;c&#39;) del ss . peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB 7.73 s ± 200 ms per loop (mean ± std. dev. of 5 runs, 1 loop each) .",
            "url": "https://kslader8.github.io/kslader8-thoughts/data-structures/2021/01/18/sorted-collections.html",
            "relUrl": "/data-structures/2021/01/18/sorted-collections.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Transparency / Privacy Tradeoff in a Differentially Private World",
            "content": "from PIL import Image from pathlib import Path . img_dir = Path().absolute().parent / &quot;images&quot; . The Common Mental Model for Transparency / Privacy Tradeoff . Historically, people think about privacy and transparency as a zero sum game. This is because to have access to another parties information means it is not private between the two parties. This makes it difficult to share information without building a strong relationship of trust between two parties. . Image.open(img_dir / &quot;privacy-transparency-pareto-tradeoff.png&quot;) . How People Should Start Thinking About the Tradeoff . Differential privacy techniques changes this to a superior (if not perfect) trade off in practice. Allowing parties to build new business models by taking advantage of less black and write trust relationships. This also gives parties more options to manage/mitigate data leakage risk without completely foregoing utilization of it. . Image.open(img_dir / &quot;moving-the-privacy-transparency-pareto-tradeoff.png&quot;) .",
            "url": "https://kslader8.github.io/kslader8-thoughts/2021/01/14/differential-privacy-visualized.html",
            "relUrl": "/2021/01/14/differential-privacy-visualized.html",
            "date": " • Jan 14, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "EDA of Sample Dataset",
            "content": "Objective . Data Scientists and Analysts are often tasked to clean and analyze datasets. We are working with an external research firm who specializes in the application of artificial intelligence to forecasting prices of financial instruments. This firm has developed a proprietary system, called “4sight”, to forecast prices of certain instruments. . To demonstrate the effectiveness of their forecasting system, the vendor has sent us attached sample dataset. The dataset includes signal values generated by the 4sight system as well as historical prices for a well-known broad market ETF. . A Portfolio Manager has asked you to: . 1) Review the quality of the data, list any potential errors, and propose corrected values. Please list each quality check error and correction applied. 2) Please analyze the signal’s effectiveness or lack thereof in forecasting ETF price, using whatever metrics you think are most relevant. 3) (Extra credit) Write a 1-2 paragraph summary for the Portfolio Manager addressing your observations about the efficacy and believability of the product, and recommendation for next steps. 4) Please budget at least one hour for completing this exercise. Please include all the intermediate steps when sending your solution to this exercise back to us. . Conclusion . Data issues noticed to discuss / inform vendor of: . The last six values of Signal are zeros. Why? drop last six rows (or fill forward as the stale value is probably what most systems will end up using) | . | A large outlier of Signal was found on the 2017-11-13. Why? drop row (or fill forward as the stale value is probably what most systems will end up using) | . | Adj Close is very different from Close / Low / High / Open. How was it adjusted? ignore until more confident about the data | . | a large ourlier of Close was found on the 2018-03-19. Why? fill the highest value of Close with the next mornings open | . | 2017-09-08 to 2017-09-22 look to be filled forward for the Close. fill with Open shifted forward 1 Day | . | high and low inverted on 2018-03-07 and 2018-07-16 ignore if you don&#39;t plan to test them as features or response | if you plan to use them. fill Low with the min of [Open, High, Low, Close] and High with max of [Open, High, Low, Close] | . | . On first glance, the signal looks to have no predictive power out of sample (20% of the most recent data). This conclusion was drawn from conducting a residual plot of Signal against Open shifted forward 1 day and Close. To quickly check for some statistical violations the same was done on a tree model which produced similar conclusions. . I&#39;d hold off on doing a lot more analysis as it is unlikely to be valuable on it&#39;s own. If we delve further, I would focus on checking the signals power as an interaction term with other features and transforming the problem into a classifier instead of a regression. . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . from pathlib import Path . import sklearn import interpret import yellowbrick import pandas as pd import numpy as np from interpret import show from interpret.data import Marginal from interpret.glassbox import ExplainableBoostingRegressor, RegressionTree from interpret.perf import RegressionPerf from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from yellowbrick.features import Rank2D from yellowbrick.features import JointPlotVisualizer from yellowbrick.regressor import ResidualsPlot . from pydqc import infer_schema, data_summary, data_compare, data_consist . print(f&quot;numpy: {np.__version__}&quot;) print(f&quot;pandas: {pd.__version__}&quot;) print(f&quot;sklearn: {sklearn.__version__}&quot;) print(f&quot;interpret: {interpret.__version__}&quot;) print(f&quot;yellowbrick: {yellowbrick.__version__}&quot;) . numpy: 1.19.2 pandas: 1.2.0 sklearn: 0.23.2 interpret: 0.2.2 yellowbrick: 1.2 . Data . eda_dir = Path(&quot;.&quot;) data_path = eda_dir / &quot;Sample Dataset.xlsx&quot; assert data_path.exists() == True df = pd.read_excel(data_path) pd.concat([df.dtypes, df.head(1).T, df.tail(1).T], axis=1) . 0 0 1037 . Date datetime64[ns] | 2015-11-19 00:00:00 | 2020-01-06 00:00:00 | . Signal float64 | 13.768540 | 0.000000 | . Open float64 | 116.440002 | 163.850006 | . High float64 | 116.650002 | 165.539993 | . Low float64 | 115.739998 | 163.539993 | . Close float64 | 116.059998 | 165.350006 | . Adj Close float64 | 108.281601 | 163.534668 | . df.describe() . Signal Open High Low Close Adj Close . count 1038.000000 | 1038.000000 | 1038.000000 | 1038.000000 | 1038.000000 | 1038.000000 | . mean 16.766190 | 141.847360 | 142.691801 | 140.907746 | 141.840973 | 136.341060 | . std 3.095783 | 18.475574 | 18.470255 | 18.404504 | 18.497010 | 21.427837 | . min 0.000000 | 94.080002 | 95.400002 | 93.639999 | 94.790001 | -152.277847 | . 25% 14.691150 | 132.132496 | 132.912495 | 130.542503 | 131.824993 | 125.290491 | . 50% 17.298240 | 146.769997 | 147.959999 | 145.634995 | 146.885002 | 142.667732 | . 75% 19.030890 | 155.367496 | 156.287495 | 154.422500 | 155.289993 | 151.798325 | . max 35.434147 | 172.789993 | 173.389999 | 171.949997 | 196.279999 | 168.842270 | . Comments . Date looks like the obvious index | Why is the adj close so far from the close? | Why is the signal 0 for last entry? Is zero a valid value? | Why is the max close greater than the max high? | . Check that Date column is unique . df[&#39;Date&#39;].value_counts().max() . 1 . Review Adj Close . df[[&#39;Close&#39;, &#39;Adj Close&#39;]].plot(figsize=(16,8)) . &lt;AxesSubplot:&gt; . (df[&#39;Close&#39;] - df[&#39;Adj Close&#39;]).hist(figsize=(16,6), bins=[0, 2, 5, 10, 15, 20, 30]) . &lt;AxesSubplot:&gt; . (df[&#39;Close&#39;] - df[&#39;Adj Close&#39;]).describe() . count 1038.000000 mean 5.499913 std 9.748196 min -33.025085 25% 3.777889 50% 5.693078 75% 6.516651 max 308.837845 dtype: float64 . Comments . This looks weird, but I&#39;m not sure what&#39;s going on. I&#39;m not comfortable using Adj Close until this is understood | . Check Signal Histogram . df[&#39;Signal&#39;].hist(figsize=(12,6)) . &lt;AxesSubplot:&gt; . df[&#39;Signal&#39;].describe() . count 1038.000000 mean 16.766190 std 3.095783 min 0.000000 25% 14.691150 50% 17.298240 75% 19.030890 max 35.434147 Name: Signal, dtype: float64 . Check Indexs of Zero Signal . df[df[&#39;Signal&#39;] == 0.0].index . Int64Index([1032, 1033, 1034, 1035, 1036, 1037], dtype=&#39;int64&#39;) . Comments . looks like an issue in signal calculation at for the last 6 rows. we should inform the signal provider and drop them for now. we could also fill forward the last real signal | . Check the Data for Max Signal . df[df[&#39;Signal&#39;] == df[&#39;Signal&#39;].max()].index . Int64Index([500], dtype=&#39;int64&#39;) . max_signal_iloc = df[df[&#39;Signal&#39;] == df[&#39;Signal&#39;].max()].index.values[0] df.iloc[max_signal_iloc - 1: max_signal_iloc + 2] . Date Signal Open High Low Close Adj Close . 499 2017-11-10 | 17.628384 | 146.710007 | 147.100006 | 146.350006 | 146.570007 | 140.810852 | . 500 2017-11-13 | 35.434147 | 145.929993 | 146.820007 | 145.500000 | 146.610001 | 140.849274 | . 501 2017-11-14 | 17.456319 | 146.059998 | 146.490005 | 145.589996 | 146.210007 | 140.465012 | . Comments . looks like the max is an error. we should inform the signal provider and drop them for now. we could also fill forward the last real signal | . Check Close Histogram . df[&#39;Close&#39;].hist(figsize=(12,6)) . &lt;AxesSubplot:&gt; . max_close_iloc = df[df[&#39;Close&#39;] == df[&#39;Close&#39;].max()].index.values[0] df.iloc[max_close_iloc - 1: max_close_iloc + 2] . Date Signal Open High Low Close Adj Close . 584 2018-03-16 | 19.385186 | 156.979996 | 158.270004 | 156.750000 | 157.800003 | 152.174042 | . 585 2018-03-19 | 18.660897 | 157.169998 | 157.210007 | 154.449997 | 196.279999 | 150.708221 | . 586 2018-03-20 | 19.177721 | 156.669998 | 157.020004 | 155.770004 | 156.240005 | 150.669647 | . Comments . looks like the max close is an error... I&#39;d recommend setting it to next days open so we can test it as a response variable | . Check Open Histogram . df[&#39;Open&#39;].hist(figsize=(12,6)) . &lt;AxesSubplot:&gt; . Check High, Low, Implied Rules . High should be greater than Close, Open, and Low | Low should be less than Close, Open, and High | . df[~(df[&#39;Close&#39;] &lt;= df[&#39;High&#39;])] . Date Signal Open High Low Close Adj Close . 431 2017-08-07 | 16.298805 | 140.440002 | 140.350000 | 139.710007 | 140.440002 | 134.595871 | . 585 2018-03-19 | 18.660897 | 157.169998 | 157.210007 | 154.449997 | 196.279999 | 150.708221 | . 766 2018-12-06 | 16.904044 | 145.449997 | 147.099997 | 143.429993 | 147.199997 | 143.173874 | . 983 2019-10-17 | 18.878412 | 152.289993 | 153.309995 | 152.050003 | 153.339996 | 151.102173 | . df[~(df[&#39;Open&#39;] &lt;= df[&#39;High&#39;])] . Date Signal Open High Low Close Adj Close . 431 2017-08-07 | 16.298805 | 140.440002 | 140.35 | 139.710007 | 140.440002 | 134.595871 | . df[~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 455 2017-09-11 | 15.838558 | 140.389999 | 140.919998 | 140.229996 | 139.110001 | 133.321198 | . 456 2017-09-12 | 15.518587 | 141.039993 | 141.690002 | 140.820007 | 139.110001 | 133.321198 | . 457 2017-09-13 | 16.158529 | 141.410004 | 142.220001 | 141.320007 | 139.110001 | 133.321198 | . 458 2017-09-14 | 16.478500 | 141.779999 | 142.160004 | 141.419998 | 139.110001 | 133.321198 | . 459 2017-09-15 | 15.198617 | 141.639999 | 142.470001 | 141.550003 | 139.110001 | 133.321198 | . 460 2017-09-18 | 15.518587 | 142.619995 | 143.809998 | 142.600006 | 139.110001 | 133.321198 | . 461 2017-09-19 | 16.798471 | 143.570007 | 143.690002 | 143.089996 | 139.110001 | 133.321198 | . 462 2017-09-20 | 15.953688 | 143.529999 | 144.020004 | 143.259995 | 139.110001 | 133.321198 | . 463 2017-09-21 | 16.004491 | 144.020004 | 144.259995 | 143.479996 | 139.110001 | 133.321198 | . 464 2017-09-22 | 16.997600 | 143.669998 | 144.669998 | 143.559998 | 139.110001 | 133.321198 | . 577 2018-03-07 | 18.885411 | 154.460007 | 156.929993 | 157.220001 | 156.740005 | 151.151840 | . 671 2018-07-16 | 20.010313 | 167.759995 | 168.029999 | 169.960007 | 166.770004 | 161.779312 | . 739 2018-10-19 | 17.461385 | 155.470001 | 156.360001 | 154.740005 | 153.360001 | 149.165390 | . 892 2019-06-10 | 19.055083 | 151.449997 | 153.139999 | 152.449997 | 151.750000 | 148.488159 | . 966 2019-09-24 | 18.630976 | 155.149994 | 155.289993 | 152.839996 | 152.429993 | 150.205444 | . df[(df[&#39;Close&#39;] == 139.110001)] . Date Signal Open High Low Close Adj Close . 313 2017-02-17 | 16.635032 | 138.449997 | 139.160004 | 138.250000 | 139.110001 | 132.366592 | . 453 2017-09-07 | 16.478500 | 139.589996 | 139.690002 | 138.589996 | 139.110001 | 133.321198 | . 454 2017-09-08 | 15.518587 | 138.929993 | 139.770004 | 138.619995 | 139.110001 | 133.321198 | . 455 2017-09-11 | 15.838558 | 140.389999 | 140.919998 | 140.229996 | 139.110001 | 133.321198 | . 456 2017-09-12 | 15.518587 | 141.039993 | 141.690002 | 140.820007 | 139.110001 | 133.321198 | . 457 2017-09-13 | 16.158529 | 141.410004 | 142.220001 | 141.320007 | 139.110001 | 133.321198 | . 458 2017-09-14 | 16.478500 | 141.779999 | 142.160004 | 141.419998 | 139.110001 | 133.321198 | . 459 2017-09-15 | 15.198617 | 141.639999 | 142.470001 | 141.550003 | 139.110001 | 133.321198 | . 460 2017-09-18 | 15.518587 | 142.619995 | 143.809998 | 142.600006 | 139.110001 | 133.321198 | . 461 2017-09-19 | 16.798471 | 143.570007 | 143.690002 | 143.089996 | 139.110001 | 133.321198 | . 462 2017-09-20 | 15.953688 | 143.529999 | 144.020004 | 143.259995 | 139.110001 | 133.321198 | . 463 2017-09-21 | 16.004491 | 144.020004 | 144.259995 | 143.479996 | 139.110001 | 133.321198 | . 464 2017-09-22 | 16.997600 | 143.669998 | 144.669998 | 143.559998 | 139.110001 | 133.321198 | . df[(df[&#39;Close&#39;] == 139.110001) &amp; ~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 455 2017-09-11 | 15.838558 | 140.389999 | 140.919998 | 140.229996 | 139.110001 | 133.321198 | . 456 2017-09-12 | 15.518587 | 141.039993 | 141.690002 | 140.820007 | 139.110001 | 133.321198 | . 457 2017-09-13 | 16.158529 | 141.410004 | 142.220001 | 141.320007 | 139.110001 | 133.321198 | . 458 2017-09-14 | 16.478500 | 141.779999 | 142.160004 | 141.419998 | 139.110001 | 133.321198 | . 459 2017-09-15 | 15.198617 | 141.639999 | 142.470001 | 141.550003 | 139.110001 | 133.321198 | . 460 2017-09-18 | 15.518587 | 142.619995 | 143.809998 | 142.600006 | 139.110001 | 133.321198 | . 461 2017-09-19 | 16.798471 | 143.570007 | 143.690002 | 143.089996 | 139.110001 | 133.321198 | . 462 2017-09-20 | 15.953688 | 143.529999 | 144.020004 | 143.259995 | 139.110001 | 133.321198 | . 463 2017-09-21 | 16.004491 | 144.020004 | 144.259995 | 143.479996 | 139.110001 | 133.321198 | . 464 2017-09-22 | 16.997600 | 143.669998 | 144.669998 | 143.559998 | 139.110001 | 133.321198 | . Comments . Close data looks to have been filled forward for several dates with 139.110001 | To use close as response variable I&#39;d recommend filling it with the next days open for these instances | . df[~(df[&#39;Open&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 407 2017-07-04 | 15.282748 | 141.339996 | 142.600000 | 141.400003 | 142.200006 | 135.700998 | . 577 2018-03-07 | 18.885411 | 154.460007 | 156.929993 | 157.220001 | 156.740005 | 151.151840 | . 671 2018-07-16 | 20.010313 | 167.759995 | 168.029999 | 169.960007 | 166.770004 | 161.779312 | . 892 2019-06-10 | 19.055083 | 151.449997 | 153.139999 | 152.449997 | 151.750000 | 148.488159 | . df[~(df[&#39;High&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 577 2018-03-07 | 18.885411 | 154.460007 | 156.929993 | 157.220001 | 156.740005 | 151.151840 | . 671 2018-07-16 | 20.010313 | 167.759995 | 168.029999 | 169.960007 | 166.770004 | 161.779312 | . Comments . High and Low data doesn&#39;t look reliable, but if we don&#39;t plan to use it we can disregard this for now | The easiest HACK is to set Low = Open in these cases | . Review Signal Predictive Power . Preprocessing / Cleanup . df = df.sort_values(by=&#39;Date&#39;) # shift open price forward 1 day df[&#39;Open +1D&#39;] = df[&#39;Open&#39;].shift(1) # Add Return Columns for a Stationary Response df[&#39;Open +1D Return&#39;] = df[&#39;Open +1D&#39;].pct_change() df[&#39;Close Return&#39;] = df[&#39;Close&#39;].pct_change() # drop last 6 zero rows from signal df = df[df[&#39;Signal&#39;] != 0.0] # drop highest value from Signal df = df[df[&#39;Signal&#39;] != df[&#39;Signal&#39;].max()] # fill the highest value of Close with the next mornings open max_close_iloc = df[df[&#39;Close&#39;] == df[&#39;Close&#39;].max()].index.values[0] df.loc[max_close_iloc, &#39;Close&#39;] = df.loc[max_close_iloc+1, &#39;Open&#39;] # adjusting the period where close is filled forward with the next days open price instead df.loc[(df[&#39;Close&#39;] == 139.110001) &amp; ~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;]), &#39;Close&#39;] = df.loc[(df[&#39;Close&#39;] == 139.110001) &amp; ~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;]), &#39;Open +1D&#39;] # set date as index df = df.set_index(&#39;Date&#39;) . Predictor &amp; Response . x will be signal | y_1 will be the cleaned up close column | y_2 will be the naturally cleaner open column | . Train &amp; dev set will be split at 20% as there is not a huge amount of data points. The data will be split without shuffling to capture the most recent values in the dev set . x_1, y_1, y_2 = df[&#39;Signal&#39;], df[&#39;Close&#39;], df[&#39;Open +1D&#39;] y_2 = y_2.dropna() x_2 = x_1[y_2.index] train_x_1, dev_x_1, train_y_1, dev_y_1 = train_test_split(x_1, y_1, test_size=0.2, shuffle=False) train_x_2, dev_x_2, train_y_2, dev_y_2 = train_test_split(x_2, y_2, test_size=0.2, shuffle=False) . Analysis . Signal vs Close Plot . visualizer = JointPlotVisualizer(size=(1080, 720)) visualizer.fit_transform(x_1, y_1) visualizer.show() . &lt;AxesSubplot:xlabel=&#39;x&#39;, ylabel=&#39;y&#39;&gt; . Signal vs Open +1D Plot . visualizer = JointPlotVisualizer(size=(1080, 720)) visualizer.fit_transform(x_2, y_2) visualizer.show() . &lt;AxesSubplot:xlabel=&#39;x&#39;, ylabel=&#39;y&#39;&gt; . Train a Simple Linear Regression and Evaluate Residual Plot . visualizer = ResidualsPlot(LinearRegression(),size=(1080, 720)) visualizer.fit(train_x_1.values.reshape(-1, 1), train_y_1.values.reshape(-1, 1)) visualizer.score(dev_x_1.values.reshape(-1, 1), dev_y_1.values.reshape(-1, 1)) visualizer.show() . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Residuals for LinearRegression Model&#39;}, xlabel=&#39;Predicted Value&#39;, ylabel=&#39;Residuals&#39;&gt; . visualizer = ResidualsPlot(LinearRegression(),size=(1080, 720)) visualizer.fit(train_x_2.values.reshape(-1, 1), train_y_2.values.reshape(-1, 1)) visualizer.score(dev_x_2.values.reshape(-1, 1), dev_y_2.values.reshape(-1, 1)) visualizer.show() . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Residuals for LinearRegression Model&#39;}, xlabel=&#39;Predicted Value&#39;, ylabel=&#39;Residuals&#39;&gt; . Review Findings with Tree Models . there are less statistical assumptions to violate, so is a good check on the linear | . ebm = ExplainableBoostingRegressor() ebm.fit(train_x_2.to_frame(), train_y_2) rt = RegressionTree() rt.fit(train_x_2.to_frame(), train_y_2) . &lt;interpret.glassbox.decisiontree.RegressionTree at 0x2842704ccd0&gt; . ebm_perf = RegressionPerf(ebm.predict).explain_perf(dev_x_2.to_frame(), dev_y_2, name=&#39;Boosting Tree&#39;) rt_perf = RegressionPerf(rt.predict).explain_perf(dev_x_2.to_frame(), dev_y_2, name=&#39;Regression Tree&#39;) . show(rt_perf) show(ebm_perf) . &lt;iframe src=&quot;http://127.0.0.1:7001/2766622033376/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; &lt;iframe src=&quot;http://127.0.0.1:7001/2766622034096/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; Check Global Tree . ebm_global = ebm.explain_global(name=&#39;Boosting Tree&#39;) rt_global = rt.explain_global(name=&#39;Regression Tree&#39;) . show(ebm_global) show(rt_global) . &lt;iframe src=&quot;http://127.0.0.1:7001/2766622408560/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; &lt;iframe src=&quot;http://127.0.0.1:7001/2766666354592/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt;",
            "url": "https://kslader8.github.io/kslader8-thoughts/2021/01/07/sample-eda.html",
            "relUrl": "/2021/01/07/sample-eda.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "HackerRank Question - Transitive Clouser of a Graph",
            "content": "Compute the Transitive Closure of a Graph . After doing a little reading, it appears that Warshall&#39;s method is the most common way of doing this in practice . there is a path from i to j going through vertex 1 | there is a path from i to j going through vertex 1 and/or 2 | there is a path from i to j going through vertex 1, 2, and/or 3 | there is a path from i to j going through any of the other vertices | . The time complexity of this algorithm is same as that of Floyd–Warshall algorithm i.e. O(3) but it reduces storage by retaining only one bit for each matrix element (e.g. we can use bool data-type instead of int). The implementation . import pandas as pd from scipy import sparse . graph = sparse.csr_matrix( [ [1,1,0,0], [1,1,0,0], [0,0,1,0], [0,0,0,1] ]) df = pd.DataFrame.sparse.from_spmatrix(graph) df . 0 1 2 3 . 0 1 | 1 | 0 | 0 | . 1 1 | 1 | 0 | 0 | . 2 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 1 | . groups, labels = sparse.csgraph.connected_components(graph, directed=False) groups . 3 .",
            "url": "https://kslader8.github.io/kslader8-thoughts/hacker-rank/2020/12/27/hackerrank-question.html",
            "relUrl": "/hacker-rank/2020/12/27/hackerrank-question.html",
            "date": " • Dec 27, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hi, my name is Kevin - the minion - from films such as Despicable Me, Despicable Me 2, Minions, etc. . What you may not know is that I am also a data science lead for (Societe Generale’s) SG (Corporate Investment Bank’s) CIB’s digital office. Diligently toiling away determining &amp; applying data driven strategy to the bank. Working into the wee hours analyzing and automating conversations in capital markets. . Thank you and Goodnight! . Minion, Consultant, Developer, Trader, Data Scientist ~ Kevin .",
          "url": "https://kslader8.github.io/kslader8-thoughts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kslader8.github.io/kslader8-thoughts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}