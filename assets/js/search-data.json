{
  
    
        "post0": {
            "title": "Azure ML Workspace - [Tensorflow] Time Series Example | Continuation 2",
            "content": "This notebook is the same as the previous notebook. The goals is simply to update it to make a few enhancements that became apparent during the process of making the first notebook. . Enhancements . Implement multiple step prediciton model | splitting pipeline into two steps: prep_data and train | rendering logged plots | switching to table interface for logging scoring metrics instead of list | . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline %config Completer.use_jedi = False . Libraries . import os import datetime as dt from PIL import Image from pathlib import Path . from dotenv import load_dotenv . import pandas as pd import numpy as np import tensorflow as tf import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt print(f&quot;pandas version {pd.__version__}&quot;) print(f&quot;numpy version {np.__version__}&quot;) print(f&quot;tensorflow version {tf.__version__}&quot;) . pandas version 1.2.3 numpy version 1.19.2 tensorflow version 2.3.0 . import azureml.core as aml from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment, Run from azureml.core import Datastore, Dataset from azureml.core.compute import ComputeTarget, AmlCompute from azureml.core.runconfig import RunConfiguration from azureml.core.conda_dependencies import CondaDependencies from azureml.core import Model from azureml.core.resource_configuration import ResourceConfiguration from azureml.data import OutputFileDatasetConfig from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.widgets import RunDetails print(f&quot;azureml version {aml.__version__}&quot;) . azureml version 1.25.0 . import tensorboard from azureml.tensorboard import Tensorboard print(f&quot;tensorboard version {tensorboard.__version__}&quot;) . tensorboard version 2.4.0 . import twelvedata from twelvedata import TDClient print(f&quot;twelvedata version {twelvedata.__version__}&quot;) . twelvedata version 1.1.8 . Project Environment Variables . This is a personal preference of mine to make a .env file per project to encapsulate tokens/secrets/etc outside of notebooks. . In this case I created a file named .env with a single variable apikey=(api key) in the same directory as my experiment. . env_path = Path(&quot;../.env&quot;) assert env_path.exists() _ = load_dotenv(env_path) . Matplotlib . It&#39;s useful to set a few global plotting defaults to save from doing them for every plot in a notebook . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 8) mpl.rcParams[&#39;axes.grid&#39;] = False . Azure ML Workspace . To setup an Azure ML Workspace you will need an azure account (with credit card). To spin it up simply go to https://portal.azure.com/ and type machine learning in the search bar and create a workspace. . Once you have a workspace you will need to download the config.json prior to going to https://ml.azure.com/ to access your workspace . workspace_config_path = Path(&quot;../config.json&quot;) assert workspace_config_path.exists() ws = Workspace.from_config(path=workspace_config_path) . Twelve Data Client . I setup an account at https://twelvedata.com/ to get a free api key to try it out. I had not heard of it before, but it was the first thing that came up in my google search for free market data... . apikey = os.environ.get(&quot;apikey&quot;) td = TDClient(apikey=apikey) . ML Workspace Compute . Get existing compute cluster or create one . compute_name = &quot;aml-compute&quot; vm_size = &quot;Standard_NC6&quot; # vm_size = &quot;Standard_NC6s_v3&quot; if compute_name in ws.compute_targets: compute_target = ws.compute_targets[compute_name] if compute_target and type(compute_target) is AmlCompute: print(&#39;Found compute target: &#39; + compute_name) else: print(&#39;Creating a new compute target...&#39;) provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size, # STANDARD_NC6 is GPU-enabled min_nodes=0, max_nodes=4) # create the compute target compute_target = ComputeTarget.create( ws, compute_name, provisioning_config) # Can poll for a minimum number of nodes and for a specific timeout. # If no min node count is provided it will use the scale settings for the cluster compute_target.wait_for_completion( show_output=True, min_node_count=None, timeout_in_minutes=20) # For a more detailed view of current cluster status, use the &#39;status&#39; property print(compute_target.status.serialize()) . Creating a new compute target... Creating... SucceededProvisioning operation finished, operation &#34;Succeeded&#34; Succeeded AmlCompute wait for completion finished Minimum number of nodes requested have been provisioned {&#39;currentNodeCount&#39;: 0, &#39;targetNodeCount&#39;: 0, &#39;nodeStateCounts&#39;: {&#39;preparingNodeCount&#39;: 0, &#39;runningNodeCount&#39;: 0, &#39;idleNodeCount&#39;: 0, &#39;unusableNodeCount&#39;: 0, &#39;leavingNodeCount&#39;: 0, &#39;preemptedNodeCount&#39;: 0}, &#39;allocationState&#39;: &#39;Steady&#39;, &#39;allocationStateTransitionTime&#39;: &#39;2021-04-06T17:41:16.106000+00:00&#39;, &#39;errors&#39;: None, &#39;creationTime&#39;: &#39;2021-04-06T17:41:13.362593+00:00&#39;, &#39;modifiedTime&#39;: &#39;2021-04-06T17:41:28.964771+00:00&#39;, &#39;provisioningState&#39;: &#39;Succeeded&#39;, &#39;provisioningStateTransitionTime&#39;: None, &#39;scaleSettings&#39;: {&#39;minNodeCount&#39;: 0, &#39;maxNodeCount&#39;: 4, &#39;nodeIdleTimeBeforeScaleDown&#39;: &#39;PT120S&#39;}, &#39;vmPriority&#39;: &#39;Dedicated&#39;, &#39;vmSize&#39;: &#39;STANDARD_NC6&#39;} . ML Workspace Data . TwelveData . List ETFs Available . etf_data = td.get_etf_list() etf_list = etf_data.as_json() etf_df = pd.DataFrame(etf_list) etf_df.head() . symbol name currency exchange . 0 8PSG | Invesco Physical Gold ETC | EUR | XETR | . 1 AAA | BetaShares Australian High Interest Cash ETF | AUD | ASX | . 2 AAAU | Perth Mint Physical Gold ETF | USD | NYSE | . 3 AADR | AdvisorShares Dorsey Wright ADR ETF | USD | NYSE | . 4 AASF | Airlie Australian Share Fund -- ETF Feeder | AUD | ASX | . Get ETF Time Series . end_date = pd.Timestamp(dt.datetime.today()) start_date = end_date - pd.tseries.offsets.BDay(252) * 2 start_date.to_pydatetime().date(), end_date.to_pydatetime().date() . (datetime.date(2019, 5, 1), datetime.date(2021, 4, 6)) . ticker = &quot;IVOL&quot; ts = td.time_series( symbol=ticker, interval=&quot;1day&quot;, start_date=start_date, end_date=end_date, outputsize=500 ) . df = ts.with_ema().with_vwap().as_pandas() df.describe() . open high low close volume ema vwap . count 475.00000 | 475.000000 | 475.000000 | 475.000000 | 4.750000e+02 | 475.000000 | 475.000000 | . mean 26.60264 | 26.671625 | 26.540325 | 26.608292 | 4.910146e+06 | 26.156760 | 26.606747 | . std 1.12284 | 1.116020 | 1.137535 | 1.123807 | 9.894917e+07 | 3.595103 | 1.123732 | . min 24.05000 | 24.727000 | 24.050000 | 24.440000 | 0.000000e+00 | 0.000000 | 24.405670 | . 25% 25.50000 | 25.579500 | 25.450000 | 25.530000 | 1.120000e+04 | 25.547135 | 25.519500 | . 50% 26.58000 | 26.750000 | 26.510000 | 26.600000 | 4.390000e+04 | 26.538130 | 26.592400 | . 75% 27.40000 | 27.465000 | 27.370000 | 27.410000 | 4.084390e+05 | 27.423495 | 27.421665 | . max 28.95000 | 28.950000 | 28.820000 | 28.945000 | 2.156860e+09 | 28.736060 | 28.905000 | . df.head().reset_index() . datetime open high low close volume ema vwap . 0 2021-04-06 | 28.4200 | 28.4500 | 28.42 | 28.420 | 703070 | 28.51697 | 28.43000 | . 1 2021-04-05 | 28.4295 | 28.4800 | 28.40 | 28.400 | 1569462 | 28.54121 | 28.42667 | . 2 2021-04-01 | 28.4861 | 28.5125 | 28.41 | 28.420 | 3013907 | 28.57651 | 28.44750 | . 3 2021-03-31 | 28.5841 | 28.6294 | 28.53 | 28.590 | 2156860498 | 28.61564 | 28.58313 | . 4 2021-03-30 | 28.5000 | 28.5500 | 28.42 | 28.505 | 5507839 | 28.62205 | 28.49167 | . Azure . Azure Workspace Datastore . data_store = ws.get_default_datastore() . Upload ETF Dataset . def get_or_upload_df(ws, data_store, df, ticker): dataset_name = f&#39;{ticker.lower()}_ds&#39; try: ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() except: Dataset.Tabular.register_pandas_dataframe(df, data_store, dataset_name) ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() return df aml_df = get_or_upload_df(ws, data_store, df.reset_index(), ticker) aml_df.head() . datetime open high low close volume ema vwap . 0 2021-04-01 04:00:00 | 28.4861 | 28.5125 | 28.4100 | 28.420 | 3013907 | 28.57651 | 28.44750 | . 1 2021-03-31 04:00:00 | 28.5841 | 28.6294 | 28.5300 | 28.590 | 2156860498 | 28.61564 | 28.58313 | . 2 2021-03-30 04:00:00 | 28.5000 | 28.5500 | 28.4200 | 28.505 | 5507839 | 28.62205 | 28.49167 | . 3 2021-03-29 04:00:00 | 28.7000 | 28.7200 | 28.6300 | 28.650 | 949622 | 28.65131 | 28.66667 | . 4 2021-03-26 04:00:00 | 28.7153 | 28.7790 | 28.7153 | 28.740 | 1108546 | 28.65164 | 28.74477 | . Training . Create Training Script . src_dir = &#39;aml-exp-multi&#39; aml_exp = Path(src_dir) if not aml_exp.exists(): aml_exp.mkdir() . %%writefile aml-exp-multi/data-prep.py # Standard Libraries import argparse import datetime as dt from pathlib import Path # 3rd Party Libraries import numpy as np import pandas as pd # Plotting Libraries import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt # Azure ML Libraries from azureml.core import Run from azureml.core import Dataset # ML Run run = Run.get_context() workspace = run.experiment.workspace # Read in Args parser = argparse.ArgumentParser() parser.add_argument(&#39;--input_dataset&#39;, dest=&#39;input_dataset&#39;, required=True) parser.add_argument(&#39;--train_dataset&#39;, dest=&#39;train_dataset&#39;, required=True) parser.add_argument(&#39;--val_dataset&#39;, dest=&#39;val_dataset&#39;, required=True) parser.add_argument(&#39;--test_dataset&#39;, dest=&#39;test_dataset&#39;, required=True) args = parser.parse_args() # Dataset &amp; Prep ds = run.input_datasets[&#39;input_dataset&#39;] df = ds.to_pandas_dataframe() # Date Feature Prep day = 24*60*60 year = (365.2425)*day date_time = pd.to_datetime(df.datetime) timestamp_s = date_time.map(dt.datetime.timestamp) df[&#39;day_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / day)) df[&#39;day_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / day)) df[&#39;year_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / year)) df[&#39;year_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / year)) # Data Filter features = [&#39;day_sin&#39;, &#39;day_cos&#39;, &#39;low&#39;, &#39;high&#39;, &#39;volume&#39;, &#39;ema&#39;, &#39;vwap&#39;] target = &#39;close&#39; columns = features + [target] df = df[columns] num_features = df.shape[1] # Data Splitting n = len(df) train_df = df[0:int(n*0.6)] val_df = df[int(n*0.6):int(n*0.8)] test_df = df[int(n*0.8):] # Data Normalization train_mean = train_df.mean() train_std = train_df.std() train_df = (train_df - train_mean) / train_std val_df = (val_df - train_mean) / train_std test_df = (test_df - train_mean) / train_std # Data Distribution Check df_std = (df - train_mean) / train_std df_std = df_std.melt(var_name=&#39;Column&#39;, value_name=&#39;Normalized&#39;) plt.figure(figsize=(12, 6)) ax = sns.violinplot(x=&#39;Column&#39;, y=&#39;Normalized&#39;, data=df_std) _ = ax.set_xticklabels(df.keys(), rotation=45) run.log_image(&#39;feature_distribution_check&#39;, plot=plt) print(run.output_datasets) print(type(args.train_dataset), args.train_dataset) with (Path(args.train_dataset) / &#39;train.csv&#39;).open(&#39;w&#39;) as f: train_df.to_csv(f) with (Path(args.val_dataset)/ &#39;val.csv&#39;).open(&#39;w&#39;) as f: val_df.to_csv(f) with (Path(args.test_dataset) / &#39;test.csv&#39;).open(&#39;w&#39;) as f: test_df.to_csv(f) . Overwriting aml-exp-multi/data-prep.py . %%writefile aml-exp-multi/train.py # Standard Libraries import argparse import json import os import datetime as dt from functools import partial # 3rd Party Libraries import numpy as np import pandas as pd import tensorflow as tf # Plotting Libraries import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt # Azure ML Libraries from azureml.core import Run from azureml.core import Dataset, Datastore from azureml.data.datapath import DataPath from azureml.core import Model from azureml.tensorboard.export import export_to_tensorboard from sklearn.metrics import confusion_matrix # Classes class WindowGenerator(): def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None): # Store the raw data. self.train_df = train_df self.val_df = val_df self.test_df = test_df # Work out the label column indices. self.label_columns = label_columns if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(train_df.columns)} # Work out the window parameters. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(0, input_width) self.input_indices = np.arange(self.total_window_size)[self.input_slice] self.label_start = self.total_window_size - self.label_width self.labels_slice = slice(self.label_start, None) self.label_indices = np.arange(self.total_window_size)[self.labels_slice] def __repr__(self): return &#39; n&#39;.join([ f&#39;Total window size: {self.total_window_size}&#39;, f&#39;Input indices: {self.input_indices}&#39;, f&#39;Label indices: {self.label_indices}&#39;, f&#39;Label column name(s): {self.label_columns}&#39;]) @property def train(self): return self.make_dataset(self.train_df) @property def val(self): return self.make_dataset(self.val_df) @property def test(self): return self.make_dataset(self.test_df) @property def example(self): &quot;&quot;&quot;Get and cache an example batch of `inputs, labels` for plotting.&quot;&quot;&quot; result = getattr(self, &#39;_example&#39;, None) if result is None: # No example batch was found, so get one from the `.train` dataset result = next(iter(self.train)) # And cache it for next time self._example = result return result def split_window(self, features): inputs = features[:, self.input_slice, :] labels = features[:, self.labels_slice, :] if self.label_columns is not None: labels = tf.stack( [labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1) # Slicing doesn&#39;t preserve static shape information, so set the shapes # manually. This way the `tf.data.Datasets` are easier to inspect. inputs.set_shape([None, self.input_width, None]) labels.set_shape([None, self.label_width, None]) return inputs, labels def make_dataset(self, data): data = np.array(data, dtype=np.float32) ds = tf.keras.preprocessing.timeseries_dataset_from_array( data=data, targets=None, sequence_length=self.total_window_size, sequence_stride=1, shuffle=True, batch_size=32,) ds = ds.map(self.split_window) return ds def plot(self, plot_col, model=None, max_subplots=3): plt.figure(figsize=(12, 8)) plot_col_index = self.column_indices[plot_col] inputs, labels = self.example max_n = min(max_subplots, len(inputs)) for n in range(max_n): plt.subplot(max_n, 1, n+1) plt.ylabel(f&#39;{plot_col} [normed]&#39;) plt.plot(self.input_indices, inputs[n, :, plot_col_index], label=&#39;Inputs&#39;, marker=&#39;.&#39;, zorder=-10) if self.label_columns: label_col_index = self.label_columns_indices.get(plot_col, None) else: label_col_index = plot_col_index if label_col_index is None: continue plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors=&#39;k&#39;, label=&#39;Labels&#39;, c=&#39;#2ca02c&#39;, s=64) if model is not None: predictions = model(inputs) plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker=&#39;X&#39;, edgecolors=&#39;k&#39;, label=&#39;Predictions&#39;, c=&#39;#ff7f0e&#39;, s=64) if n == 0: plt.legend() plt.xlabel(&#39;Time&#39;) class MultiStepLastBaseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1]) return tf.tile(tf.expand_dims(inputs[:, -1:, self.label_index], -1), [1, OUT_STEPS, 1]) class RepeatBaseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return inputs result = inputs[:, :, self.label_index] return result[:, :, tf.newaxis] class FeedBack(tf.keras.Model): def __init__(self, units, out_steps, num_features): super().__init__() self.out_steps = out_steps self.units = units self.lstm_cell = tf.keras.layers.LSTMCell(units) # Also wrap the LSTMCell in an RNN to simplify the `warmup` method. self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True) self.dense = tf.keras.layers.Dense(num_features) def warmup(self, inputs): # inputs.shape =&gt; (batch, time, features) # x.shape =&gt; (batch, lstm_units) x, *state = self.lstm_rnn(inputs) # predictions.shape =&gt; (batch, features) prediction = self.dense(x) return prediction, state def call(self, inputs, training=None): # Use a TensorArray to capture dynamically unrolled outputs. predictions = [] # Initialize the lstm state prediction, state = self.warmup(inputs) # Insert the first prediction predictions.append(prediction) # Run the rest of the prediction steps for n in range(1, self.out_steps): # Use the last prediction as input. x = prediction # Execute one lstm step. x, state = self.lstm_cell(x, states=state, training=training) # Convert the lstm output to a prediction. prediction = self.dense(x) # Add the prediction to the output predictions.append(prediction) # predictions.shape =&gt; (time, batch, features) predictions = tf.stack(predictions) # predictions.shape =&gt; (batch, time, features) predictions = tf.transpose(predictions, [1, 0, 2]) return predictions # Global Variables MAX_EPOCHS = 20 OUT_STEPS = 24 CONV_WIDTH = 3 # Paths os.makedirs(&#39;./outputs&#39;, exist_ok=True) os.makedirs(&#39;./outputs/model&#39;, exist_ok=True) os.makedirs(&#39;./outputs/log&#39;, exist_ok=True) # Read in Args parser = argparse.ArgumentParser() parser.add_argument(&#39;--train_dataset&#39;, dest=&#39;train_dataset&#39;, required=True) parser.add_argument(&#39;--val_dataset&#39;, dest=&#39;val_dataset&#39;, required=True) parser.add_argument(&#39;--test_dataset&#39;, dest=&#39;test_dataset&#39;, required=True) args = parser.parse_args() # ML Run run = Run.get_context() workspace = run.experiment.workspace datastore = workspace.get_default_datastore() train_ds = run.input_datasets[&#39;train_dataset&#39;] val_ds = run.input_datasets[&#39;val_dataset&#39;] test_ds = run.input_datasets[&#39;test_dataset&#39;] train_df = train_ds.to_pandas_dataframe() val_df = val_ds.to_pandas_dataframe() test_df = test_ds.to_pandas_dataframe() target = train_df.columns[-1] num_features = train_df.shape[1] # Data Windows multi_window = WindowGenerator(input_width=24, label_width=OUT_STEPS, shift=OUT_STEPS, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) multi_window.plot(target) run.log_image(f&#39;{target}_variable&#39;, plot=plt) # Baseline val_performance, tst_performance = {}, {} def log_result(name, model, window, target, vals, tsts): vals[name] = model.evaluate(window.val) tsts[name] = model.evaluate(window.test, verbose=0) window.plot(target, model) run.log_image(f&#39;{name}_pred&#39;, plot=plt) tf.saved_model.save(model, f&#39;outputs/model/{name}&#39;) log_model = partial(log_result, window=multi_window, target=target, vals=val_performance, tsts=tst_performance) print(f&quot;target indice: {multi_window.column_indices.get(target)}&quot;) last_baseline = MultiStepLastBaseline(-1) last_baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) log_model(&#39;last_baseline&#39;, last_baseline) repeat_baseline = RepeatBaseline(-1) repeat_baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) log_model(&#39;repeat_baseline&#39;, repeat_baseline) # Train Models def compile_and_fit(model, window, patience=4): early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=patience, mode=&#39;min&#39;) model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=[tf.metrics.MeanAbsoluteError()]) history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping]) model.summary() return history compile_and_fit_multi = partial(compile_and_fit, window=multi_window) # Train Linear Model linear = tf.keras.Sequential([ # Take the last time-step. # Shape [batch, time, features] =&gt; [batch, 1, features] tf.keras.layers.Lambda(lambda x: x[:, -1:, :]), # Shape =&gt; [batch, 1, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(linear) log_model(&#39;linear&#39;, linear) # Train Dense Model dense = tf.keras.Sequential([ # Take the last time step. # Shape [batch, time, features] =&gt; [batch, 1, features] tf.keras.layers.Lambda(lambda x: x[:, -1:, :]), # Shape =&gt; [batch, 1, dense_units] tf.keras.layers.Dense(512, activation=&#39;relu&#39;), # Shape =&gt; [batch, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(dense) log_model(&#39;dense&#39;, dense) # Train Conv Model conv = tf.keras.Sequential([ # Shape [batch, time, features] =&gt; [batch, CONV_WIDTH, features] tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]), # Shape =&gt; [batch, 1, conv_units] tf.keras.layers.Conv1D(256, activation=&#39;relu&#39;, kernel_size=(CONV_WIDTH)), # Shape =&gt; [batch, 1, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(conv) log_model(&#39;conv&#39;, conv) # Train LSTM Model lstm = tf.keras.Sequential([ # Shape [batch, time, features] =&gt; [batch, lstm_units] # Adding more `lstm_units` just overfits more quickly. tf.keras.layers.LSTM(32, return_sequences=False), # Shape =&gt; [batch, out_steps*features] tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), # Shape =&gt; [batch, out_steps, features] tf.keras.layers.Reshape([OUT_STEPS, num_features]) ]) history = compile_and_fit_multi(lstm) log_model(&quot;lstm&quot;, lstm) # Train AR RNN Feedback Model feedback = FeedBack(units=32, out_steps=OUT_STEPS, num_features=num_features) prediction, state = feedback.warmup(multi_window.example[0]) history = compile_and_fit_multi(feedback) log_model(&#39;lstm_feedback&#39;, feedback) # Log Results &amp; Select Best Model best_model, best_score = None, None if run is not None: # log results run.log_table(&#39;mae_val&#39;, {k: v[1] for (k, v) in val_performance.items()}) run.log_table(&#39;mse_val&#39;, {k: v[0] for (k, v) in val_performance.items()}) run.log_table(&#39;mae_tst&#39;, {k: v[1] for (k, v) in tst_performance.items()}) run.log_table(&#39;mse_tst&#39;, {k: v[0] for (k, v) in tst_performance.items()}) # select best for k, v in tst_performance.items(): try: mae = float(v[1]) if best_score is None and best_model is None: best_model = k best_score = mae elif best_score &gt; mae: best_model = k best_score = mae except: continue run.log(&#39;best_model&#39;, best_model) run.log(&#39;best_score&#39;, best_score) # best_model_path = f&#39;outputs/model/{best_model}&#39; # model = run.register_model(model_name=best_model, model_path=best_model_path) . Overwriting aml-exp-multi/train.py . Setup Training Environment . aml_run_config = RunConfiguration() aml_run_config.target = compute_target aml_run_config.environment.python.user_managed_dependencies = False # Add some packages relied on by data prep step deps = CondaDependencies.create( conda_packages=[&#39;pandas&#39;,&#39;scikit-learn&#39;, &#39;matplotlib&#39;, &#39;seaborn&#39;], pip_packages=[&#39;azureml-sdk&#39;, &#39;azureml-dataprep[fuse,pandas]&#39;, &#39;azureml-pipeline&#39;, &#39;azureml.tensorboard&#39;, &#39;azureml-interpret&#39;], python_version=&#39;3.6.2&#39;, pin_sdk_version=True) deps.add_tensorflow_pip_package(core_type=&#39;gpu&#39;, version=&#39;2.3.1&#39;) aml_run_config.environment.python.conda_dependencies = deps . Build Data Prep Step . train_data = OutputFileDatasetConfig(name=&quot;train_data&quot;, destination=(data_store, &#39;train/tabular/&#39;)).read_delimited_files() val_data = OutputFileDatasetConfig(name=&quot;val_data&quot;, destination=(data_store, &#39;val/tabular/&#39;)).read_delimited_files() test_data = OutputFileDatasetConfig(name=&quot;test_data&quot;, destination=(data_store, &#39;test/tabular/&#39;)).read_delimited_files() dataset_name = f&#39;{ticker.lower()}_ds&#39; input_dataset = Dataset.get_by_name(ws, dataset_name) prep_data_step = PythonScriptStep( name=&quot;prep_data_step&quot;, source_directory=src_dir, script_name=&quot;data-prep.py&quot;, arguments=[ &quot;--input_dataset&quot;, input_dataset.as_named_input(&quot;input_dataset&quot;), &quot;--train_dataset&quot;, train_data, &quot;--val_dataset&quot;, val_data, &quot;--test_dataset&quot;, test_data ], runconfig=aml_run_config, allow_reuse=True ) . Build Train Step . train_step = PythonScriptStep( name=&quot;train_step&quot;, source_directory=src_dir, script_name=&quot;train.py&quot;, arguments=[ &quot;--train_dataset&quot;, train_data.as_input(name=&quot;train_dataset&quot;), &quot;--val_dataset&quot;, val_data.as_input(name=&quot;val_dataset&quot;), &quot;--test_dataset&quot;, test_data.as_input(name=&quot;test_dataset&quot;) ], runconfig=aml_run_config ) . Build Pipeline . steps = [prep_data_step, train_step] pipeline = Pipeline(workspace=ws, steps=steps) . Run Experiment . %%capture experiment = Experiment(ws, &#39;aml_exp_multi&#39;) script_run = experiment.submit(pipeline) script_run.wait_for_completion(show_output=False) . RunDetails(script_run).show() . Review . Azure Portal . I find it best to simply go to the experiment portal url to review from the gui. It contains all the runs from your experiment and makes it easy to review changes from a central location. . script_run.get_portal_url() . &#39;https://ml.azure.com/runs/b3e487ab-146d-4d0c-a47c-1fb9e3e534ce?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . Data Preprocessing . step_name = &#39;prep_data_step&#39; step = script_run.find_step_run(step_name)[0] step.get_portal_url() . &#39;https://ml.azure.com/runs/fff72717-d728-4ebb-8065-c3fdcaf22876?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . The most useful thing for myself is to display the images stored during the running of a step. . imgs = [f for f in step.get_file_names() if f.endswith(&#39;.png&#39;)] for img in imgs: step.download_file(img) display(img) display(Image.open(img)) Path(img).unlink() . &#39;feature_distribution_check_1617743006.png&#39; . We can see that volumne and ema have wide tails / outliers to review and potentially deal with above. . Model Metrics . However, you can choose to do the model review inside the notebook too. . The first place to look when doing this is the experiments metrics. In this example I&#39;m logging the mse and mae for validation &amp; test datasets for each model . step_name = &#39;train_step&#39; step = script_run.find_step_run(step_name)[0] step.get_portal_url() . &#39;https://ml.azure.com/runs/ddaf55c1-11fb-4a9c-8b19-987e7d61422b?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . metrics_dict = step.get_metrics() scores_df = pd.DataFrame() for k, v in metrics_dict.items(): if isinstance(v, dict): scores_df = scores_df.append(pd.DataFrame.from_dict(v, orient=&#39;index&#39;, columns=[k]).T) scores_df.T.sort_values(&#39;mae_tst&#39;) . mae_val mse_val mae_tst mse_tst . last_baseline 0.107446 | 0.023849 | 0.216114 | 0.091769 | . dense 0.913333 | 0.903850 | 0.269590 | 0.126594 | . repeat_baseline 0.128062 | 0.029194 | 0.276632 | 0.119011 | . lstm 0.816010 | 0.706341 | 0.288581 | 0.131727 | . linear 1.056055 | 1.173291 | 0.318440 | 0.144961 | . conv 0.225789 | 0.099458 | 0.431668 | 0.297038 | . lstm_feedback 0.633682 | 0.755861 | 0.457924 | 0.367349 | . Our baseline is performing really well here. Proof that you should always start simple... . imgs = [f for f in step.get_file_names() if f.endswith(&#39;.png&#39;)] for img in imgs: display(img) step.download_file(img) display(Image.open(img)) Path(img).unlink() . &#39;close_variable_1617743085.png&#39; . &#39;conv_pred_1617743096.png&#39; . &#39;dense_pred_1617743092.png&#39; . &#39;last_baseline_pred_1617743086.png&#39; . &#39;linear_pred_1617743089.png&#39; . &#39;lstm_feedback_pred_1617743118.png&#39; . &#39;lstm_pred_1617743105.png&#39; . &#39;repeat_baseline_pred_1617743087.png&#39; . Clean up . Delete Compute Cluster . This is an important step if you don&#39;t want save some money 😉 . print(&quot;starting compute cleanup&quot;) for name, compute in ws.compute_targets.items(): print(f&quot;deleting {name} instance&quot;) compute.delete() while len(ws.compute_targets.items()) != 0: continue print(&quot;compute cleanup complete&quot;) . starting compute cleanup deleting aml-compute instance compute cleanup complete .",
            "url": "https://kslader8.github.io/kslader8-thoughts/azureml/tensorflow/time-series/2021/04/11/aml-exp-continuation-2.html",
            "relUrl": "/azureml/tensorflow/time-series/2021/04/11/aml-exp-continuation-2.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Dispersion as a Hedge Against Bond Selloff?",
            "content": "Dispersion as a Hedge Against Bond Selloff? . Conjecture . When bonds selloff, investors should expect a wave of equity dispersion and subdued index volatility. . What is a dispersion trade? . Dispersion is a velative value trade comprised of: . A short position on the underlying index’s implied volatility. The lower the realized volatility the better | A long position on the index components (that are likely to have volatility spike). The higher the average realized voatility the better. | . Why trade dispersion? . Strong demand by outright investors for index performance protection (because of put options) . A relatively high implied volatiltiy on the index in relation to its components | A steeper skew on an index’s implied volatitly that that of single stocks | . Strong demand for yield enhancement products (resulting in depressed volatility) . The impacts of hedging these products pushes | The call overwritting flows on these products pushing short term vol lower on single stocks | . Dispersion trades caputre both of these dislocations! . Potential benefits of considering a dispersion as a hedge against a bond selloff . Significant secter rotation, usually leads to larger winners and losers, opening opportunities for depressed volatility levels on the index, but causing single stock volatilities to increase materially | Correlation discrepancies between the equity sector to bonds (USD 10Y Swap vs 3M RV SPX Baskets) | . Choosing the right dispersion . Two ways to structure the dispersion: . Vega Flat - the index vega notional is set equal to the total vega notional of the stocks | Theta Flat - the index vega notional is overweight and higher than the total vega notional on the stocks e.g. index vol: 20% &amp; stocks weighted vol: 30% » vega notional: 30/20= 1.5x the notional on the stocks | . | . Two market dynamics . Bullish Market - Theta Flat outperforms due to the extra short index volatility normally | Bearish Market - Vega Flat is more resilient thanks to net long volatitily position | .",
            "url": "https://kslader8.github.io/kslader8-thoughts/trading/2021/04/09/dispersion-bond-selloff.html",
            "relUrl": "/trading/2021/04/09/dispersion-bond-selloff.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Azure ML Workspace - [Tensorflow] Time Series Example | Continuation 1",
            "content": "This notebook is the same as the previous notebook. The goals is simply to update it to make a few enhancements that became apparent during the process of making the first notebook. . Enhancements . Start using the pipeline api | Adding plots for all models | . Bug Fixes . Normalizing input data | Increasing the test data set for lstm testing &amp; performance plot | . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline . Libraries . import os import datetime as dt from pathlib import Path . from dotenv import load_dotenv . import pandas as pd import numpy as np # import tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt print(f&quot;pandas version {pd.__version__}&quot;) print(f&quot;numpy version {np.__version__}&quot;) # print(f&quot;tensorflow version {tf.__version__}&quot;) . pandas version 1.2.0 numpy version 1.18.5 . import azureml.core as aml from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment, Run from azureml.core import Datastore, Dataset from azureml.core.compute import ComputeTarget, AmlCompute from azureml.core.runconfig import RunConfiguration from azureml.core.conda_dependencies import CondaDependencies from azureml.core import Model from azureml.core.resource_configuration import ResourceConfiguration from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.widgets import RunDetails print(f&quot;azureml version {aml.__version__}&quot;) . azureml version 1.25.0 . import tensorboard from azureml.tensorboard import Tensorboard print(f&quot;tensorboard version {tensorboard.__version__}&quot;) . tensorboard version 2.4.1 . import twelvedata from twelvedata import TDClient print(f&quot;twelvedata version {twelvedata.__version__}&quot;) . twelvedata version 1.1.7 . Project Environment Variables . This is a personal preference of mine to make a .env file per project to encapsulate tokens/secrets/etc outside of notebooks. . In this case I created a file named .env with a single variable apikey=(api key) in the same directory as my experiment. . env_path = Path(&quot;.env&quot;) assert env_path.exists() _ = load_dotenv(env_path) . Matplotlib . It&#39;s useful to set a few global plotting defaults to save from doing them for every plot in a notebook . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 8) mpl.rcParams[&#39;axes.grid&#39;] = False . Azure ML Workspace . To setup an Azure ML Workspace you will need an azure account (with credit card). To spin it up simply go to https://portal.azure.com/ and type machine learning in the search bar and create a workspace. . Once you have a workspace you will need to download the config.json prior to going to https://ml.azure.com/ to access your workspace . workspace_config_path = Path(&quot;config.json&quot;) assert workspace_config_path.exists() ws = Workspace.from_config(path=workspace_config_path) . Twelve Data Client . I setup an account at https://twelvedata.com/ to get a free api key to try it out. I had not heard of it before, but it was the first thing that came up in my google search for free market data... . apikey = os.environ.get(&quot;apikey&quot;) td = TDClient(apikey=apikey) . ML Workspace Compute . Get existing compute cluster or create one . compute_name = &quot;aml-compute&quot; vm_size = &quot;Standard_NC6&quot; # vm_size = &quot;Standard_NC6s_v3&quot; if compute_name in ws.compute_targets: compute_target = ws.compute_targets[compute_name] if compute_target and type(compute_target) is AmlCompute: print(&#39;Found compute target: &#39; + compute_name) else: print(&#39;Creating a new compute target...&#39;) provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size, # STANDARD_NC6 is GPU-enabled min_nodes=0, max_nodes=4) # create the compute target compute_target = ComputeTarget.create( ws, compute_name, provisioning_config) # Can poll for a minimum number of nodes and for a specific timeout. # If no min node count is provided it will use the scale settings for the cluster compute_target.wait_for_completion( show_output=True, min_node_count=None, timeout_in_minutes=20) # For a more detailed view of current cluster status, use the &#39;status&#39; property print(compute_target.status.serialize()) . Creating a new compute target... Creating.... SucceededProvisioning operation finished, operation &#34;Succeeded&#34; Succeeded AmlCompute wait for completion finished Minimum number of nodes requested have been provisioned {&#39;currentNodeCount&#39;: 0, &#39;targetNodeCount&#39;: 0, &#39;nodeStateCounts&#39;: {&#39;preparingNodeCount&#39;: 0, &#39;runningNodeCount&#39;: 0, &#39;idleNodeCount&#39;: 0, &#39;unusableNodeCount&#39;: 0, &#39;leavingNodeCount&#39;: 0, &#39;preemptedNodeCount&#39;: 0}, &#39;allocationState&#39;: &#39;Steady&#39;, &#39;allocationStateTransitionTime&#39;: &#39;2021-04-02T20:04:47.531000+00:00&#39;, &#39;errors&#39;: None, &#39;creationTime&#39;: &#39;2021-04-02T20:04:42.205768+00:00&#39;, &#39;modifiedTime&#39;: &#39;2021-04-02T20:04:59.941680+00:00&#39;, &#39;provisioningState&#39;: &#39;Succeeded&#39;, &#39;provisioningStateTransitionTime&#39;: None, &#39;scaleSettings&#39;: {&#39;minNodeCount&#39;: 0, &#39;maxNodeCount&#39;: 4, &#39;nodeIdleTimeBeforeScaleDown&#39;: &#39;PT120S&#39;}, &#39;vmPriority&#39;: &#39;Dedicated&#39;, &#39;vmSize&#39;: &#39;STANDARD_NC6&#39;} . ML Workspace Data . TwelveData . List ETFs Available . etf_data = td.get_etf_list() etf_list = etf_data.as_json() etf_df = pd.DataFrame(etf_list) etf_df.head() . symbol name currency exchange . 0 8PSG | Invesco Physical Gold ETC | EUR | XETR | . 1 AAA | BetaShares Australian High Interest Cash ETF | AUD | ASX | . 2 AAAU | Perth Mint Physical Gold ETF | USD | NYSE | . 3 AADR | AdvisorShares Dorsey Wright ADR ETF | USD | NYSE | . 4 AASF | Airlie Australian Share Fund -- ETF Feeder | AUD | ASX | . Get ETF Time Series . end_date = pd.Timestamp(dt.datetime.today()) start_date = end_date - pd.tseries.offsets.BDay(252) start_date.to_pydatetime().date(), end_date.to_pydatetime().date() . (datetime.date(2020, 4, 15), datetime.date(2021, 4, 2)) . ticker = &quot;VOO&quot; ts = td.time_series( symbol=ticker, interval=&quot;1day&quot;, start_date=start_date, end_date=end_date, outputsize=300 ) df = ts.with_ema().as_pandas() df.describe() . open high low close volume ema . count 240.000000 | 240.000000 | 240.000000 | 240.000000 | 2.400000e+02 | 240.000000 | . mean 316.532337 | 318.708476 | 314.401410 | 316.757663 | 3.385244e+06 | 314.852005 | . std 30.684477 | 30.533911 | 30.831302 | 30.738751 | 1.376637e+06 | 30.938463 | . min 250.960010 | 255.490010 | 250.000000 | 250.960010 | 7.530980e+05 | 250.599230 | . 25% 294.715005 | 296.435857 | 293.557575 | 295.067500 | 2.356972e+06 | 291.208500 | . 50% 314.860000 | 317.341200 | 313.375000 | 315.255005 | 3.077382e+06 | 313.968755 | . 75% 342.612498 | 345.372500 | 340.907497 | 342.372490 | 4.080284e+06 | 340.965635 | . max 366.205990 | 368.290010 | 366.030000 | 368.160000 | 8.397805e+06 | 363.463540 | . df.head().reset_index() . datetime open high low close volume ema . 0 2021-04-01 | 366.20599 | 368.29001 | 366.03000 | 368.16000 | 4591212 | 363.46354 | . 1 2021-03-31 | 362.85999 | 365.82001 | 362.85999 | 364.29001 | 4870674 | 362.28942 | . 2 2021-03-30 | 363.79001 | 363.79001 | 361.28500 | 363.00000 | 3637520 | 361.78927 | . 3 2021-03-29 | 362.66000 | 364.67001 | 361.10971 | 363.79001 | 3062900 | 361.48659 | . 4 2021-03-26 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . Azure . Azure Workspace Datastore . data_store = ws.get_default_datastore() . Upload ETF Dataset . def get_or_upload_df(ws, data_store, df, ticker): dataset_name = f&#39;{ticker.lower()}_ds&#39; try: ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() except: Dataset.Tabular.register_pandas_dataframe(df, data_store, dataset_name) ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() return df aml_df = get_or_upload_df(ws, data_store, df.reset_index(), ticker) aml_df.head() . datetime open high low close volume ema . 0 2021-03-26 04:00:00 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . 1 2021-03-25 04:00:00 | 357.42001 | 360.23999 | 354.14001 | 359.47000 | 5361270 | 360.14842 | . 2 2021-03-24 04:00:00 | 360.70999 | 362.26999 | 357.44000 | 357.57999 | 3989728 | 360.31803 | . 3 2021-03-23 04:00:00 | 359.79501 | 362.51001 | 359.79501 | 362.32001 | 1208455 | 361.00254 | . 4 2021-03-22 04:00:00 | 359.88000 | 363.50000 | 359.76999 | 362.10999 | 3320390 | 360.67317 | . Training . Create Training Script . src_dir = &#39;aml-exp&#39; aml_exp = Path(src_dir) if not aml_exp.exists(): aml_exp.mkdir() . %%writefile aml-exp/train.py # Standard Libraries import argparse import json import os import datetime as dt # 3rd Party Libraries import numpy as np import pandas as pd import tensorflow as tf import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt from azureml.core import Run from azureml.core import Dataset from azureml.core import Model from azureml.tensorboard.export import export_to_tensorboard from azureml.interpret import ExplanationClient from sklearn.metrics import confusion_matrix # Classes class WindowGenerator(): def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None): # Store the raw data. self.train_df = train_df self.val_df = val_df self.test_df = test_df # Work out the label column indices. self.label_columns = label_columns if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(train_df.columns)} # Work out the window parameters. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(0, input_width) self.input_indices = np.arange(self.total_window_size)[self.input_slice] self.label_start = self.total_window_size - self.label_width self.labels_slice = slice(self.label_start, None) self.label_indices = np.arange(self.total_window_size)[self.labels_slice] def __repr__(self): return &#39; n&#39;.join([ f&#39;Total window size: {self.total_window_size}&#39;, f&#39;Input indices: {self.input_indices}&#39;, f&#39;Label indices: {self.label_indices}&#39;, f&#39;Label column name(s): {self.label_columns}&#39;]) @property def train(self): return self.make_dataset(self.train_df) @property def val(self): return self.make_dataset(self.val_df) @property def test(self): return self.make_dataset(self.test_df) @property def example(self): &quot;&quot;&quot;Get and cache an example batch of `inputs, labels` for plotting.&quot;&quot;&quot; result = getattr(self, &#39;_example&#39;, None) if result is None: # No example batch was found, so get one from the `.train` dataset result = next(iter(self.train)) # And cache it for next time self._example = result return result def split_window(self, features): inputs = features[:, self.input_slice, :] labels = features[:, self.labels_slice, :] if self.label_columns is not None: labels = tf.stack( [labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1) # Slicing doesn&#39;t preserve static shape information, so set the shapes # manually. This way the `tf.data.Datasets` are easier to inspect. inputs.set_shape([None, self.input_width, None]) labels.set_shape([None, self.label_width, None]) return inputs, labels def plot(self, plot_col, model=None, max_subplots=3): plt.figure(figsize=(12, 8)) plot_col_index = self.column_indices[plot_col] inputs, labels = self.example max_n = min(max_subplots, len(inputs)) for n in range(max_n): plt.subplot(max_n, 1, n+1) plt.ylabel(f&#39;{plot_col} [normed]&#39;) plt.plot(self.input_indices, inputs[n, :, plot_col_index], label=&#39;Inputs&#39;, marker=&#39;.&#39;, zorder=-10) if self.label_columns: label_col_index = self.label_columns_indices.get(plot_col, None) else: label_col_index = plot_col_index if label_col_index is None: continue plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors=&#39;k&#39;, label=&#39;Labels&#39;, c=&#39;#2ca02c&#39;, s=64) if model is not None: predictions = model(inputs) plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker=&#39;X&#39;, edgecolors=&#39;k&#39;, label=&#39;Predictions&#39;, c=&#39;#ff7f0e&#39;, s=64) if n == 0: plt.legend() plt.xlabel(&#39;Time&#39;) def make_dataset(self, data): data = np.array(data, dtype=np.float32) ds = tf.keras.preprocessing.timeseries_dataset_from_array( data=data, targets=None, sequence_length=self.total_window_size, sequence_stride=1, shuffle=True, batch_size=32,) ds = ds.map(self.split_window) return ds class Baseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return inputs result = inputs[:, :, self.label_index] return result[:, :, tf.newaxis] # Global Variables MAX_EPOCHS = 20 CONV_WIDTH = 3 # Read in Args parser = argparse.ArgumentParser(description=&#39;Train&#39;) parser.add_argument(&#39;--dataset_name&#39;, type=str, dest=&#39;dataset_name&#39;) args = parser.parse_args() # Paths os.makedirs(&#39;./outputs&#39;, exist_ok=True) os.makedirs(&#39;./outputs/model&#39;, exist_ok=True) os.makedirs(&#39;./outputs/log&#39;, exist_ok=True) # ML Run run = Run.get_context() workspace = run.experiment.workspace # ML Dataset ds = Dataset.get_by_name(workspace=workspace, name=args.dataset_name) df = ds.to_pandas_dataframe() # Date Feature Prep day = 24*60*60 year = (365.2425)*day date_time = pd.to_datetime(df.datetime) timestamp_s = date_time.map(dt.datetime.timestamp) df[&#39;day_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / day)) df[&#39;day_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / day)) df[&#39;year_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / year)) df[&#39;year_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / year)) # Data Filter features = [&#39;day_sin&#39;, &#39;day_cos&#39;, &#39;ema&#39;] target = &#39;close&#39; columns = features + [target] df = df[columns] # Data Splitting n = len(df) train_df = df[0:int(n*0.6)] val_df = df[int(n*0.6):int(n*0.8)] test_df = df[int(n*0.8):] # Data Normalization train_mean = train_df.mean() train_std = train_df.std() train_df = (train_df - train_mean) / train_std val_df = (val_df - train_mean) / train_std test_df = (test_df - train_mean) / train_std # Data Distribution Check df_std = (df - train_mean) / train_std df_std = df_std.melt(var_name=&#39;Column&#39;, value_name=&#39;Normalized&#39;) plt.figure(figsize=(12, 6)) ax = sns.violinplot(x=&#39;Column&#39;, y=&#39;Normalized&#39;, data=df_std) _ = ax.set_xticklabels(df.keys(), rotation=45) run.log_image(&#39;feature_distribution_check&#39;, plot=plt) # Data Windows single_step_window = WindowGenerator( input_width=1, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) wide_window = WindowGenerator( input_width=24, label_width=24, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) conv_window = WindowGenerator( input_width=CONV_WIDTH, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) wide_conv_window = WindowGenerator( input_width=24 + CONV_WIDTH - 1, label_width=24, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) # Train Baseline baseline = Baseline(label_index=single_step_window.column_indices.get(target)) baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) val_performance, tst_performance = {}, {} val_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.val) tst_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.test, verbose=0) wide_window.plot(target, baseline) run.log_image(&#39;baseline_pred&#39;, plot=plt) # Train Models def compile_and_fit(model, window, patience=4): early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=patience, mode=&#39;min&#39;) model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=[tf.metrics.MeanAbsoluteError()]) history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping]) return history # Train Linear Model linear = tf.keras.Sequential([ tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(linear, single_step_window) val_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.val) tst_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(linear, &#39;./outputs/model/linear&#39;) fig1 = plt.figure() ax = fig1.add_subplot(111) ax.bar(x = range(len(train_df.columns)), height=linear.layers[0].kernel[:,0].numpy()) ax.set_xticks(range(len(train_df.columns))) _ = ax.set_xticklabels(train_df.columns, rotation=45) run.log_image(&#39;linear_coef&#39;, plot=plt) wide_window.plot(target, linear) run.log_image(&#39;linear_pred&#39;, plot=plt) # Train Single Step Dense Model single_step_dense = tf.keras.Sequential([ tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(single_step_dense, single_step_window) val_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.val) tst_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(single_step_dense, &#39;./outputs/model/single_step_dense&#39;) wide_window.plot(target, single_step_dense) run.log_image(&#39;single_step_dense_pred&#39;, plot=plt) # Train Multi Step Dense Model multi_step_dense = tf.keras.Sequential([ # Shape: (time, features) =&gt; (time*features) tf.keras.layers.Flatten(), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), # Add back the time dimension. # Shape: (outputs) =&gt; (1, outputs) tf.keras.layers.Reshape([1, -1]), ]) history = compile_and_fit(multi_step_dense, conv_window) val_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.val) tst_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.test, verbose=0) tf.saved_model.save(multi_step_dense, &#39;./outputs/model/multi_step_dense&#39;) conv_window.plot(target, multi_step_dense) run.log_image(&#39;multi_step_dense_pred&#39;, plot=plt) # Train Conv Model conv = tf.keras.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=(CONV_WIDTH,), activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), ]) history = compile_and_fit(conv, conv_window) # TODO - log training epoch history to tesnorboard val_performance[&#39;conv&#39;] = conv.evaluate(conv_window.val) tst_performance[&#39;conv&#39;] = conv.evaluate(conv_window.test, verbose=0) tf.saved_model.save(conv, &#39;./outputs/model/conv&#39;) wide_conv_window.plot(target, conv) run.log_image(&#39;conv_pred&#39;, plot=plt) # Train LSTM Model lstm = tf.keras.models.Sequential([ # Shape [batch, time, features] =&gt; [batch, time, lstm_units] tf.keras.layers.LSTM(10, return_sequences=True), # Shape =&gt; [batch, time, features] tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(lstm, wide_window) val_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.val) tst_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.test, verbose=0) tf.saved_model.save(lstm, &#39;./outputs/model/lstm&#39;) wide_window.plot(target, lstm) run.log_image(&#39;lstm_pred&#39;, plot=plt) # Performance x = np.arange(len(val_performance)) width = 0.3 metric_name = &#39;mean_absolute_error&#39; metric_index = lstm.metrics_names.index(&#39;mean_absolute_error&#39;) val_mae = [v[metric_index] for v in val_performance.values()] test_mae = [v[metric_index] for v in tst_performance.values()] fig2 = plt.figure() ax = fig2.add_subplot(111) b1 = ax.bar(x - 0.2, val_mae, width, label=&#39;validation&#39;) b2 = ax.bar(x + 0.2, test_mae, width, label=&#39;test&#39;) ax.set_xticks(range(len(val_mae))) _ = ax.set_xticklabels(val_performance.keys(), rotation=45) run.log_image(&#39;performance_mae&#39;, plot=plt) # Log Results &amp; Select Best Model best_model, best_score = None, None if run is not None: for k, v in val_performance.items(): run.log_list(f&#39;val_{k}&#39;, v) for k, v in tst_performance.items(): run.log_list(f&#39;tst_{k}&#39;, v) try: mae = float(v[1]) if best_score is None and best_model is None: best_model = k best_score = mae elif best_score &gt; mae: best_model = k best_score = mae except: continue run.log(&#39;best_model&#39;, best_model) run.log(&#39;best_score&#39;, best_score) if best_model != &quot;baseline&quot;: model = run.register_model(model_name=best_model, model_path=f&#39;outputs/model/{best_model}&#39;) . Overwriting aml-exp/train.py . . Setup Training Environment . aml_run_config = RunConfiguration() aml_run_config.target = compute_target aml_run_config.environment.python.user_managed_dependencies = False # Add some packages relied on by data prep step deps = CondaDependencies.create( conda_packages=[&#39;pandas&#39;,&#39;scikit-learn&#39;, &#39;matplotlib&#39;, &#39;seaborn&#39;], pip_packages=[&#39;azureml-sdk&#39;, &#39;azureml-dataprep[fuse,pandas]&#39;, &#39;azureml-pipeline&#39;, &#39;azureml.tensorboard&#39;, &#39;azureml-interpret&#39;], python_version=&#39;3.6.2&#39;, pin_sdk_version=True) deps.add_tensorflow_pip_package(core_type=&#39;gpu&#39;, version=&#39;2.3.1&#39;) aml_run_config.environment.python.conda_dependencies = deps . Build Train Step . step1 = PythonScriptStep(name=&quot;train_step&quot;, source_directory=src_dir, script_name=&quot;train.py&quot;, arguments=[&#39;--dataset_name&#39;, f&#39;{ticker.lower()}_ds&#39;], runconfig=aml_run_config, allow_reuse=True) . Build Pipeline . steps = [step1] pipeline = Pipeline(workspace=ws, steps=steps) . Run Experiment . %%capture experiment = Experiment(ws, &#39;aml_exp&#39;) script_run = experiment.submit(pipeline) script_run.wait_for_completion(show_output=False) . RunDetails(script_run).show() . Review . Azure Portal . I find it best to simply go to the experiment portal url to review from the gui. It contains all the runs from your experiment and makes it easy to review changes from a central location. . script_run.get_portal_url() . &#39;https://ml.azure.com/runs/0e922ade-bbce-44c9-94c1-33ebdb17e44f?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . Metrics . However, you can choose to do the model review inside the notebook too. . The first place to look when doing this is the experiments metrics. In this example I&#39;m logging the mse and mae for validation &amp; test datasets for each model . step_name = &#39;train_step&#39; step = script_run.find_step_run(step_name)[0] . metrics_dict = step.get_metrics() print(f&quot;{metrics_dict.get(&#39;best_model&#39;)}: {metrics_dict.get(&#39;best_score&#39;):0.4f}&quot;) print() print(&#39; 033[1m&#39; + f&quot;Model Val Tst&quot; + &#39; 033[0m&#39;) metrics_list = list(filter(lambda v: isinstance(v[1], list), metrics_dict.items())) mae_metrics = [] for name, values in metrics_list: splits = name.split(&quot;_&quot;) grp, model = splits[0], &quot;_&quot;.join(splits[1:]) mae_metrics.append((model, grp, values[1])) mae_metrics = list(sorted(mae_metrics, key=lambda o: o[0])) for cur, nxt in zip(mae_metrics[0::2], mae_metrics[1::2]): name_1, _, value_1 = cur name_2, _, value_2 = nxt assert name_1 == name_2 print(f&#39;{name_1:25s}: {value_1:0.4f} | {value_2:0.4f}&#39;) . single_step_dense: 0.1610 Model Val Tst baseline : 0.1176 | 0.1872 conv : 0.0875 | 0.1773 linear : 3.0643 | 4.7024 lstm : 0.9358 | 2.6143 multi_step_dense : 0.1977 | 0.6740 single_step_dense : 0.1043 | 0.1610 . All Stored Files . It can also be useful to review the log files to figure out wtf is going wrong constantly... . files = step.get_file_names() files . [&#39;azureml-logs/20_image_build_log.txt&#39;, &#39;azureml-logs/55_azureml-execution-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt&#39;, &#39;azureml-logs/65_job_prep-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt&#39;, &#39;azureml-logs/70_driver_log.txt&#39;, &#39;azureml-logs/75_job_post-tvmps_1d261b42abd5181b8b59daf381ea85917514878d6568f63fa19a6608f53ca32c_d.txt&#39;, &#39;azureml-logs/process_info.json&#39;, &#39;azureml-logs/process_status.json&#39;, &#39;baseline_pred_1617395547.png&#39;, &#39;conv_pred_1617395562.png&#39;, &#39;feature_distribution_check_1617395545.png&#39;, &#39;linear_coef_1617395551.png&#39;, &#39;linear_pred_1617395551.png&#39;, &#39;logs/azureml/106_azureml.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess_Telemetry.log&#39;, &#39;logs/azureml/executionlogs.txt&#39;, &#39;logs/azureml/job_prep_azureml.log&#39;, &#39;logs/azureml/job_release_azureml.log&#39;, &#39;logs/azureml/stderrlogs.txt&#39;, &#39;logs/azureml/stdoutlogs.txt&#39;, &#39;lstm_pred_1617395572.png&#39;, &#39;multi_step_dense_pred_1617395558.png&#39;, &#39;outputs/model/conv/saved_model.pb&#39;, &#39;outputs/model/conv/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/conv/variables/variables.index&#39;, &#39;outputs/model/linear/saved_model.pb&#39;, &#39;outputs/model/linear/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/linear/variables/variables.index&#39;, &#39;outputs/model/lstm/saved_model.pb&#39;, &#39;outputs/model/lstm/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/lstm/variables/variables.index&#39;, &#39;outputs/model/multi_step_dense/saved_model.pb&#39;, &#39;outputs/model/multi_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/multi_step_dense/variables/variables.index&#39;, &#39;outputs/model/single_step_dense/saved_model.pb&#39;, &#39;outputs/model/single_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/single_step_dense/variables/variables.index&#39;, &#39;performance_mae_1617395572.png&#39;, &#39;single_step_dense_pred_1617395555.png&#39;] . Clean up . Delete Compute Cluster . This is an important step if you don&#39;t want save some money 😉 . print(&quot;starting compute cleanup&quot;) for name, compute in ws.compute_targets.items(): print(f&quot;deleting {name} instance&quot;) compute.delete() while len(ws.compute_targets.items()) != 0: continue print(&quot;compute cleanup complete&quot;) . starting compute cleanup deleting aml-compute instance compute cleanup complete .",
            "url": "https://kslader8.github.io/kslader8-thoughts/azureml/tensorflow/time-series/2021/04/04/aml-exp-continuation-1.html",
            "relUrl": "/azureml/tensorflow/time-series/2021/04/04/aml-exp-continuation-1.html",
            "date": " • Apr 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Azure ML Workspace - [Tensorflow] Time Series Example",
            "content": "Okay, so I kind of live in Azure ML Workspace these days... leading me to want to make a small notebook utilizing it here. It&#39;s been changing pretty rapidly every ~6 months, so I&#39;m going to include the versions I work on. . If your company is going down the the Azure road for public cloud, Azure ML Workspace (or AWS SageMaker) is probably the best solution to scale easy access to compute, datasets, experiments, etc. to different data science teams accross a large organization. . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline . Libraries . import os import datetime as dt from pathlib import Path . from dotenv import load_dotenv . import pandas as pd import numpy as np # import tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt print(f&quot;pandas version {pd.__version__}&quot;) print(f&quot;numpy version {np.__version__}&quot;) # print(f&quot;tensorflow version {tf.__version__}&quot;) . pandas version 1.2.0 numpy version 1.18.5 . import azureml.core as aml from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment, Run from azureml.core import Datastore, Dataset from azureml.core.compute import ComputeTarget, AmlCompute from azureml.core.runconfig import RunConfiguration from azureml.core.conda_dependencies import CondaDependencies from azureml.core import Model from azureml.core.resource_configuration import ResourceConfiguration from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.widgets import RunDetails print(f&quot;azureml version {aml.__version__}&quot;) . azureml version 1.25.0 . import twelvedata from twelvedata import TDClient print(f&quot;twelvedata version {twelvedata.__version__}&quot;) . twelvedata version 1.1.7 . Project Environment Variables . This is a personal preference of mine to make a .env file per project to encapsulate tokens/secrets/etc outside of notebooks. . In this case I created a file named .env with a single variable apikey=(api key) in the same directory as my experiment. . env_path = Path(&quot;.env&quot;) assert env_path.exists() _ = load_dotenv(env_path) . Matplotlib . It&#39;s useful to set a few global plotting defaults to save from doing them for every plot in a notebook . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 8) mpl.rcParams[&#39;axes.grid&#39;] = False . Azure ML Workspace . To setup an Azure ML Workspace you will need an azure account (with credit card). To spin it up simply go to https://portal.azure.com/ and type machine learning in the search bar and create a workspace. . Once you have a workspace you will need to download the config.json prior to going to https://ml.azure.com/ to access your workspace . workspace_config_path = Path(&quot;config.json&quot;) assert workspace_config_path.exists() ws = Workspace.from_config(path=workspace_config_path) . Twelve Data Client . I setup an account at https://twelvedata.com/ to get a free api key to try it out. I had not heard of it before, but it was the first thing that came up in my google search for free market data... . apikey = os.environ.get(&quot;apikey&quot;) td = TDClient(apikey=apikey) . ML Workspace Compute . Get existing compute cluster or create one . compute_name = &quot;aml-compute&quot; vm_size = &quot;Standard_NC6&quot; # vm_size = &quot;Standard_NC6s_v3&quot; if compute_name in ws.compute_targets: compute_target = ws.compute_targets[compute_name] if compute_target and type(compute_target) is AmlCompute: print(&#39;Found compute target: &#39; + compute_name) else: print(&#39;Creating a new compute target...&#39;) provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size, # STANDARD_NC6 is GPU-enabled min_nodes=0, max_nodes=4) # create the compute target compute_target = ComputeTarget.create( ws, compute_name, provisioning_config) # Can poll for a minimum number of nodes and for a specific timeout. # If no min node count is provided it will use the scale settings for the cluster compute_target.wait_for_completion( show_output=True, min_node_count=None, timeout_in_minutes=20) # For a more detailed view of current cluster status, use the &#39;status&#39; property print(compute_target.status.serialize()) . Creating a new compute target... Creating... SucceededProvisioning operation finished, operation &#34;Succeeded&#34; Succeeded AmlCompute wait for completion finished Minimum number of nodes requested have been provisioned {&#39;currentNodeCount&#39;: 0, &#39;targetNodeCount&#39;: 0, &#39;nodeStateCounts&#39;: {&#39;preparingNodeCount&#39;: 0, &#39;runningNodeCount&#39;: 0, &#39;idleNodeCount&#39;: 0, &#39;unusableNodeCount&#39;: 0, &#39;leavingNodeCount&#39;: 0, &#39;preemptedNodeCount&#39;: 0}, &#39;allocationState&#39;: &#39;Steady&#39;, &#39;allocationStateTransitionTime&#39;: &#39;2021-03-31T18:41:03.129000+00:00&#39;, &#39;errors&#39;: None, &#39;creationTime&#39;: &#39;2021-03-31T18:41:00.552530+00:00&#39;, &#39;modifiedTime&#39;: &#39;2021-03-31T18:41:16.029047+00:00&#39;, &#39;provisioningState&#39;: &#39;Succeeded&#39;, &#39;provisioningStateTransitionTime&#39;: None, &#39;scaleSettings&#39;: {&#39;minNodeCount&#39;: 0, &#39;maxNodeCount&#39;: 4, &#39;nodeIdleTimeBeforeScaleDown&#39;: &#39;PT120S&#39;}, &#39;vmPriority&#39;: &#39;Dedicated&#39;, &#39;vmSize&#39;: &#39;STANDARD_NC6&#39;} . ML Workspace Data . TwelveData . List ETFs Available . etf_data = td.get_etf_list() etf_list = etf_data.as_json() etf_df = pd.DataFrame(etf_list) etf_df.head() . symbol name currency exchange . 0 8PSG | Invesco Physical Gold ETC | EUR | XETR | . 1 AAA | BetaShares Australian High Interest Cash ETF | AUD | ASX | . 2 AAAU | Perth Mint Physical Gold ETF | USD | NYSE | . 3 AADR | AdvisorShares Dorsey Wright ADR ETF | USD | NYSE | . 4 AASF | Airlie Australian Share Fund -- ETF Feeder | AUD | ASX | . Get ETF Time Series . end_date = pd.Timestamp(dt.datetime.today()) start_date = end_date - pd.tseries.offsets.BDay(252) start_date.to_pydatetime().date(), end_date.to_pydatetime().date() . (datetime.date(2020, 4, 13), datetime.date(2021, 3, 31)) . ticker = &quot;VOO&quot; ts = td.time_series( symbol=ticker, interval=&quot;1day&quot;, start_date=start_date, end_date=end_date, outputsize=300 ) df = ts.with_ema().as_pandas() df.describe() . open high low close volume ema . count 241.000000 | 241.000000 | 241.000000 | 241.000000 | 2.410000e+02 | 241.000000 | . mean 315.827105 | 318.008939 | 313.696467 | 316.061946 | 3.388346e+06 | 314.099308 | . std 30.935291 | 30.777465 | 31.051232 | 30.962168 | 1.384690e+06 | 31.300765 | . min 250.960010 | 255.490010 | 250.000000 | 250.960010 | 7.530980e+05 | 247.567540 | . 25% 294.420010 | 296.386990 | 293.149990 | 294.780000 | 2.359274e+06 | 289.318520 | . 50% 314.355010 | 316.260010 | 312.989990 | 314.810000 | 3.065109e+06 | 313.609280 | . 75% 342.239990 | 344.370000 | 340.179990 | 341.989990 | 4.069956e+06 | 340.674540 | . max 365.079990 | 366.049990 | 363.250000 | 365.410000 | 8.397805e+06 | 362.513420 | . df.head().reset_index() . datetime open high low close volume ema . 0 2021-03-31 | 362.85999 | 365.82001 | 362.85999 | 365.41000 | 2687756 | 362.51342 | . 1 2021-03-30 | 363.79001 | 363.79001 | 361.28500 | 363.00000 | 3637520 | 361.78927 | . 2 2021-03-29 | 362.66000 | 364.67001 | 361.10971 | 363.79001 | 3062900 | 361.48659 | . 3 2021-03-26 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . 4 2021-03-25 | 357.42001 | 360.23999 | 354.14001 | 359.47000 | 5361270 | 360.14842 | . Azure . Azure Workspace Datastore . data_store = ws.get_default_datastore() . Upload ETF Dataset . def get_or_upload_df(ws, data_store, df, ticker): dataset_name = f&#39;{ticker.lower()}_ds&#39; try: ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() except: Dataset.Tabular.register_pandas_dataframe(df, data_store, dataset_name) ds = Dataset.get_by_name(workspace=ws, name=dataset_name) df = ds.to_pandas_dataframe() return df aml_df = get_or_upload_df(ws, data_store, df.reset_index(), ticker) aml_df.head() . datetime open high low close volume ema . 0 2021-03-26 04:00:00 | 359.42999 | 364.35001 | 358.75000 | 363.95999 | 3212525 | 360.91074 | . 1 2021-03-25 04:00:00 | 357.42001 | 360.23999 | 354.14001 | 359.47000 | 5361270 | 360.14842 | . 2 2021-03-24 04:00:00 | 360.70999 | 362.26999 | 357.44000 | 357.57999 | 3989728 | 360.31803 | . 3 2021-03-23 04:00:00 | 359.79501 | 362.51001 | 359.79501 | 362.32001 | 1208455 | 361.00254 | . 4 2021-03-22 04:00:00 | 359.88000 | 363.50000 | 359.76999 | 362.10999 | 3320390 | 360.67317 | . Training . Create Training Script . src_dir = &#39;aml-exp&#39; aml_exp = Path(src_dir) if not aml_exp.exists(): aml_path.mkdir() . %%writefile aml-exp/train.py # Standard Libraries import argparse import json import os import datetime as dt # 3rd Party Libraries import numpy as np import pandas as pd import tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt from azureml.core import Run from azureml.core import Dataset from azureml.core import Model from azureml.pipeline.core import Pipeline, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.interpret import ExplanationClient from sklearn.metrics import confusion_matrix # Classes class WindowGenerator(): def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None): # Store the raw data. self.train_df = train_df self.val_df = val_df self.test_df = test_df # Work out the label column indices. self.label_columns = label_columns if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(train_df.columns)} # Work out the window parameters. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(0, input_width) self.input_indices = np.arange(self.total_window_size)[self.input_slice] self.label_start = self.total_window_size - self.label_width self.labels_slice = slice(self.label_start, None) self.label_indices = np.arange(self.total_window_size)[self.labels_slice] def __repr__(self): return &#39; n&#39;.join([ f&#39;Total window size: {self.total_window_size}&#39;, f&#39;Input indices: {self.input_indices}&#39;, f&#39;Label indices: {self.label_indices}&#39;, f&#39;Label column name(s): {self.label_columns}&#39;]) @property def train(self): return self.make_dataset(self.train_df) @property def val(self): return self.make_dataset(self.val_df) @property def test(self): return self.make_dataset(self.test_df) @property def example(self): &quot;&quot;&quot;Get and cache an example batch of `inputs, labels` for plotting.&quot;&quot;&quot; result = getattr(self, &#39;_example&#39;, None) if result is None: # No example batch was found, so get one from the `.train` dataset result = next(iter(self.train)) # And cache it for next time self._example = result return result def split_window(self, features): inputs = features[:, self.input_slice, :] labels = features[:, self.labels_slice, :] if self.label_columns is not None: labels = tf.stack( [labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1) # Slicing doesn&#39;t preserve static shape information, so set the shapes # manually. This way the `tf.data.Datasets` are easier to inspect. inputs.set_shape([None, self.input_width, None]) labels.set_shape([None, self.label_width, None]) return inputs, labels def plot(self, plot_col, model=None, max_subplots=3): plt.figure(figsize=(12, 8)) plot_col_index = self.column_indices[plot_col] inputs, labels = self.example max_n = min(max_subplots, len(inputs)) for n in range(max_n): plt.subplot(max_n, 1, n+1) plt.ylabel(f&#39;{plot_col} [normed]&#39;) plt.plot(self.input_indices, inputs[n, :, plot_col_index], label=&#39;Inputs&#39;, marker=&#39;.&#39;, zorder=-10) if self.label_columns: label_col_index = self.label_columns_indices.get(plot_col, None) else: label_col_index = plot_col_index if label_col_index is None: continue plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors=&#39;k&#39;, label=&#39;Labels&#39;, c=&#39;#2ca02c&#39;, s=64) if model is not None: predictions = model(inputs) plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker=&#39;X&#39;, edgecolors=&#39;k&#39;, label=&#39;Predictions&#39;, c=&#39;#ff7f0e&#39;, s=64) if n == 0: plt.legend() plt.xlabel(&#39;Time&#39;) def make_dataset(self, data): data = np.array(data, dtype=np.float32) ds = tf.keras.preprocessing.timeseries_dataset_from_array( data=data, targets=None, sequence_length=self.total_window_size, sequence_stride=1, shuffle=True, batch_size=32,) ds = ds.map(self.split_window) return ds class Baseline(tf.keras.Model): def __init__(self, label_index=None): super().__init__() self.label_index = label_index def call(self, inputs): if self.label_index is None: return inputs result = inputs[:, :, self.label_index] return result[:, :, tf.newaxis] # Global Variables MAX_EPOCHS = 20 CONV_WIDTH = 3 # Read in Args parser = argparse.ArgumentParser(description=&#39;Train&#39;) parser.add_argument(&#39;--dataset_name&#39;, type=str, dest=&#39;dataset_name&#39;) args = parser.parse_args() # Paths os.makedirs(&#39;./outputs&#39;, exist_ok=True) os.makedirs(&#39;./outputs/model&#39;, exist_ok=True) # ML Run run = Run.get_context() workspace = run.experiment.workspace # ML Dataset ds = Dataset.get_by_name(workspace=workspace, name=args.dataset_name) df = ds.to_pandas_dataframe() # Date Feature Prep day = 24*60*60 year = (365.2425)*day date_time = pd.to_datetime(df.datetime) timestamp_s = date_time.map(dt.datetime.timestamp) df[&#39;day_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / day)) df[&#39;day_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / day)) df[&#39;year_sin&#39;] = np.sin(timestamp_s * (2 * np.pi / year)) df[&#39;year_cos&#39;] = np.cos(timestamp_s * (2 * np.pi / year)) # Data Filter features = [&#39;day_sin&#39;, &#39;day_cos&#39;, &#39;ema&#39;] target = &#39;close&#39; columns = features + [target] df = df[columns] # Data Splitting n = len(df) train_df = df[0:int(n*0.7)] val_df = df[int(n*0.7):int(n*0.9)] test_df = df[int(n*0.9):] # Data Normalization # TODO - normalize step based on train_df # Data Windows single_step_window = WindowGenerator( input_width=1, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) wide_window = WindowGenerator( input_width=24, label_width=24, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) conv_window = WindowGenerator( input_width=CONV_WIDTH, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=[target]) # Train Baseline baseline = Baseline(label_index=single_step_window.column_indices.get(target)) baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanAbsoluteError()]) val_performance, tst_performance = {}, {} val_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.val) tst_performance[&#39;baseline&#39;] = baseline.evaluate(single_step_window.test, verbose=0) wide_window.plot(target, baseline) run.log_image(&#39;baseline_pred&#39;, plot=plt) # Train Models def compile_and_fit(model, window, patience=4): early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=patience, mode=&#39;min&#39;) model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=[tf.metrics.MeanAbsoluteError()]) history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping]) return history # Train Linear Model linear = tf.keras.Sequential([ tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(linear, single_step_window) val_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.val) tst_performance[&#39;linear&#39;] = linear.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(linear, &#39;./outputs/model/linear&#39;) fig1 = plt.figure() ax = fig1.add_subplot(111) ax.bar(x = range(len(train_df.columns)), height=linear.layers[0].kernel[:,0].numpy()) ax.set_xticks(range(len(train_df.columns))) _ = ax.set_xticklabels(train_df.columns, rotation=90) run.log_image(&#39;linear_coef&#39;, plot=plt) wide_window.plot(target, linear) run.log_image(&#39;linear_pred&#39;, plot=plt) # Train Single Step Dense Model single_step_dense = tf.keras.Sequential([ tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=64, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(single_step_dense, single_step_window) val_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.val) tst_performance[&#39;single_step_dense&#39;] = single_step_dense.evaluate(single_step_window.test, verbose=0) tf.saved_model.save(single_step_dense, &#39;./outputs/model/single_step_dense&#39;) wide_window.plot(target, single_step_dense) run.log_image(&#39;single_step_dense_pred&#39;, plot=plt) # Train Multi Step Dense Model multi_step_dense = tf.keras.Sequential([ # Shape: (time, features) =&gt; (time*features) tf.keras.layers.Flatten(), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), # Add back the time dimension. # Shape: (outputs) =&gt; (1, outputs) tf.keras.layers.Reshape([1, -1]), ]) history = compile_and_fit(multi_step_dense, conv_window) val_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.val) tst_performance[&#39;multi_step_dense&#39;] = multi_step_dense.evaluate(conv_window.test, verbose=0) tf.saved_model.save(multi_step_dense, &#39;./outputs/model/multi_step_dense&#39;) # wide_window.plot(target, multi_step_dense) # run.log_image(&#39;multi_step_dense_pred&#39;, plot=plt) # Train Conv Model conv = tf.keras.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=(CONV_WIDTH,), activation=&#39;relu&#39;), tf.keras.layers.Dense(units=32, activation=&#39;relu&#39;), tf.keras.layers.Dense(units=1), ]) history = compile_and_fit(conv, conv_window) val_performance[&#39;conv&#39;] = conv.evaluate(conv_window.val) tst_performance[&#39;conv&#39;] = conv.evaluate(conv_window.test, verbose=0) tf.saved_model.save(conv, &#39;./outputs/model/conv&#39;) # Train LSTM Model lstm = tf.keras.models.Sequential([ # Shape [batch, time, features] =&gt; [batch, time, lstm_units] tf.keras.layers.LSTM(10, return_sequences=True), # Shape =&gt; [batch, time, features] tf.keras.layers.Dense(units=1) ]) history = compile_and_fit(lstm, wide_window) val_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.val) tst_performance[&#39;lstm&#39;] = lstm.evaluate(wide_window.test, verbose=0) tf.saved_model.save(lstm, &#39;./outputs/model/lstm&#39;) # Performance x = np.arange(len(val_performance)) width = 0.3 metric_name = &#39;mean_absolute_error&#39; metric_index = lstm.metrics_names.index(&#39;mean_absolute_error&#39;) val_mae = [v[metric_index] for v in val_performance.values()] test_mae = [v[metric_index] for v in tst_performance.values()] fig2 = plt.figure() ax = fig2.add_subplot(111) b1 = ax.bar(x - 0.2, val_mae, width, label=&#39;validation&#39;) # b2 = ax.bar(x + 0.2, test_mae, width, label=&#39;test&#39;) ax.set_xticks(range(len(val_mae))) _ = ax.set_xticklabels(val_performance.keys(), rotation=90) run.log_image(&#39;performance_mae&#39;, plot=plt) # Log Results &amp; Select Best Model best_model, best_score = None, None if run is not None: for k, v in val_performance.items(): run.log_list(f&#39;val_{k}&#39;, v) for k, v in tst_performance.items(): run.log_list(f&#39;tst_{k}&#39;, v) try: mae = float(v[1]) if best_score is None and best_model is None: best_model = k best_score = mae elif best_score &gt; mae: best_model = k best_score = mae except: continue run.log(&#39;best_model&#39;, best_model) run.log(&#39;best_score&#39;, best_score) if best_model != &quot;baseline&quot;: model = run.register_model(model_name=best_model, model_path=f&#39;outputs/model/{best_model}&#39;) #### Setup Training Environment . Overwriting aml-exp/train.py . aml_run_config = RunConfiguration() aml_run_config.target = compute_target aml_run_config.environment.python.user_managed_dependencies = False # Add some packages relied on by data prep step deps = CondaDependencies.create( conda_packages=[&#39;pandas&#39;,&#39;scikit-learn&#39;, &#39;matplotlib&#39;], pip_packages=[&#39;azureml-sdk&#39;, &#39;azureml-dataprep[fuse,pandas]&#39;, &#39;azureml-pipeline&#39;, &#39;azureml-interpret&#39;], python_version=&#39;3.6.2&#39;, pin_sdk_version=True) deps.add_tensorflow_pip_package(core_type=&#39;gpu&#39;, version=&#39;2.3.1&#39;) aml_run_config.environment.python.conda_dependencies = deps src = ScriptRunConfig(source_directory=src_dir, script=&#39;train.py&#39;, arguments=[&#39;--dataset_name&#39;, f&#39;{ticker.lower()}_ds&#39;], run_config=aml_run_config) . Run Experiment . %%capture experiment = Experiment(ws, &#39;aml_exp&#39;) script_run = experiment.submit(src) script_run.wait_for_completion(show_output=False) . RunDetails(script_run).show() . Review . I find it best to simply go to the experiment portal url to review from the gui. It contains all the runs from your experiment and makes it easy to review changes from a central location. . script_run.get_portal_url() . &#39;https://ml.azure.com/runs/aml_exp_1617233359_a7595e84?wsid=/subscriptions/f3b5840b-706e-44ba-8aa1-6fd3fc8aaab0/resourcegroups/ds-workspace/workspaces/minion-lab&amp;tid=e6777dcd-6f87-4dd0-92e5-e98312157dac&#39; . However, you can choose to do the model review inside the notebook too. . The first place to look when doing this is the experiments metrics. In this example I&#39;m logging the mse and mae for validation &amp; test datasets for each model . metrics = script_run.get_metrics() metrics . {&#39;baseline_pred&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/baseline_pred_1617233672.png&#39;, &#39;linear_coef&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/linear_coef_1617233676.png&#39;, &#39;linear_pred&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/linear_pred_1617233676.png&#39;, &#39;single_step_dense_pred&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/single_step_dense_pred_1617233681.png&#39;, &#39;performance_mae&#39;: &#39;aml://artifactId/ExperimentRun/dcid.aml_exp_1617233359_a7595e84/performance_mae_1617233696.png&#39;, &#39;val_baseline&#39;: [18.686260223388672, 3.2065956592559814], &#39;val_linear&#39;: [605690.9375, 777.9266357421875], &#39;val_single_step_dense&#39;: [12.129790306091309, 2.596614122390747], &#39;val_multi_step_dense&#39;: [23.9085693359375, 3.903738021850586], &#39;val_conv&#39;: [17.16225242614746, 2.9845776557922363], &#39;val_lstm&#39;: [81859.9296875, 286.0354919433594], &#39;tst_baseline&#39;: [20.958070755004883, 3.920870542526245], &#39;tst_linear&#39;: [502930.53125, 709.0858154296875], &#39;tst_single_step_dense&#39;: [14.61068344116211, 3.2360215187072754], &#39;tst_multi_step_dense&#39;: [20.170698165893555, 4.205769062042236], &#39;tst_conv&#39;: [17.6562557220459, 3.5512776374816895], &#39;best_model&#39;: &#39;single_step_dense&#39;, &#39;best_score&#39;: 3.2360215187072754} . mae_metrics = [] for name, value in metrics.items(): if isinstance(value, list) and len(value) &gt;= 2: splits = name.split(&quot;_&quot;) grp, model = splits[0], &quot;_&quot;.join(splits[1:]) mae_metrics.append((model, grp, value[1])) for model, grp, mae in sorted(mae_metrics, key=lambda o: o[0]): name = f&#39;{model}_{grp}&#39; print(f&#39;{name:25s}: {mae:0.4f}&#39;) . baseline_val : 3.2066 baseline_tst : 3.9209 conv_val : 2.9846 conv_tst : 3.5513 linear_val : 777.9266 linear_tst : 709.0858 lstm_val : 286.0355 multi_step_dense_val : 3.9037 multi_step_dense_tst : 4.2058 single_step_dense_val : 2.5966 single_step_dense_tst : 3.2360 . It can also be useful to review the log files to figure out wtf is going wrong constantly... . files = script_run.get_file_names() files . [&#39;azureml-logs/55_azureml-execution-tvmps_0c357d5ce4f8eda465b3f47f7b95b579e656516823eef14997ff742d99f7565e_d.txt&#39;, &#39;azureml-logs/65_job_prep-tvmps_0c357d5ce4f8eda465b3f47f7b95b579e656516823eef14997ff742d99f7565e_d.txt&#39;, &#39;azureml-logs/70_driver_log.txt&#39;, &#39;azureml-logs/75_job_post-tvmps_0c357d5ce4f8eda465b3f47f7b95b579e656516823eef14997ff742d99f7565e_d.txt&#39;, &#39;azureml-logs/process_info.json&#39;, &#39;azureml-logs/process_status.json&#39;, &#39;baseline_pred_1617233672.png&#39;, &#39;linear_coef_1617233676.png&#39;, &#39;linear_pred_1617233676.png&#39;, &#39;logs/azureml/106_azureml.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess.log&#39;, &#39;logs/azureml/dataprep/backgroundProcess_Telemetry.log&#39;, &#39;logs/azureml/job_prep_azureml.log&#39;, &#39;logs/azureml/job_release_azureml.log&#39;, &#39;outputs/model/conv/saved_model.pb&#39;, &#39;outputs/model/conv/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/conv/variables/variables.index&#39;, &#39;outputs/model/linear/saved_model.pb&#39;, &#39;outputs/model/linear/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/linear/variables/variables.index&#39;, &#39;outputs/model/lstm/saved_model.pb&#39;, &#39;outputs/model/lstm/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/lstm/variables/variables.index&#39;, &#39;outputs/model/multi_step_dense/saved_model.pb&#39;, &#39;outputs/model/multi_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/multi_step_dense/variables/variables.index&#39;, &#39;outputs/model/single_step_dense/saved_model.pb&#39;, &#39;outputs/model/single_step_dense/variables/variables.data-00000-of-00001&#39;, &#39;outputs/model/single_step_dense/variables/variables.index&#39;, &#39;performance_mae_1617233696.png&#39;, &#39;single_step_dense_pred_1617233681.png&#39;] . Clean up . This is an important step if you don&#39;t want to end up having a big bill at the end of the month 😉 . print(&quot;starting compute cleanup&quot;) for name, compute in ws.compute_targets.items(): print(f&quot;deleting {name} instance&quot;) compute.delete() while len(ws.compute_targets.items()) != 0: continue print(&quot;compute cleanup complete&quot;) . starting compute cleanup deleting aml-compute instance deleting ks-nb instance Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; Current provisioning state of AmlCompute is &#34;Deleting&#34; compute cleanup complete .",
            "url": "https://kslader8.github.io/kslader8-thoughts/azureml/tensorflow/time-series/2021/03/29/aml-exp.html",
            "relUrl": "/azureml/tensorflow/time-series/2021/03/29/aml-exp.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "HackerRank Questions - Graph Traversal",
            "content": "I recently did a few hacker rank questions. No promises that these are ideal solutions, but this is my work below. . Problems . import big_o import random from typing import List, Tuple, Dict, Optional . Q1 . Given the number of rows and columns for a grid, find the number of different paths when you can only move down or right assuming you start at the top left and finish at the bottom right. . Example: (7, 3) -&gt; 28; (3,3) -&gt; 6 . def calc_matrix_paths(rows:int, columns:Optional[int]=None) -&gt; int: if columns is None: columns = rows first_row, first_column = 0, 0 count_matrix = [[0 for column in range(columns)] for row in range(rows)] # r for row in range(rows): count_matrix[row][first_column] = 1 # c for column in range(columns): count_matrix[first_row][column] = 1 # r*c for row in range(1, rows): for column in range(1, columns): count_matrix[row][column] = count_matrix[row-1][column] + count_matrix[row][column-1] return count_matrix[rows-1][columns-1] cases = [ (3, 3), # square matrix (7, 3), # non square matrix ] for rows, columns in cases: print(f&quot;rows: {rows} | columns: {columns}&quot;) result = calc_matrix_paths(rows, columns) print(f&quot;number of paths: {result}&quot;) . rows: 3 | columns: 3 number of paths: 6 rows: 7 | columns: 3 number of paths: 28 . big_o.complexities.ALL_CLASSES . [big_o.complexities.Constant, big_o.complexities.Linear, big_o.complexities.Quadratic, big_o.complexities.Cubic, big_o.complexities.Polynomial, big_o.complexities.Logarithmic, big_o.complexities.Linearithmic, big_o.complexities.Exponential] . gen = lambda n: n best, others = big_o.big_o(calc_matrix_paths, gen, min_n=2, max_n=5_000, n_measures=10, n_repeats=2) . print(f&quot;predicted: {best}&quot;) print() for class_, residual in sorted(others.items(), key=lambda t: t[1], reverse=False): print(f&quot;{class_} | {residual}&quot;) . predicted: Exponential: time = -3.2 * 0.002^n (sec) Exponential: time = -3.2 * 0.002^n (sec) | 9.055977664509317 Polynomial: time = -7.5 * x^1.3 (sec) | 22.531114726371193 Cubic: time = -49 + 4.3E-09*n^3 (sec) | 56631.68144191306 Quadratic: time = -72 + 2E-05*n^2 (sec) | 101322.81044136792 Linearithmic: time = -1.1E+02 + 0.01*n*log(n) (sec) | 165159.6456644176 Linear: time = -1.1E+02 + 0.086*n (sec) | 176698.92796259583 Logarithmic: time = -1E+02 + 29*log(n) (sec) | 324034.55046488193 Constant: time = 1E+02 (sec) | 364452.09251446975 . I wonder if the looping is that bad in python... Unless I&#39;m missing something it looks like O(r*c) so should be polynomial time . Q2 . Given an r x c grid, print all cells in spiral order . Example: [[1,2,3],[4,5,6],[7,8,9]] -&gt; 1,2,3,6,9,8,7,4,5 . def get_spiral_order(inp_matrix:List[List[int]]) -&gt; List[int]: start_row = 0 start_col = 0 end_row = len(inp_matrix) end_col = len(inp_matrix[0]) result = [] while end_row &gt; start_row and end_col &gt; start_col: for item in inp_matrix[start_row][start_col: end_col]: result.append(item) for row in range(start_row+1, end_row): result.append(inp_matrix[row][end_col-1]) for col in reversed(list(range(start_col, end_col-1))): result.append(inp_matrix[end_row-1][col]) for i in reversed(list(range(start_row+1, end_row-1))): result.append(inp_matrix[i][start_col]) start_col += 1 start_row += 1 end_col -= 1 end_row -= 1 return result cases = [ [[1,2,3],[4,5,6],[7,8,9]], [[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]], ] for case in cases: print(f&quot;input matrix: {case}&quot;) spiral_order = get_spiral_order(case) print(f&quot;spiral order: {spiral_order}&quot;) . input matrix: [[1, 2, 3], [4, 5, 6], [7, 8, 9]] spiral order: [1, 2, 3, 6, 9, 8, 7, 4, 5] input matrix: [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]] spiral order: [1, 2, 3, 4, 5, 10, 15, 20, 19, 18, 17, 16, 11, 6, 7, 8, 9, 14, 13, 12] . Q3 . Scheduling Problem . You are tasked with creating an algorithm to determine the execution order of a set of tasks. This algorithm will be provided with a list of task labels and a list of task dependencies. Your algorithm should determine a valid execution order of the provided tasks. If there is no valid execution order your algorithm should return None. . Example 1: . labels = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] . where each element in &quot;labels&quot; represents a task label . dependencies = [(&#39;b&#39;, &#39;a&#39;), (&#39;c&#39;, &#39;a&#39;), (&#39;d&#39;, &#39;c&#39;)] . where each element tuple represents a task dependency in which the first element depends on the second . i.e. &#39;b&#39; depends on &#39;a&#39;, &#39;c&#39; depends on &#39;a&#39;, and &#39;d&#39; depends on &#39;c&#39; . In this case a valid execution order would be [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] . another valid execution order could be [&#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;d&#39;] . Example 2: . labels = [&#39;a&#39;, &#39;b&#39;] . dependencies = [(&#39;b&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;)] . In this case there is no valid execution order so your algorithm should return None . from collections import defaultdict, deque def make_dependency_graph(labels: List[str], dependencies: List[Tuple[str]]) -&gt; Dict[str, List[str]]: graph = {} for l in labels: graph[l] = [] for k, v in dependencies: deps = graph[k] deps.append(v) return graph def get_execution_plan(nodes: List[str], graph: Dict[str, List[str]]) -&gt; Optional[List[str]]: to_visit, visited, checked = deque(labels), [], defaultdict(int) while to_visit: cur_place = to_visit.pop() checked[cur_place] += 1 for dep_place in graph.get(cur_place): # dependencies if dep_place not in visited: to_visit.append(cur_place) break else: visited.append(cur_place) if checked[cur_place] &gt; 2: return None return visited for labels, dependencies in [ [[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], [(&#39;a&#39;, &#39;e&#39;), (&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;c&#39;), (&#39;c&#39;, &#39;d&#39;)]], [[&#39;a&#39;, &#39;b&#39;], [(&#39;b&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;)]],]: graph = make_dependency_graph(labels, dependencies) print(f&quot;Dependency Graph: {graph}&quot;) visited = get_execution_plan(labels, graph) print(f&quot;Suggested Execution Plan: {visited}&quot;) . Dependency Graph: {&#39;a&#39;: [&#39;e&#39;, &#39;b&#39;], &#39;b&#39;: [&#39;c&#39;], &#39;c&#39;: [&#39;d&#39;], &#39;d&#39;: [], &#39;e&#39;: []} Suggested Execution Plan: [&#39;e&#39;, &#39;d&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;] Dependency Graph: {&#39;a&#39;: [&#39;b&#39;], &#39;b&#39;: [&#39;a&#39;]} Suggested Execution Plan: None .",
            "url": "https://kslader8.github.io/kslader8-thoughts/hacker-rank/2021/02/13/hackerrank-questions.html",
            "relUrl": "/hacker-rank/2021/02/13/hackerrank-questions.html",
            "date": " • Feb 13, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "SpaCy 3 - New Toys",
            "content": "The new version of SpaCy in pre-release hase several new features i&#39;m eagerly awaiting. . Transformer Model | Trainable Sentence Splitter | Improved Interface for Training | Groups of Overlapable Spans | . import json . import spacy from spacy.tokens import Doc from spacy.training import Example spacy.__version__ . &#39;3.0.0rc3&#39; . Models . There is now a transformers based model for greater accuracy . model_acc = spacy.load(&quot;en_core_web_trf&quot;) model_acc.components . [(&#39;transformer&#39;, &lt;spacy_transformers.pipeline_component.Transformer at 0x1be4e4513b0&gt;), (&#39;tagger&#39;, &lt;spacy.pipeline.tagger.Tagger at 0x1be4e457e50&gt;), (&#39;parser&#39;, &lt;spacy.pipeline.dep_parser.DependencyParser at 0x1bd9286b040&gt;), (&#39;ner&#39;, &lt;spacy.pipeline.ner.EntityRecognizer at 0x1bdb7373ee0&gt;), (&#39;attribute_ruler&#39;, &lt;spacy.pipeline.attributeruler.AttributeRuler at 0x1be0a08a600&gt;), (&#39;lemmatizer&#39;, &lt;spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1be0a091900&gt;)] . As well as an improved version of the model we all know and love . model_eff = spacy.load(&quot;en_core_web_sm&quot;) model_eff.components . [(&#39;tok2vec&#39;, &lt;spacy.pipeline.tok2vec.Tok2Vec at 0x1be4ba76860&gt;), (&#39;tagger&#39;, &lt;spacy.pipeline.tagger.Tagger at 0x1be0a1c94a0&gt;), (&#39;parser&#39;, &lt;spacy.pipeline.dep_parser.DependencyParser at 0x1bdb5b896a0&gt;), (&#39;senter&#39;, &lt;spacy.pipeline.senter.SentenceRecognizer at 0x1be4ba76b80&gt;), (&#39;ner&#39;, &lt;spacy.pipeline.ner.EntityRecognizer at 0x1bdb5cb7fa0&gt;), (&#39;attribute_ruler&#39;, &lt;spacy.pipeline.attributeruler.AttributeRuler at 0x1bdb5f156c0&gt;), (&#39;lemmatizer&#39;, &lt;spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1bdb5f6b340&gt;)] . Example Class . I used to dread getting everything formatted into Gold objects previously... The new Example makes a uniform and simple way of formatting data for training . predicted = Doc(model_eff.vocab, words=[&quot;Apply&quot;, &quot;some&quot;, &quot;sun&quot;, &quot;screen&quot;]) token_ref = [&quot;Apply&quot;, &quot;some&quot;, &quot;sun&quot;, &quot;screen&quot;] tags_ref = [&quot;VERB&quot;, &quot;DET&quot;, &quot;NOUN&quot;, &quot;NOUN&quot;] sent_refs = [1, 0, 0, 0] example = Example.from_dict(predicted, {&quot;words&quot;: token_ref, &quot;tags&quot;: tags_ref, &quot;sent_starts&quot;: sent_refs}) example.to_dict() . {&#39;doc_annotation&#39;: {&#39;cats&#39;: {}, &#39;entities&#39;: [&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;], &#39;links&#39;: {}}, &#39;token_annotation&#39;: {&#39;ORTH&#39;: [&#39;Apply&#39;, &#39;some&#39;, &#39;sun&#39;, &#39;screen&#39;], &#39;SPACY&#39;: [True, True, True, True], &#39;TAG&#39;: [&#39;VERB&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;], &#39;LEMMA&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;POS&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;MORPH&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;HEAD&#39;: [0, 1, 2, 3], &#39;DEP&#39;: [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;], &#39;SENT_START&#39;: [1, 0, 1, 0]}} . A Statistical Sentence Splitter . This has been one of my dreams for quite a while. I mentioned it a few times on the SpaCy forums too. Very glad to see it make it&#39;s way into the library now. . senter = model_eff.get_pipe(&quot;senter&quot;) optimizer = model_eff.initialize() examples = [example] losses = senter.update(examples, sgd=optimizer) losses . {&#39;senter&#39;: 1.9999818801879883} .",
            "url": "https://kslader8.github.io/kslader8-thoughts/nlp/2021/01/22/spacy-version-3.html",
            "relUrl": "/nlp/2021/01/22/spacy-version-3.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Sorted Collections in Python",
            "content": "I recently was looking at a making predictions on a graph transaction and required maintaining a sorted collection to implement. After spining my wheels (wasting time and failing to build it myself), I google for a bit and found this amazing library called sortedcontainers that is pure python to boot. . If you didn&#39;t know about it previously, check it out. It will make your life easier in the future. . !conda install memory_profiler -n data-structures -y . Collecting package metadata (repodata.json): ...working... done Solving environment: ...working... done # All requested packages already installed. . !conda install sortedcontainers -n data-structures -y . Collecting package metadata (repodata.json): ...working... done Solving environment: ...working... done # All requested packages already installed. . %load_ext autoreload %load_ext memory_profiler %autoreload 2 . from sortedcontainers import SortedList, SortedDict, SortedSet . Sorted List . l = [&#39;e&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;b&#39;] print(f&quot;List({l})&quot;) sl = SortedList(l) print(f&quot;{sl}&quot;) sl *= 10_000_000 print(f&quot;count of c: {sl.count(&#39;c&#39;):,}&quot;) del sl . List([&#39;e&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;b&#39;]) SortedList([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]) count of c: 10,000,000 . %%timeit -n 1 -r 5 %memit sl = SortedList([&#39;e&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;b&#39;]) sl *= 10_000_000 sl.count(&#39;c&#39;) del sl . peak memory: 51.81 MiB, increment: 0.15 MiB peak memory: 52.68 MiB, increment: 0.01 MiB peak memory: 53.14 MiB, increment: -0.00 MiB peak memory: 53.18 MiB, increment: 0.02 MiB peak memory: 53.34 MiB, increment: 0.01 MiB 15.9 s ± 254 ms per loop (mean ± std. dev. of 5 runs, 1 loop each) . Sorted Dict . d = {&#39;c&#39;: 3, &#39;a&#39;: 1, &#39;b&#39;: 2} print(f&quot;Dict({d})&quot;) sd = SortedDict(d) print(f&quot;{sd}&quot;) print(f&quot;pop last item: {sd.popitem(index=-1)}&quot;) del sd . Dict({&#39;c&#39;: 3, &#39;a&#39;: 1, &#39;b&#39;: 2}) SortedDict({&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}) pop last item: (&#39;c&#39;, 3) . %%timeit -n 1 -r 5 %memit sd = SortedDict({&#39;c&#39;: 3, &#39;a&#39;: 1, &#39;b&#39;: 2}) sd.popitem(index=-1) del sd . peak memory: 53.62 MiB, increment: 0.00 MiB peak memory: 53.62 MiB, increment: 0.00 MiB peak memory: 53.62 MiB, increment: 0.00 MiB peak memory: 53.63 MiB, increment: 0.00 MiB peak memory: 53.63 MiB, increment: 0.00 MiB 7.84 s ± 248 ms per loop (mean ± std. dev. of 5 runs, 1 loop each) . Sorted Set . s = set(&#39;abracadabra&#39;) print(f&quot;Set({s})&quot;) ss = SortedSet(s) print(f&quot;{ss}&quot;) print(f&quot;index of c: {ss.bisect_left(&#39;c&#39;)}&quot;) del ss . Set({&#39;d&#39;, &#39;c&#39;, &#39;r&#39;, &#39;b&#39;, &#39;a&#39;}) SortedSet([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]) index of c: 2 . %%timeit -n 1 -r 5 %memit ss = SortedSet(&#39;abracadabra&#39;) ss.bisect_left(&#39;c&#39;) del ss . peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB peak memory: 53.68 MiB, increment: 0.00 MiB 7.73 s ± 200 ms per loop (mean ± std. dev. of 5 runs, 1 loop each) .",
            "url": "https://kslader8.github.io/kslader8-thoughts/data-structures/2021/01/18/sorted-collections.html",
            "relUrl": "/data-structures/2021/01/18/sorted-collections.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Transparency / Privacy Tradeoff in a Differentially Private World",
            "content": "from PIL import Image from pathlib import Path . img_dir = Path().absolute().parent / &quot;images&quot; . The Common Mental Model for Transparency / Privacy Tradeoff . Historically, people think about privacy and transparency as a zero sum game. This is because to have access to another parties information means it is not private between the two parties. This makes it difficult to share information without building a strong relationship of trust between two parties. . Image.open(img_dir / &quot;privacy-transparency-pareto-tradeoff.png&quot;) . How People Should Start Thinking About the Tradeoff . Differential privacy techniques changes this to a superior (if not perfect) trade off in practice. Allowing parties to build new business models by taking advantage of less black and write trust relationships. This also gives parties more options to manage/mitigate data leakage risk without completely foregoing utilization of it. . Image.open(img_dir / &quot;moving-the-privacy-transparency-pareto-tradeoff.png&quot;) .",
            "url": "https://kslader8.github.io/kslader8-thoughts/2021/01/14/differential-privacy-visualized.html",
            "relUrl": "/2021/01/14/differential-privacy-visualized.html",
            "date": " • Jan 14, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "EDA of Sample Dataset",
            "content": "Objective . Data Scientists and Analysts are often tasked to clean and analyze datasets. We are working with an external research firm who specializes in the application of artificial intelligence to forecasting prices of financial instruments. This firm has developed a proprietary system, called “4sight”, to forecast prices of certain instruments. . To demonstrate the effectiveness of their forecasting system, the vendor has sent us attached sample dataset. The dataset includes signal values generated by the 4sight system as well as historical prices for a well-known broad market ETF. . A Portfolio Manager has asked you to: . 1) Review the quality of the data, list any potential errors, and propose corrected values. Please list each quality check error and correction applied. 2) Please analyze the signal’s effectiveness or lack thereof in forecasting ETF price, using whatever metrics you think are most relevant. 3) (Extra credit) Write a 1-2 paragraph summary for the Portfolio Manager addressing your observations about the efficacy and believability of the product, and recommendation for next steps. 4) Please budget at least one hour for completing this exercise. Please include all the intermediate steps when sending your solution to this exercise back to us. . Conclusion . Data issues noticed to discuss / inform vendor of: . The last six values of Signal are zeros. Why? drop last six rows (or fill forward as the stale value is probably what most systems will end up using) | . | A large outlier of Signal was found on the 2017-11-13. Why? drop row (or fill forward as the stale value is probably what most systems will end up using) | . | Adj Close is very different from Close / Low / High / Open. How was it adjusted? ignore until more confident about the data | . | a large ourlier of Close was found on the 2018-03-19. Why? fill the highest value of Close with the next mornings open | . | 2017-09-08 to 2017-09-22 look to be filled forward for the Close. fill with Open shifted forward 1 Day | . | high and low inverted on 2018-03-07 and 2018-07-16 ignore if you don&#39;t plan to test them as features or response | if you plan to use them. fill Low with the min of [Open, High, Low, Close] and High with max of [Open, High, Low, Close] | . | . On first glance, the signal looks to have no predictive power out of sample (20% of the most recent data). This conclusion was drawn from conducting a residual plot of Signal against Open shifted forward 1 day and Close. To quickly check for some statistical violations the same was done on a tree model which produced similar conclusions. . I&#39;d hold off on doing a lot more analysis as it is unlikely to be valuable on it&#39;s own. If we delve further, I would focus on checking the signals power as an interaction term with other features and transforming the problem into a classifier instead of a regression. . Notebook Setup . %load_ext autoreload %autoreload 2 %matplotlib inline . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . from pathlib import Path . import sklearn import interpret import yellowbrick import pandas as pd import numpy as np from interpret import show from interpret.data import Marginal from interpret.glassbox import ExplainableBoostingRegressor, RegressionTree from interpret.perf import RegressionPerf from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from yellowbrick.features import Rank2D from yellowbrick.features import JointPlotVisualizer from yellowbrick.regressor import ResidualsPlot . from pydqc import infer_schema, data_summary, data_compare, data_consist . print(f&quot;numpy: {np.__version__}&quot;) print(f&quot;pandas: {pd.__version__}&quot;) print(f&quot;sklearn: {sklearn.__version__}&quot;) print(f&quot;interpret: {interpret.__version__}&quot;) print(f&quot;yellowbrick: {yellowbrick.__version__}&quot;) . numpy: 1.19.2 pandas: 1.2.0 sklearn: 0.23.2 interpret: 0.2.2 yellowbrick: 1.2 . Data . eda_dir = Path(&quot;.&quot;) data_path = eda_dir / &quot;Sample Dataset.xlsx&quot; assert data_path.exists() == True df = pd.read_excel(data_path) pd.concat([df.dtypes, df.head(1).T, df.tail(1).T], axis=1) . 0 0 1037 . Date datetime64[ns] | 2015-11-19 00:00:00 | 2020-01-06 00:00:00 | . Signal float64 | 13.768540 | 0.000000 | . Open float64 | 116.440002 | 163.850006 | . High float64 | 116.650002 | 165.539993 | . Low float64 | 115.739998 | 163.539993 | . Close float64 | 116.059998 | 165.350006 | . Adj Close float64 | 108.281601 | 163.534668 | . df.describe() . Signal Open High Low Close Adj Close . count 1038.000000 | 1038.000000 | 1038.000000 | 1038.000000 | 1038.000000 | 1038.000000 | . mean 16.766190 | 141.847360 | 142.691801 | 140.907746 | 141.840973 | 136.341060 | . std 3.095783 | 18.475574 | 18.470255 | 18.404504 | 18.497010 | 21.427837 | . min 0.000000 | 94.080002 | 95.400002 | 93.639999 | 94.790001 | -152.277847 | . 25% 14.691150 | 132.132496 | 132.912495 | 130.542503 | 131.824993 | 125.290491 | . 50% 17.298240 | 146.769997 | 147.959999 | 145.634995 | 146.885002 | 142.667732 | . 75% 19.030890 | 155.367496 | 156.287495 | 154.422500 | 155.289993 | 151.798325 | . max 35.434147 | 172.789993 | 173.389999 | 171.949997 | 196.279999 | 168.842270 | . Comments . Date looks like the obvious index | Why is the adj close so far from the close? | Why is the signal 0 for last entry? Is zero a valid value? | Why is the max close greater than the max high? | . Check that Date column is unique . df[&#39;Date&#39;].value_counts().max() . 1 . Review Adj Close . df[[&#39;Close&#39;, &#39;Adj Close&#39;]].plot(figsize=(16,8)) . &lt;AxesSubplot:&gt; . (df[&#39;Close&#39;] - df[&#39;Adj Close&#39;]).hist(figsize=(16,6), bins=[0, 2, 5, 10, 15, 20, 30]) . &lt;AxesSubplot:&gt; . (df[&#39;Close&#39;] - df[&#39;Adj Close&#39;]).describe() . count 1038.000000 mean 5.499913 std 9.748196 min -33.025085 25% 3.777889 50% 5.693078 75% 6.516651 max 308.837845 dtype: float64 . Comments . This looks weird, but I&#39;m not sure what&#39;s going on. I&#39;m not comfortable using Adj Close until this is understood | . Check Signal Histogram . df[&#39;Signal&#39;].hist(figsize=(12,6)) . &lt;AxesSubplot:&gt; . df[&#39;Signal&#39;].describe() . count 1038.000000 mean 16.766190 std 3.095783 min 0.000000 25% 14.691150 50% 17.298240 75% 19.030890 max 35.434147 Name: Signal, dtype: float64 . Check Indexs of Zero Signal . df[df[&#39;Signal&#39;] == 0.0].index . Int64Index([1032, 1033, 1034, 1035, 1036, 1037], dtype=&#39;int64&#39;) . Comments . looks like an issue in signal calculation at for the last 6 rows. we should inform the signal provider and drop them for now. we could also fill forward the last real signal | . Check the Data for Max Signal . df[df[&#39;Signal&#39;] == df[&#39;Signal&#39;].max()].index . Int64Index([500], dtype=&#39;int64&#39;) . max_signal_iloc = df[df[&#39;Signal&#39;] == df[&#39;Signal&#39;].max()].index.values[0] df.iloc[max_signal_iloc - 1: max_signal_iloc + 2] . Date Signal Open High Low Close Adj Close . 499 2017-11-10 | 17.628384 | 146.710007 | 147.100006 | 146.350006 | 146.570007 | 140.810852 | . 500 2017-11-13 | 35.434147 | 145.929993 | 146.820007 | 145.500000 | 146.610001 | 140.849274 | . 501 2017-11-14 | 17.456319 | 146.059998 | 146.490005 | 145.589996 | 146.210007 | 140.465012 | . Comments . looks like the max is an error. we should inform the signal provider and drop them for now. we could also fill forward the last real signal | . Check Close Histogram . df[&#39;Close&#39;].hist(figsize=(12,6)) . &lt;AxesSubplot:&gt; . max_close_iloc = df[df[&#39;Close&#39;] == df[&#39;Close&#39;].max()].index.values[0] df.iloc[max_close_iloc - 1: max_close_iloc + 2] . Date Signal Open High Low Close Adj Close . 584 2018-03-16 | 19.385186 | 156.979996 | 158.270004 | 156.750000 | 157.800003 | 152.174042 | . 585 2018-03-19 | 18.660897 | 157.169998 | 157.210007 | 154.449997 | 196.279999 | 150.708221 | . 586 2018-03-20 | 19.177721 | 156.669998 | 157.020004 | 155.770004 | 156.240005 | 150.669647 | . Comments . looks like the max close is an error... I&#39;d recommend setting it to next days open so we can test it as a response variable | . Check Open Histogram . df[&#39;Open&#39;].hist(figsize=(12,6)) . &lt;AxesSubplot:&gt; . Check High, Low, Implied Rules . High should be greater than Close, Open, and Low | Low should be less than Close, Open, and High | . df[~(df[&#39;Close&#39;] &lt;= df[&#39;High&#39;])] . Date Signal Open High Low Close Adj Close . 431 2017-08-07 | 16.298805 | 140.440002 | 140.350000 | 139.710007 | 140.440002 | 134.595871 | . 585 2018-03-19 | 18.660897 | 157.169998 | 157.210007 | 154.449997 | 196.279999 | 150.708221 | . 766 2018-12-06 | 16.904044 | 145.449997 | 147.099997 | 143.429993 | 147.199997 | 143.173874 | . 983 2019-10-17 | 18.878412 | 152.289993 | 153.309995 | 152.050003 | 153.339996 | 151.102173 | . df[~(df[&#39;Open&#39;] &lt;= df[&#39;High&#39;])] . Date Signal Open High Low Close Adj Close . 431 2017-08-07 | 16.298805 | 140.440002 | 140.35 | 139.710007 | 140.440002 | 134.595871 | . df[~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 455 2017-09-11 | 15.838558 | 140.389999 | 140.919998 | 140.229996 | 139.110001 | 133.321198 | . 456 2017-09-12 | 15.518587 | 141.039993 | 141.690002 | 140.820007 | 139.110001 | 133.321198 | . 457 2017-09-13 | 16.158529 | 141.410004 | 142.220001 | 141.320007 | 139.110001 | 133.321198 | . 458 2017-09-14 | 16.478500 | 141.779999 | 142.160004 | 141.419998 | 139.110001 | 133.321198 | . 459 2017-09-15 | 15.198617 | 141.639999 | 142.470001 | 141.550003 | 139.110001 | 133.321198 | . 460 2017-09-18 | 15.518587 | 142.619995 | 143.809998 | 142.600006 | 139.110001 | 133.321198 | . 461 2017-09-19 | 16.798471 | 143.570007 | 143.690002 | 143.089996 | 139.110001 | 133.321198 | . 462 2017-09-20 | 15.953688 | 143.529999 | 144.020004 | 143.259995 | 139.110001 | 133.321198 | . 463 2017-09-21 | 16.004491 | 144.020004 | 144.259995 | 143.479996 | 139.110001 | 133.321198 | . 464 2017-09-22 | 16.997600 | 143.669998 | 144.669998 | 143.559998 | 139.110001 | 133.321198 | . 577 2018-03-07 | 18.885411 | 154.460007 | 156.929993 | 157.220001 | 156.740005 | 151.151840 | . 671 2018-07-16 | 20.010313 | 167.759995 | 168.029999 | 169.960007 | 166.770004 | 161.779312 | . 739 2018-10-19 | 17.461385 | 155.470001 | 156.360001 | 154.740005 | 153.360001 | 149.165390 | . 892 2019-06-10 | 19.055083 | 151.449997 | 153.139999 | 152.449997 | 151.750000 | 148.488159 | . 966 2019-09-24 | 18.630976 | 155.149994 | 155.289993 | 152.839996 | 152.429993 | 150.205444 | . df[(df[&#39;Close&#39;] == 139.110001)] . Date Signal Open High Low Close Adj Close . 313 2017-02-17 | 16.635032 | 138.449997 | 139.160004 | 138.250000 | 139.110001 | 132.366592 | . 453 2017-09-07 | 16.478500 | 139.589996 | 139.690002 | 138.589996 | 139.110001 | 133.321198 | . 454 2017-09-08 | 15.518587 | 138.929993 | 139.770004 | 138.619995 | 139.110001 | 133.321198 | . 455 2017-09-11 | 15.838558 | 140.389999 | 140.919998 | 140.229996 | 139.110001 | 133.321198 | . 456 2017-09-12 | 15.518587 | 141.039993 | 141.690002 | 140.820007 | 139.110001 | 133.321198 | . 457 2017-09-13 | 16.158529 | 141.410004 | 142.220001 | 141.320007 | 139.110001 | 133.321198 | . 458 2017-09-14 | 16.478500 | 141.779999 | 142.160004 | 141.419998 | 139.110001 | 133.321198 | . 459 2017-09-15 | 15.198617 | 141.639999 | 142.470001 | 141.550003 | 139.110001 | 133.321198 | . 460 2017-09-18 | 15.518587 | 142.619995 | 143.809998 | 142.600006 | 139.110001 | 133.321198 | . 461 2017-09-19 | 16.798471 | 143.570007 | 143.690002 | 143.089996 | 139.110001 | 133.321198 | . 462 2017-09-20 | 15.953688 | 143.529999 | 144.020004 | 143.259995 | 139.110001 | 133.321198 | . 463 2017-09-21 | 16.004491 | 144.020004 | 144.259995 | 143.479996 | 139.110001 | 133.321198 | . 464 2017-09-22 | 16.997600 | 143.669998 | 144.669998 | 143.559998 | 139.110001 | 133.321198 | . df[(df[&#39;Close&#39;] == 139.110001) &amp; ~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 455 2017-09-11 | 15.838558 | 140.389999 | 140.919998 | 140.229996 | 139.110001 | 133.321198 | . 456 2017-09-12 | 15.518587 | 141.039993 | 141.690002 | 140.820007 | 139.110001 | 133.321198 | . 457 2017-09-13 | 16.158529 | 141.410004 | 142.220001 | 141.320007 | 139.110001 | 133.321198 | . 458 2017-09-14 | 16.478500 | 141.779999 | 142.160004 | 141.419998 | 139.110001 | 133.321198 | . 459 2017-09-15 | 15.198617 | 141.639999 | 142.470001 | 141.550003 | 139.110001 | 133.321198 | . 460 2017-09-18 | 15.518587 | 142.619995 | 143.809998 | 142.600006 | 139.110001 | 133.321198 | . 461 2017-09-19 | 16.798471 | 143.570007 | 143.690002 | 143.089996 | 139.110001 | 133.321198 | . 462 2017-09-20 | 15.953688 | 143.529999 | 144.020004 | 143.259995 | 139.110001 | 133.321198 | . 463 2017-09-21 | 16.004491 | 144.020004 | 144.259995 | 143.479996 | 139.110001 | 133.321198 | . 464 2017-09-22 | 16.997600 | 143.669998 | 144.669998 | 143.559998 | 139.110001 | 133.321198 | . Comments . Close data looks to have been filled forward for several dates with 139.110001 | To use close as response variable I&#39;d recommend filling it with the next days open for these instances | . df[~(df[&#39;Open&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 407 2017-07-04 | 15.282748 | 141.339996 | 142.600000 | 141.400003 | 142.200006 | 135.700998 | . 577 2018-03-07 | 18.885411 | 154.460007 | 156.929993 | 157.220001 | 156.740005 | 151.151840 | . 671 2018-07-16 | 20.010313 | 167.759995 | 168.029999 | 169.960007 | 166.770004 | 161.779312 | . 892 2019-06-10 | 19.055083 | 151.449997 | 153.139999 | 152.449997 | 151.750000 | 148.488159 | . df[~(df[&#39;High&#39;] &gt;= df[&#39;Low&#39;])] . Date Signal Open High Low Close Adj Close . 577 2018-03-07 | 18.885411 | 154.460007 | 156.929993 | 157.220001 | 156.740005 | 151.151840 | . 671 2018-07-16 | 20.010313 | 167.759995 | 168.029999 | 169.960007 | 166.770004 | 161.779312 | . Comments . High and Low data doesn&#39;t look reliable, but if we don&#39;t plan to use it we can disregard this for now | The easiest HACK is to set Low = Open in these cases | . Review Signal Predictive Power . Preprocessing / Cleanup . df = df.sort_values(by=&#39;Date&#39;) # shift open price forward 1 day df[&#39;Open +1D&#39;] = df[&#39;Open&#39;].shift(1) # Add Return Columns for a Stationary Response df[&#39;Open +1D Return&#39;] = df[&#39;Open +1D&#39;].pct_change() df[&#39;Close Return&#39;] = df[&#39;Close&#39;].pct_change() # drop last 6 zero rows from signal df = df[df[&#39;Signal&#39;] != 0.0] # drop highest value from Signal df = df[df[&#39;Signal&#39;] != df[&#39;Signal&#39;].max()] # fill the highest value of Close with the next mornings open max_close_iloc = df[df[&#39;Close&#39;] == df[&#39;Close&#39;].max()].index.values[0] df.loc[max_close_iloc, &#39;Close&#39;] = df.loc[max_close_iloc+1, &#39;Open&#39;] # adjusting the period where close is filled forward with the next days open price instead df.loc[(df[&#39;Close&#39;] == 139.110001) &amp; ~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;]), &#39;Close&#39;] = df.loc[(df[&#39;Close&#39;] == 139.110001) &amp; ~(df[&#39;Close&#39;] &gt;= df[&#39;Low&#39;]), &#39;Open +1D&#39;] # set date as index df = df.set_index(&#39;Date&#39;) . Predictor &amp; Response . x will be signal | y_1 will be the cleaned up close column | y_2 will be the naturally cleaner open column | . Train &amp; dev set will be split at 20% as there is not a huge amount of data points. The data will be split without shuffling to capture the most recent values in the dev set . x_1, y_1, y_2 = df[&#39;Signal&#39;], df[&#39;Close&#39;], df[&#39;Open +1D&#39;] y_2 = y_2.dropna() x_2 = x_1[y_2.index] train_x_1, dev_x_1, train_y_1, dev_y_1 = train_test_split(x_1, y_1, test_size=0.2, shuffle=False) train_x_2, dev_x_2, train_y_2, dev_y_2 = train_test_split(x_2, y_2, test_size=0.2, shuffle=False) . Analysis . Signal vs Close Plot . visualizer = JointPlotVisualizer(size=(1080, 720)) visualizer.fit_transform(x_1, y_1) visualizer.show() . &lt;AxesSubplot:xlabel=&#39;x&#39;, ylabel=&#39;y&#39;&gt; . Signal vs Open +1D Plot . visualizer = JointPlotVisualizer(size=(1080, 720)) visualizer.fit_transform(x_2, y_2) visualizer.show() . &lt;AxesSubplot:xlabel=&#39;x&#39;, ylabel=&#39;y&#39;&gt; . Train a Simple Linear Regression and Evaluate Residual Plot . visualizer = ResidualsPlot(LinearRegression(),size=(1080, 720)) visualizer.fit(train_x_1.values.reshape(-1, 1), train_y_1.values.reshape(-1, 1)) visualizer.score(dev_x_1.values.reshape(-1, 1), dev_y_1.values.reshape(-1, 1)) visualizer.show() . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Residuals for LinearRegression Model&#39;}, xlabel=&#39;Predicted Value&#39;, ylabel=&#39;Residuals&#39;&gt; . visualizer = ResidualsPlot(LinearRegression(),size=(1080, 720)) visualizer.fit(train_x_2.values.reshape(-1, 1), train_y_2.values.reshape(-1, 1)) visualizer.score(dev_x_2.values.reshape(-1, 1), dev_y_2.values.reshape(-1, 1)) visualizer.show() . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Residuals for LinearRegression Model&#39;}, xlabel=&#39;Predicted Value&#39;, ylabel=&#39;Residuals&#39;&gt; . Review Findings with Tree Models . there are less statistical assumptions to violate, so is a good check on the linear | . ebm = ExplainableBoostingRegressor() ebm.fit(train_x_2.to_frame(), train_y_2) rt = RegressionTree() rt.fit(train_x_2.to_frame(), train_y_2) . &lt;interpret.glassbox.decisiontree.RegressionTree at 0x2842704ccd0&gt; . ebm_perf = RegressionPerf(ebm.predict).explain_perf(dev_x_2.to_frame(), dev_y_2, name=&#39;Boosting Tree&#39;) rt_perf = RegressionPerf(rt.predict).explain_perf(dev_x_2.to_frame(), dev_y_2, name=&#39;Regression Tree&#39;) . show(rt_perf) show(ebm_perf) . &lt;iframe src=&quot;http://127.0.0.1:7001/2766622033376/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; &lt;iframe src=&quot;http://127.0.0.1:7001/2766622034096/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; Check Global Tree . ebm_global = ebm.explain_global(name=&#39;Boosting Tree&#39;) rt_global = rt.explain_global(name=&#39;Regression Tree&#39;) . show(ebm_global) show(rt_global) . &lt;iframe src=&quot;http://127.0.0.1:7001/2766622408560/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; &lt;iframe src=&quot;http://127.0.0.1:7001/2766666354592/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt;",
            "url": "https://kslader8.github.io/kslader8-thoughts/2021/01/07/sample-eda.html",
            "relUrl": "/2021/01/07/sample-eda.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "HackerRank Question - Transitive Clouser of a Graph",
            "content": "Compute the Transitive Closure of a Graph . After doing a little reading, it appears that Warshall&#39;s method is the most common way of doing this in practice . there is a path from i to j going through vertex 1 | there is a path from i to j going through vertex 1 and/or 2 | there is a path from i to j going through vertex 1, 2, and/or 3 | there is a path from i to j going through any of the other vertices | . The time complexity of this algorithm is same as that of Floyd–Warshall algorithm i.e. O(3) but it reduces storage by retaining only one bit for each matrix element (e.g. we can use bool data-type instead of int). The implementation . import pandas as pd from scipy import sparse . graph = sparse.csr_matrix( [ [1,1,0,0], [1,1,0,0], [0,0,1,0], [0,0,0,1] ]) df = pd.DataFrame.sparse.from_spmatrix(graph) df . 0 1 2 3 . 0 1 | 1 | 0 | 0 | . 1 1 | 1 | 0 | 0 | . 2 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 1 | . groups, labels = sparse.csgraph.connected_components(graph, directed=False) groups . 3 .",
            "url": "https://kslader8.github.io/kslader8-thoughts/hacker-rank/2020/12/27/hackerrank-question.html",
            "relUrl": "/hacker-rank/2020/12/27/hackerrank-question.html",
            "date": " • Dec 27, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hi, my name is Kevin - the minion - from films such as Despicable Me, Despicable Me 2, and Minions. . What you may not know is that I am also a senior data scientist and member of (Societe Generale’s) SG (Corporate Investment Bank’s) CIB’s digital office. Diligently toiling away determining &amp; applying data driven strategy to the bank. Working into the wee hours analyzing and automating conversations in capital markets. . Thank you and Goodnight! . Minion, Consultant, Developer, Trader, Data Scientist ~ Kevin .",
          "url": "https://kslader8.github.io/kslader8-thoughts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kslader8.github.io/kslader8-thoughts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}